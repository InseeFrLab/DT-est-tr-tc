% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt,
  a4paper,french]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[hmargin = 25mm, vmargin = 25mm]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{french}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\usepackage{pdfpages, setspace, mathptmx} %times roman
\usepackage{textpos}%pour textblock
\makeatletter
\def\@maketitle{%
\thispagestyle{fancy}
\begin{center}
%TODO
\line(1,0){450}
\vspace{5mm}
\textbf{{\huge E}\Large stimation en temps réel de la tendance-cycle : apport de l’utilisation des filtres asymétriques dans la détection des points de retournement}
\end{center}

\begin{center}
\textit{Alain QUARTIER-LA-TENTE*} \\
\vspace{2mm}
\textit{(*) Insee,  Département des études économiques (DEE), LEMNA}\\
\vspace{2mm}
\url{alain.quartier-la-tente@insee.fr} 
\end{center}

\begin{center}
\line(1,0){450}
\end{center}

\normalsize
}
\usepackage{fancyhdr} 
\pagestyle{fancy}

\renewcommand{\headrulewidth}{0pt}
\fancyhead[C]{} 
\fancyhead[L]{}
\fancyhead[R]{}

\renewcommand{\footrulewidth}{0pt}
\fancyfoot[C]{\thepage}
\fancyfoot[C]{\thepage}

\makeatother% cinsérer page de garde

\usepackage{stmaryrd}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{animate, dsfont, here, xspace}
%\usepackage{tikz}       
\usepackage{tikz,pgfplots}
 \pgfplotsset{compat=1.17}
%\includepdf[fitpaper=true, pages=-]{img/pdg.pdf}


\DeclareMathOperator{\e}{e}
\DeclareMathOperator{\Determinant}{det}
\renewcommand{\P}{\mathds{P}} %Apparement \P existe déjà ?
\newcommand\N{\mathds{N}}
\newcommand\Norm{\mathcal{N}}
\newcommand\R{\mathds{R}}
\newcommand\Z{\mathds{Z}}
\newcommand{\determinant}[1]{\Determinant\left(#1\right)}
%\newcommand\C{\mathds{C}}
%\newcommand\Z{\mathds{Z}}


\newcommand\1{\mathds{1}}
\newcommand{\E}[2][]{{\mathds{E}}_{#1}
  \def\temp{#2}\ifx\temp\empty
  \else
    \left[#2\right]
  \fi
}
\newcommand{\V}[2][]{{\mathds{V}}_{#1}
  \def\temp{#2}\ifx\temp\empty
  \else
    \left[#2\right]
  \fi
}
\newcommand\ud{\,\mathrm{d}}
\newcommand{\ps}[2]{\left\langle #1 \,,\, #2 \right\rangle}

% blocks
\usepackage{environ}
\usepackage[tikz]{bclogo}

\tikzstyle{titlestyle} =[draw=black!80,fill=black!20, text=black,
 right=10pt, rounded corners]
\mdfdefinestyle{symmaryboxstyle}{
	linecolor=black!80, backgroundcolor = black!5,
	skipabove=\baselineskip, innertopmargin=\baselineskip,
	innerbottommargin=\baselineskip,
	userdefinedwidth=\textwidth,
	middlelinewidth=1.2pt, roundcorner=5pt,
	skipabove={\dimexpr0.5\baselineskip+\topskip\relax},
	frametitleaboveskip=\dimexpr-\ht\strutbox\relax,
	innerlinewidth=0pt,
}
\newcounter{summarybox}
\NewEnviron{summary_box}[2][true]{%
\refstepcounter{summarybox} % on incrémente le compteur
\begin{mdframed}[style=symmaryboxstyle,
nobreak=#1,
frametitle={%
      \tikz[baseline=(current bounding box.east),outer sep=0pt]
      \node[titlestyle,anchor=east]
    {Encadré \thesummarybox{} --- #2};}
]
\vspace{-0.5em}
\BODY
\end{mdframed}
}

\usepackage{amsthm}
%\theoremstyle{remark}
\newtheorem*{remarque}{Remarque}

\usepackage{mathrsfs}
\usepackage{fontawesome5}

\usepackage[style=authoryear,uniquename=false, uniquelist=false]{biblatex}
\DefineBibliographyStrings{french}{andothers={et\addabbrvspace alii}}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[style=authoryear,]{biblatex}
\addbibresource{biblio.bib}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Estimation en temps réel de la tendance-cycle : apport de l'utilisation des moyennes mobiles asymétriques},
  pdflang={fr},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\title{Estimation en temps réel de la tendance-cycle : apport de l'utilisation des moyennes mobiles asymétriques}
\author{Alain Quartier-la-Tente\\
Insee, Département des études économiques (DEE), LEMNA}
\date{Janvier 2024}

\begin{document}
\maketitle

\thispagestyle{fancy}

\hypertarget{ruxe9sumuxe9}{%
\section*{Résumé}\label{ruxe9sumuxe9}}
\addcontentsline{toc}{section}{Résumé}

Cette étude s'intéresse à différentes méthodes de construction de
moyennes mobiles pour l'estimation en temps réel de la tendance-cycle et
la détection rapide des points de retournement. Nous proposons une
comparaison des principales méthodes, en s'appuyant sur une formulation
générique de construction de moyennes mobiles. Nous décrivons également
deux prolongements possibles aux filtres polynomiaux locaux : l'ajout
d'un critère permettant de contrôler le déphasage (délai dans la
détection des points de retournement) et une façon de paramétriser
localement ces filtres. La comparaison empirique des méthodes montre que
: les problèmes d'optimisation de filtres issus des espaces de Hilbert à
noyau reproduisant (RKHS) augmentent le déphasage et les révisions des
estimations de la tendance-cycle ; modéliser des tendances polynomiales
trop complexes introduit plus de révisions sans diminuer le déphasage~;
pour les filtres polynomiaux, une paramétrisation locale permet une
réduction des révisions et du délai de détection des points de
retournement.

Mots clés : séries temporelles, tendance-cycle, désaisonnalisation,
points de retournement.

\hypertarget{abstract}{%
\section*{Abstract}\label{abstract}}
\addcontentsline{toc}{section}{Abstract}

This paper focuses on different approaches to build moving averages for
real-time trend-cycle estimation and fast turning point detection. We
propose a comparison of the main methods, based on a general unifying
framework to derive linear filters. We also describe two possible
extensions to local polynomial filters: the addition of a timeliness
criterion to control the phase shift (delay in the detection of turning
points) and a way to locally parameterize these filters. The empirical
comparison of the methods shows that: the optimization problems of the
filters from the Reproducing Kernel Hilbert Space (RKHS) theory increase
the phase shift and the revisions of the trend-cycle estimates; modeling
polynomial trends that are too complex introduces more revisions without
decreasing the phase shift; for polynomial filters, a local
parameterization reduces the phase shift and the revisions.

Keywords: time series, trend-cycle, seasonal adjustment, turning points.

JEL Classification: E32, E37.

\newpage

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
\newpage

\hypertarget{introduction}{%
\section*{Introduction}\label{introduction}}
\addcontentsline{toc}{section}{Introduction}

L'analyse du cycle économique, et en particulier la détection rapide des points de retournement d'une série, est un sujet de première importance dans l'analyse de la conjoncture économique.
Pour cela, les indicateurs économiques sont généralement corrigés des variations saisonnières.
Toutefois, afin d'améliorer leur lisibilité, il peut être nécessaire d'effectuer un lissage supplémentaire afin de réduire le bruit, et ainsi analyser la composante tendance-cycle.
Par construction, les méthodes d'extraction de tendance-cycle sont étroitement liées aux méthodes de désaisonnalisation.
En effet, afin d'estimer la composante saisonnière, les algorithmes de désaisonnalisation estiment préalablement une composante tendance-cycle.
Ainsi, même si les méthodes d'extraction de tendance-cycle sont généralement appliquées sur des séries corrigées des variations saisonnières, l'estimation de ces séries dépend également également des méthodes d'estimation de la tendance-cycle.

Les moyennes mobiles, ou les filtres linéaires, sont omniprésents dans les méthodes d'extraction du cycle économique et d'ajustement saisonnier\footnote{
  Une moyenne mobile est une méthode statistique qui consiste à appliquer une moyenne pondérée glissante à une série temporelle : à chaque date \(t\) on calcule une moyenne pondérée de \(p\) points passés et \(q\) points futurs où \(p,q\geq0\) dépend de la moyenne mobile.}.
Ainsi, la méthode de désaisonnalisation X-13ARIMA-SEATS utilise des moyennes mobiles de Henderson et des moyennes mobiles composites pour estimer les principales composantes d'une série chronologique.
Au centre de la série, des filtres symétriques sont appliqués.
Pour l'extraction de la tendance-cycle, le filtre symétrique le plus connu est celui de \textcite{henderson1916note}, notamment utilisé dans l'algorithme de désaisonnalisation X-13ARIMA.

En revanche, pour les estimations en temps réel, en raison de l'absence d'observations futures, toutes ces méthodes doivent s'appuyer sur des filtres asymétriques pour estimer les points les plus récents.
Par exemple, même si X-13ARIMA-SEATS applique des moyennes symétriques aux prévisions obtenues à partir d'un modèle ARIMA, cela revient à appliquer des filtres asymétriques en fin de série, car les valeurs prédites sont des combinaisons linéaires de valeurs passées.

Si ces moyennes mobiles asymétriques ont de bonnes propriétés concernant la taille des révisions futures induites par le processus de lissage\footnote{Voir par exemple \textcite{pierce1980SA}.}, elles induisent également, par construction, des déphasages qui retardent en général la détection en temps réel des points de retournement.

L'objectif de cette étude est de décrire et de comparer les approches récentes permettant l'extraction de tendance-cycle en temps-réel.
Après une description des propriétés générales d'un filtre linéaire (section \ref{sec-propMM}), différentes méthodes seront présentées :
filtres polynomiaux locaux, filtres fondés sur les espaces de Hilbert à noyau reproduisant (RKHS, section \ref{sec-nonparamreg}) et méthodes fondées sur une optimisation des propriétés des filtres --- \emph{Fidelity-Smoothness-Timeliness} (FST, section~\ref{subsec-GuggemosEtAl}) et \emph{accuracy, timeliness, smoothness} (ATS, section \ref{sec-WildiMcLeroy}).\\
Bien que ces méthodes aient été présentées avec des approches générales de construction des filtres linéaires, leurs propriétés théoriques et leurs performances empiriques n'ont pas été comparées.
Cette étude décrit l'ensemble de ces méthodes en développant une formulation générale pour la construction des filtres, permettant de mettre en exergue les spécificités de chacune.\\
Ainsi, les méthodes polynomiales ont l'avantage d'être simples et facilement compréhensibles.
Elles peuvent également prendre en compte des problèmes complexes (comme l'autocorrélation induite par l'utilisation d'un plan de sondage rotatif avec une période de recouvrement).
En revanche, l'inconvénient est que le déphasage ne peut pas être contrôlé (ce qui peut introduire des délais plus importants dans la détection des points de retournement).
Cette analyse montre aussi comment les filtres polynomiaux peuvent être étendus en ajoutant un critère permettant de contrôler le déphasage ou en les paramétrisant localement.\\
Les RKHS permettent de construire facilement des filtres adaptés à toutes les fréquences (y compris des fréquences irrégulières) mais ont notamment l'inconvénient d'être numériquement instables (des problèmes d'optimisation peuvent apparaître, comme dans la méthode FST).\\
La méthode FST a l'avantage d'être analytiquement soluble mais a l'inconvénient d'être plus difficilement paramétrisable car les différents critères utilisés ne sont pas normalisés : les poids associés aux différents critères n'ont donc pas de sens.

Enfin, dans une dernière partie (section \ref{sec-comparison}), toutes ces méthodes sont comparées empiriquement sur des séries simulées et sur des séries réelles désaisonnalisées.
En raison du lien entre désaisonnalisation et extraction de la tendance-cycle, les applications pratiques se concentreront sur les méthodes non-paramétriques qui pourraient être incluses dans X-13ARIMA-SEATS (la méthode ATS, paramétrique, est donc exclue).
Cette analyse permet tout d'abord de montrer comment les problèmes d'optimisation des filtres issus des RKHS peuvent conduire à des filtres sous-optimaux (augmentation des révisions et du délai dans la détection des points de retournement).
Ensuite, la méthode proposée pour paramétrer localement les filtres polynomiaux permet à la fois de détecter plus rapidement les points de retournement et de réduire les révisions dans les estimations successives de la tendance-cycle.
Enfin, les résultats montrent que les futures études sur les méthodes d'estimations de la tendance-cycle peuvent probablement se restreindre à celles qui modélisent des tendances locales de degré au plus 2 : modéliser des tendances plus complexes aboutit à plus de révisions sans gain en termes de détection des points de retournement.

Cette étude est reproductible.
Tous les codes utilisés sont disponibles sous \url{https://github.com/InseeFrLab/DT-est-tr-tc}.
Par ailleurs, toutes les méthodes décrites sont implémentées dans le \emph{package} \faIcon{r-project} \texttt{rjd3filters}\footnote{\url{https://github.com/rjdemetra/rjd3filters}.} qui accompagne cette étude.
Celui-ci propose également plusieurs outils pour manipuler les moyennes mobiles et évaluer la qualité des estimations.

\newpage

\hypertarget{sec-propMM}{%
\section{Quelques propriétés sur les moyennes mobiles}\label{sec-propMM}}

Cette section présente les définitions et les propriétés des moyennes mobiles utiles pour comprendre les méthodes présentées dans les prochaines sections.
Pour plus de détails sur les moyennes mobiles, voir par exemple \textcite{ch12HBSA}.

Soient deux entiers \(p\) et \(f\).
Une \emph{moyenne mobile} \(M_{\boldsymbol\theta}\) est un opérateur linéaire défini par un ensemble de coefficients \(\boldsymbol \theta=(\theta_{-p},\dots,\theta_{f})'\) qui transforme toute série temporelle \(X_t\) en :
\[
M_{\boldsymbol\theta}(X_t)=\sum_{k=-p}^{+f}\theta_kX_{t+k}.
\]
On a les définitions suivantes :

\begin{itemize}
\item
  La quantité \(p+f+1\) est appelée \emph{ordre de la moyenne mobile}.
\item
  Lorsque \(p=f\) la moyenne mobile est dite \emph{centrée}.
  Si de plus on a \(\forall k:\:\theta_{-k} = \theta_k\), la moyenne mobile \(M_{\boldsymbol\theta}\) est dite \emph{symétrique}.
  Dans ce cas, la quantité \(p=f\) est appelée \emph{fenêtre} (\emph{bandwidth}).
\end{itemize}

\hypertarget{subsec:gain-deph}{%
\subsection{Fonctions de gain et de déphasage}\label{subsec:gain-deph}}

Pour interpréter les notions de gain et de déphasage, il est utile d'illustrer les effets des moyennes mobiles sur les séries harmoniques \(X_t=\e^{-i\omega t}\) avec \(\omega\in[0,\pi]\).
La moyenne mobile \(M_{\boldsymbol\theta}\) transforme \(X_t\) en :
\[
Y_t = M_{\boldsymbol\theta}X_t = \sum_{k=-p}^{+f} \theta_k \e^{-i \omega (t+k)}
= \left(\sum_{k=-p}^{+f} \theta_k \e^{-i \omega k}\right)\cdot X_t.
\]
La fonction \(\Gamma_{\boldsymbol\theta}(\omega)=\sum_{k=-p}^{+f} \theta_k e^{-i \omega k}\) est appelée \emph{fonction de transfert} ou \emph{fonction de réponse en fréquence} (\emph{frequency response function})\footnote{
  La fonction de transfert peut être définie de manière équivalente par \(\Gamma_{\boldsymbol\theta}(\omega)=\sum_{k=-p}^{+f} \theta_k e^{i \omega k}\) ou \(\Gamma_{\boldsymbol\theta}(\omega)=\sum_{k=-p}^{+f} \theta_k e^{2\pi i \omega k}\).}.
Elle peut être réécrite en :
\[
\Gamma_{\boldsymbol\theta}(\omega) = \rho_{\boldsymbol\theta}(\omega)\e^{i\varphi_{\boldsymbol\theta}(\omega)},
\]
où \(\rho_{\boldsymbol\theta}(\omega)=G_{\boldsymbol\theta}(\omega)=\lvert\Gamma_{\boldsymbol\theta}(\omega)\rvert\) est la fonction de \emph{gain} ou \emph{d'amplitude} et \(\varphi_{\boldsymbol\theta}(\omega)\) est le \emph{déphasage} (\emph{phase shift} ou \emph{time shift})\footnote{
  Cette fonction est parfois définie comme \(\phi_{\boldsymbol\theta}(\omega)=\frac{\varphi_{\boldsymbol\theta}(\omega)}{\omega}\) pour mesurer le déphasage en termes de période.}.
Pour tous les filtres symétriques on a \(\varphi_{\boldsymbol\theta}(\omega)\equiv 0 \;(mod\;{\pi})\).
En effet, la contribution du couple de termes d'ordre \(k>0\) s'écrit \(\theta_k\e^{i\omega k}+\theta_k\e^{-i\omega k} = 2\theta_k\cos (\omega k),\) sa partie imaginaire est donc nulle.
Le terme d'ordre \(0\) étant \(\theta_0,\) la partie imaginaire de la fonction \(\Gamma_{\boldsymbol\theta}(\omega)\) est nulle pour tout \(\omega\in\R\) et donc \(\varphi_{\boldsymbol\theta}(\omega)\equiv 0 \;(mod\;{\pi}).\)

En somme, appliquer une moyenne mobile à une série harmonique (de la forme \(X_t=\e^{-i\omega t}\)) la modifie de deux façons :

\begin{itemize}
\item
  en la multipliant par un coefficient égal à \(\rho_{\boldsymbol\theta}\left(\omega\right)\) (gain) ;
\item
  en la « décalant » dans le temps de \(\varphi_{\boldsymbol\theta}(\omega)/\omega\), ce qui a un impact sur la détection des points de retournement (déphasage) \footnote{
    Lorsque \(\varphi_{\boldsymbol\theta}(\omega)/\omega<0\) le déphasage est négatif : le point de retournement est détecté avec retard.}.
\end{itemize}

La décomposition de Fourier permet d'analyser toute série temporelle (stationnaire ou stationnaire autour d'une tendance) comme une somme de séries harmoniques et chaque composante (tendance, cycle, saisonnalité, irrégulier) est associée à un ensemble de fréquences.
En effet, en notant \(\omega = 2\pi/p\), la série harmonique de fréquence \(\omega\) représente une série qui se répète toutes les \(p\) périodes.
Par exemple, pour une série mensuelle (12 observations par an), les mouvement saisonniers sont ceux qui se répètent chaque année : ils sont donc associés aux fréquences \(2\pi/12\) (périodicité annuelle), \(2\pi/12\times 2=2\pi/6,\dots,2\pi/12\times 5.\)
Nous considérons que la tendance-cycle est associée aux fréquences dans l'intervalle \([0, 2\pi/12[\), c'est-à-dire aux mouvements se répétant au moins tous les 12 mois\footnote{
  Même si des fréquences différentes sont parfois retenues (par exemple \([0, 2\pi/36]\) pour ne considérer que les cycles d'au moins 36 mois), cela n'a pas d'impact sur les différentes simulations.}.
Les autres fréquences, \([2\pi/12, \pi],\) sont ici associées à l'irrégulier (oscillations indésirables).

La figure \ref{fig:graphsmusgrave} montre la fonction de gain et de déphasage pour le filtre asymétrique de Musgrave (voir section \ref{subsec-lppasymf}) souvent utilisé pour l'estimation en temps-réel de la tendance-cycle (c'est-à-dire lorsqu'aucun point dans le futur n'est connu).
La fonction de gain est supérieure ou égale à 1 sur les fréquences associées à la tendance-cycle (\([0, 2\pi/12]\)) : cela signifie que la tendance-cycle est bien conservée et que les cycles courts de 1 à 2 ans (\([2\pi/24, 2\pi/12]\)) sont mêmes amplifiés.
En revanche, les cycles de 8 à 12 mois (\([2\pi/12, 2\pi/8]\)), considérés comme indésirables car associés à l'irrégulier, sont également amplifiés : cela peut engendrer la détection de faux points de retournement.
Sur les autres fréquences, la fonction de gain est inférieure à 1 mais toujours positive : cela signifie que la série lissée par cette moyenne mobile contiendra toujours du bruit, même si celui-ci est atténué.
L'analyse du déphasage montre que le déphasage est d'autant plus élevé que les cycles sont courts : cela signifie que sur les séries lissées par cette moyenne mobile, les points de retournement pourraient être détectés à la mauvaise date.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{img/filters_used/musgrave} 

}

\caption[Coefficients, fonction de gain et de déphasage pour le filtre de Musgrave en temps réel avec \(I/C=3,5\)]{Coefficients, fonction de gain et de déphasage pour le filtre de Musgrave en temps réel avec \(I/C=3,5\).}\label{fig:graphsmusgrave}

\footnotesize
\normalsize\end{figure}

\hypertarget{propriuxe9tuxe9s-souhaitables-dune-moyenne-mobile}{%
\subsection{Propriétés souhaitables d'une moyenne mobile}\label{propriuxe9tuxe9s-souhaitables-dune-moyenne-mobile}}

Pour décomposer une série temporelle en une composante saisonnière, une tendance-cycle et l'irrégulier, l'algorithme de décomposition X-11 (utilisé dans X-13ARIMA) utilise une succession de moyennes mobiles ayant toutes des contraintes spécifiques.

Dans notre cas, on suppose que notre série initiale \(y_t\) est désaisonnalisée et peut s'écrire comme la somme d'une tendance-cycle, \(TC_t\), et d'une composante irrégulière, \(I_t\) :
\[
y_t=TC_t+I_t.
\]

L'objectif va notamment être de construire des moyennes mobiles préservant au mieux la tendance-cycle (\(M_{\boldsymbol\theta} (TC_t)\simeq TC_t\)) et réduisant au maximum le bruit (\(M_{\boldsymbol\theta} (I_t)\simeq 0\)).

\hypertarget{pruxe9servation-de-tendances}{%
\subsubsection{Préservation de tendances}\label{pruxe9servation-de-tendances}}

Les tendances-cycles sont généralement modélisés par des tendances polynomiales locales (voir section \ref{sec-nonparamreg}).
Afin de conserver au mieux les tendances-cycles, on cherche à avoir des moyennes mobiles qui conservent les tendances polynomiales.
Une moyenne mobile \(M_{\boldsymbol\theta}\) conserve une fonction du temps \(f(t)\) si \(\forall t:\:M_{\boldsymbol\theta} f(t)=f(t)\).

Nous avons les propriétés suivantes pour la moyenne mobile \(M_{\boldsymbol\theta}\)~:

\begin{itemize}
\item
  Pour conserver les constantes \(X_t=a\) il faut :
  \[
  \forall t:M_{\boldsymbol\theta}(X_t)=\sum_{k=-p}^{+f}\theta_kX_{t+k}=\sum_{k=-p}^{+f}\theta_ka=a\sum_{k=-p}^{+f}\theta_k=a.
  \]
  C'est-à-dire qu'il faut que la somme des coefficients \(\sum_{k=-p}^{+f}\theta_k\) soit égale à \(1\).
\item
  Pour conserver les tendances linéaires \(X_t=at+b\) il faut :
  \[
  \forall t:\:M_{\boldsymbol\theta}(X_t)=\sum_{k=-p}^{+f}\theta_kX_{t+k}=\sum_{k=-p}^{+f}\theta_k[a(t+k)+b]=a\sum_{k=-p}^{+f}k\theta_k+(at+b)\sum_{k=-p}^{+f}\theta_k=at+b.
  \]
  Ce qui est équivalent à :
  \[
  \sum_{k=-p}^{+f}\theta_k=1
  \quad\text{et}\quad
  \sum_{k=-p}^{+f}k\theta_k=0.
  \]
\item
  De manière générale, \(M_{\boldsymbol\theta}\) conserves les tendances de degré \(d\) si et seulement si :
  \[
  \sum_{k=-p}^{+f}\theta_k=1
   \text{ et }
  \forall j \in \left\llbracket 1,d\right\rrbracket:\:
  \sum_{k=-p}^{+f}k^j\theta_k=0.
  \]
\item
  Si \(M_{\boldsymbol\theta}\) est symétrique (\(p=f\) et \(\theta_{-k} = \theta_k\) pour tout \(k\)) et conserve les tendances de degré \(2d\) alors elle conserve aussi les tendances de degré \(2d+1\) car :
  \[
  \sum_{k=-p}^{+p}k^{2d+1}\theta_k= 0\times\theta_0+ \sum_{k=1}^{+p}k^{2d+1}\theta_k +\sum_{k=1}^{+p}(-1)\times k^{2d+1}\underbrace{\theta_{-k}}_{=\theta_k}=0.
  \]
\end{itemize}

\hypertarget{ruxe9duction-du-bruit}{%
\subsubsection{Réduction du bruit}\label{ruxe9duction-du-bruit}}

Toutes les séries temporelles sont affectées par du bruit qui peut brouiller l'extraction de la tendance et du cycle.
C'est pourquoi on cherche à réduire ce bruit (en réduisant sa variance) tout en conservant les évolutions pertinentes.
La somme des carrés des coefficients \(\sum_{k=-p}^{+f}\theta_k^2,\) généralement inférieure ou égale à 1, est le rapport de \emph{réduction de la variance}.

En effet, soit \(\{\varepsilon_t\}\) une suite de variables aléatoires indépendantes avec \(\E{\varepsilon_t}=0\), \(\V{\varepsilon_t}=\sigma^2\).
On a :
\[
\V{M_{\boldsymbol\theta}\varepsilon_t}=\V{\sum_{k=-p}^{+f} \theta_k \varepsilon_{t+k}}
= \sum_{k=-p}^{+f} \theta_k^2 \V{\varepsilon_{t+k}}=
\sigma^2\sum_{k=-p}^{+f} \theta_k^2.
\]

\hypertarget{sec-mmasym}{%
\subsection{Estimation en temps réel et moyennes mobiles asymétriques}\label{sec-mmasym}}

\hypertarget{subec:mmetprev}{%
\subsubsection{Moyennes mobiles asymétriques et prévision}\label{subec:mmetprev}}

Pour les filtres symétriques, la fonction de déphasage est égale à zéro (modulo \(\pi\)).
Il n'y a donc aucun retard dans la détection de points de retournement.
Du fait du manque de données disponibles, ils ne peuvent toutefois pas être utilisés au début et à la fin de la série.

Pour l'estimation en temps réel, plusieurs approches peuvent être utilisées :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Utiliser des moyennes mobiles asymétriques pour prendre en compte le manque de données disponibles ;
\item
  Appliquer les filtres symétriques sur les séries prolongées par prévision.
  Cette méthode semble remonter à \textcite{deforest1877adjustment} qui suggère également de modéliser en fin de période une tendance polynomiale de degré au plus trois\footnote{
    «~\emph{As the first \(m\) and last \(m\) terms of the series cannot be reached directly by the formula, the series should be graphically extended by m terms at both ends, first plotting the observations on paper as ordinates, and then extending the curve along what seems to be its probable course, and measuring the ordinates of the extended portions.}
    \emph{It is not necessary that this extension should coincide with what would be the true course of the curve in those parts. }
    \emph{The important point is that the m terms thus added, taken together with the \(m+1\) adjacent given terms, should follow a curve whose form is approximately algebraic and of a degree not higher than the third.}~»}.
  C'est également l'approche utilisée dans la méthode de désaisonnalisation X-13ARIMA qui prolonge la série sur 1 an par un modèle ARIMA.
\end{enumerate}

In fine, la seconde méthode revient à utiliser des moyennes mobiles asymétriques puisque les prévisions sont des combinaisons linéaires du passé.

Inversement, à partir d'une moyenne mobile symétrique de référence, on peut déduire les \textbf{prévisions implicites} d'une moyenne mobile asymétrique.
Cela permet notamment de juger de la qualité des estimations en temps réel de la tendance-cycle et d'anticiper les futures révisions lorsque les prévisions sont éloignées des évolutions attendues.\\
Notons \(\boldsymbol v=(v_{-h},\dots, v_{h})\) la moyenne mobile symétrique de référence et \(\boldsymbol w^0,\dots \boldsymbol w^{h-1}\) une suite de moyennes mobiles asymétriques, d'ordre \(h+1\) à \(2h\) utilisée pour l'estimation des \(h\) derniers points avec, pour convention, \(w_t^q=0\) pour \(t>q\).
C'est-à-dire que \(\boldsymbol w^0=(w_{-h}^0,\dots, w_{0}^0)\) est utilisée pour l'estimation en temps réel (lorsqu'on ne connaît aucun point dans le futur), \(\boldsymbol w^1=(w_{-h}^1,\dots, w_{1}^1)\) pour l'estimation de l'avant-dernier point (lorsqu'on ne connaît qu'un point dans le futur), etc.
Notons également \(y_{-h},\dots,y_{0}\) la série étudiée observée et \(y_{1}^*,\dots y_h^*\) la prévision implicite induite par \(\boldsymbol w^0,\dots \boldsymbol w^{h-1}\).
Cela signifie, que pour tout \(q\) on a :
\[
\forall q, \quad \underbrace{\sum_{i=-h}^0 v_iy_i + \sum_{i=1}^h v_iy_i^*}_{\text{lissage par }v\text{ de la série prolongée}}
=\underbrace{\sum_{i=-h}^0 w_i^qy_i + \sum_{i=1}^h w_i^qy_i^*}_{\text{lissage par }w^q\text{ de la série prolongée}}.
\]
Ce qui est équivalent à :
\[
\forall q, \quad \sum_{i=1}^h (v_i- w_i^q) y_i^*
=\sum_{i=-h}^0 (w_i^q-v_i)y_i.
\]
En somme, matriciellement, cela revient donc à résoudre :
\[\scriptstyle
\begin{pmatrix}
  v_1 & v_2 & \cdots & v_h \\
  v_1 - w_1^1 & v_2 & \cdots & v_h \\
  \vdots & \vdots & \cdots & \vdots \\
   v_1 - w_1^{h-1} & v_2-w_2^{h-1} & \cdots & v_h
\end{pmatrix}
\begin{pmatrix}y_1^* \\ \vdots \\ y_h^*\end{pmatrix}=
\begin{pmatrix}
  w_{-h}^0 - v_{-h} & w_{-(h-1)}^0 - v_{-(h-1)} & \cdots & w_{0}^0 - v_{0} \\
  w_{-h}^1 - v_{-h} & w_{-(h-1)}^1 - v_{-(h-1)} & \cdots & w_{0}^1 - v_{0} \\
  \vdots & \vdots & \cdots & \vdots \\
  w_{-h}^{h-1} - v_{-h} & w_{-(h-1)}^{h-1} - v_{-(h-1)} & \cdots & w_{0}^{h-1} - v_{0}
\end{pmatrix}
\begin{pmatrix}y_{-h} \\ \vdots \\ y_0\end{pmatrix}.\]
C'est ce qui implémenté dans la fonction \texttt{rjd3filters::implicit\_forecast()}.

Comme notamment souligné par \textcite{wildischis2004}, étendre la série par prévision d'un modèle ARIMA revient à calculer des filtres asymétriques dont les coefficients sont optimisés par rapport à la prévision à horizon d'une période --- \emph{one-step ahead forecasting}.
Autrement dit, on cherche à minimiser les révisions entre la première et la dernière estimation (avec le filtre symétrique).
Cependant, le déphasage induit par les filtres asymétriques n'est pas contrôlé : on pourrait préférer avoir une détection plus rapide des points de retournement et une révision plus grande plutôt que de juste minimiser les révisions entre la première et la dernière estimation.
Par ailleurs, puisque les coefficients du filtre symétrique (et donc le poids associé aux prévisions lointaines) décroissent lentement, il faudrait également s'intéresser à la performance des prévisions à horizon de plusieurs périodes --- \emph{multi-step ahead forecasting}.
C'est pourquoi il peut être nécessaire de définir des critères alternatifs pour juger la qualité des moyennes mobiles asymétriques.

\hypertarget{subsec:crit-qual}{%
\subsubsection{Indicateurs de qualité des moyennes mobiles asymétriques}\label{subsec:crit-qual}}

Pour les filtres asymétriques, la majorité des critères proviennent de ceux définis par \textcite{ch15HBSA} et \textcite{trilemmaWMR2019} pour construire les filtres asymétriques.
Ils sont résumés dans le tableau \ref{tab:QC} et calculables avec la fonction \texttt{rjd3filters::diagnostic\_matrix()}.

\textcite{ch15HBSA} proposent une approche générale pour dériver des filtres linéaires, fondée sur un problème d'optimisation de trois critères : \emph{Fidelity} (\(F_g\), Fidélité), \emph{Smoothness} (\(S_g\), lissage) et \emph{Timeliness} (\(T_g\), rapidité).
La fidélité peut être directement reliée à la réduction de variance créée par le filtre et la rapidité à la notion de déphasage, qu'on souhaite là encore faible.

\textcite{trilemmaWMR2019} proposent une approche qui s'appuie sur la décomposition de l'erreur quadratique moyenne entre le filtre symétrique et le filtre asymétrique en quatre quantités~: \emph{Accuracy} (\(A_w\), précision), \emph{Timeliness} (\(T_w\), rapidité), \emph{Smoothness} (\(S_w\), lissage) et \emph{Residual} (\(R_w\), résidus).
Voir section \ref{sec-WildiMcLeroy} pour plus de détails.

\begin{table}

\caption{\label{tab:QC}Critères de qualité d'une moyenne mobile $\boldsymbol\theta=(\theta_k)_{-p\leq k\leq f}$ définie par une fonction de gain $\rho_{\boldsymbol\theta}$ et une fonction de déphasage $\varphi_{\boldsymbol\theta}$.}
{
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{ccc}
\toprule
Sigle & Description & Formule\\
\midrule
$b_c$ & Biais constant & $\sum_{k=-p}^{+f}\theta_{k}-1$\\
$b_l$ & Biais linéaire & $\sum_{k=-p}^{+f}k\theta_{k}$\\
$b_q$ & Biais quadratique & $\sum_{k=-p}^{+f}k^{2}\theta_{k}$\\
$F_g$ & Réduction de la variance / Fidelity (Guggemos) & $\sum_{k=-p}^{+f}\theta_{k}^{2}$\\
$S_g$ & Smoothness (Guggemos) & $\sum_{j}(\nabla^{3}\theta_{j})^{2}$\\
\addlinespace
$T_g$ & Timeliness (Guggemos) & $\int_{0}^{\omega_1}\rho_{\boldsymbol\theta}(\omega)\sin(\varphi_{\boldsymbol\theta}(\omega))^{2}\ud\omega$\\
$A_w$ & Accuracy (Wildi) & $2\int_0^{\omega_1}\left(\rho_{s}(\omega)-\rho_{\boldsymbol\theta}(\omega)\right)^{2}h(\omega)\ud\omega$\\
$T_w$ & Timeliness (Wildi) & $8\int_0^{\omega_1} \rho_{s}(\omega)\rho_{\boldsymbol\theta}(\omega)\sin^{2}\left(\frac{\varphi_s(\omega)-\varphi_{\boldsymbol\theta}(\omega)}{2}\right)h(\omega)\ud\omega$\\
$S_w$ & Smoothness (Wildi) & $2\int_{\omega_1}^{\pi}\left(\rho_{s}(\omega)-\rho_{\boldsymbol\theta}(\omega)\right)^{2}h(\omega)\ud\omega$\\
$R_w$ & Residual (Wildi) & $8\int_{\omega_1}^{\pi} \rho_{s}(\omega)\rho_{\boldsymbol\theta}(\omega)\sin^{2}\left(\frac{\varphi_s(\omega)-\varphi_{\boldsymbol\theta}(\omega)}{2}\right)h(\omega)\ud\omega$\\
\bottomrule
\end{tabular}}
}
\footnotesize


\emph{Notes} : \emph{\(X_g\) critères provenant de \textcite{ch15HBSA} et \(X_w\) critères provenant de \textcite{trilemmaWMR2019}.}

\emph{\(\rho_s\) et \(\varphi_s\) représentent le gain et la fonction de déphasage du filtre symétrique d'Henderson.}

\emph{\(h\) est la densité spectrale de la série en entrée, considérée comme étant celle d'un bruit blanc (\(h_{WN}(x)=1\)) ou d'une marche aléatoire (\(h_{RW}(x)=\frac{1}{2(1-\cos(x))}\)).}
\normalsize\end{table}

\hypertarget{subsec-formulegen}{%
\subsubsection{Formule générale de construction des moyennes mobiles}\label{subsec-formulegen}}

Toutes les moyennes mobiles et tous les critères de qualité étudiés peuvent se voir comme des cas particuliers d'une formule générale de construction des filtres (symétriques et asymétriques).
Celle-ci est décrite dans l'annexe~\ref{an-diag}, qui montre également les liens entre les différentes méthodes.
Tous les filtres utilisés dans les simulations sont résumés dans l'annexe \ref{an-graphs}.

\hypertarget{illustration}{%
\subsubsection{Illustration}\label{illustration}}

Les différentes méthodes de construction de moyennes mobiles asymétriques seront illustrées à partir de l'exemple du lissage du climat des affaires dans le secteur des matériels de transport (C4), publié par l'Insee\footnote{
  \url{https://bdm.insee.fr/series/sdmx/data/SERIES_BDM/001786505}.}, jusqu'en mai 2023.

L'indicateur synthétique du climat des affaires résume l'opinion des chefs d'entreprise sur la conjoncture du secteur associé (ici les matériels de transport) : plus sa valeur est élevée, plus les industriels considèrent que la conjoncture est favorable.
Il est calculé à partir de soldes d'opinion issus des enquêtes de conjoncture de l'Insee.
Toutefois, du fait de la volatilité des soldes d'opinion utilisés, les climats des affaires peuvent être bruités, ce qui peut rendre difficile l'identification des périodes de retournement conjoncturel (voir figure \ref{fig:graphs-ex-tcfin}), c'est-à-dire les périodes où les industriels considèrent que la conjoncture passe d'un état favorable à un état moins favorable, ou l'inverse.
Les méthodes présentées dans cette étude permettent de lisser les séries afin d'extraire la composante tendance-cycle pour faciliter l'analyse des retournements conjoncturels dans le cycle classique (également appelé cycle des affaires).
On ne cherche donc pas à estimer séparément la tendance (qui représente les évolutions de long terme) et le cycle (qui représente les évolutions cycliques autour de la tendance) puisque ces composantes peuvent être difficiles à séparer (quelle définition prendre pour distinguer les cycles courts et des cycles longs ?).
Par ailleurs, la décomposition entre tendance et cycle, associée à des méthodes de filtrage de type Hodrick-Prescott ou Baxter-King, sont plutôt utilisées pour analyser les points de retournement dans le cycle de croissance (voir \textcite{ferrara2009caracterisationcycles} pour une description des différents cycles économiques).
Ces méthodes de décomposition tendance et cycle nécessitent d'avoir des séries longues (afin de distinguer cycles courts et tendance de long terme) alors que les méthodes utilisées dans cet article utilisent peu d'observations (en général 13 pour des séries mensuelles).

La figure \ref{fig:graphs-ex-tcfin} montre la série initiale et la série lissée avec le filtre symétrique d'Henderson (présenté dans la section \ref{sec-nonparamreg}), qui sera utilisé pour les estimations finales de la tendance-cycle (c'est-à-dire lorsque suffisamment d'observations sont disponibles afin de permettre son utilisation).
La série lissée permet, par construction, de localiser plus facilement les points de retournement et d'interpréter les évolutions mensuelles de l'indicateur.
L'objectif de cette étude est de comparer différentes méthodes pour estimer les estimations intermédiaires des derniers points de la série lissée, avant que les points futurs ne soient disponibles.
L'annexe \ref{ann-ex-r} contient le code utilisé pour cet exemple illustratif.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{img/ex/tc_finale} 

}

\caption[Série initiale et lissée, avec le filtre symétrique d'Henderson, du climat des affaires dans les matériels de transport]{Série initiale et lissée, avec le filtre symétrique d'Henderson, du climat des affaires dans les matériels de transport.}\label{fig:graphs-ex-tcfin}

\footnotesize
\normalsize\end{figure}

\hypertarget{sec-nonparamreg}{%
\section{Régression non paramétrique et régression polynomiale locale}\label{sec-nonparamreg}}

De nombreuses méthodes d'extraction de la tendance-cycle sont fondées sur des régressions non-paramétriques, particulièrement souples car elles ne supposent pas de dépendance prédéterminée dans les prédicteurs. En pratique, on peut s'appuyer sur des régressions locales.
Plus précisément, si on considère un ensemble de points \((x_i,y_i)_{1\leq i\leq n},\) la régression non paramétrique consiste à supposer qu'il existe une fonction \(\mu\), à estimer, telle que \(y_i=\mu(x_i)+\varepsilon_i\) avec \(\varepsilon_i\) un terme d'erreur.
D'après le théorème de Taylor, pour tout point \(x_0\), si \(\mu\) est différentiable \(d\) fois, alors :
\begin{equation}
\forall x \::\:\mu(x) = \mu(x_0) + \mu'(x_0)(x-x_0)+\dots +
\frac{\mu^{(d)}(x_0)}{d!}(x-x_0)^d+R_d(x), \label{eq:taylor}
\end{equation}
où \(R_d\) est un terme résiduel négligeable au voisinage de \(x_0\).
Dans un voisinage \(h(x_0)\) autour de \(x_0\), \(\mu\) peut être approchée par un polynôme de degré \(d\).
La quantité \(h(x_0)\) est appelée \emph{fenêtre} (\emph{bandwidth}).
Si \(\varepsilon_i\) est un bruit blanc, on peut donc estimer par les moindres carrés \(\mu(x_0)\) en utilisant les observations qui sont dans \(\left[x_0-h(x_0),x_0+h(x_0)\right]\).

En pratique, cela revient donc à faire l'hypothèse que la tendance est localement polynomiale. Différentes méthodes d'estimation peuvent être utilisées pour en déduire des moyennes mobiles symétriques et asymétriques.\\
\textcite{GrayThomson1996} (section \ref{subsec-graythomson}) proposent un cadre statistique complet permettant notamment de modéliser l'erreur d'approximation de la tendance par des polynômes locaux.
Toutefois, la spécification de cette erreur étant en général complexe, des modélisations plus simples peuvent être préférées, comme celle de \textcite{proietti2008} (section \ref{sec-proietti}).\\
Enfin, \textcite{dagumbianconcini2008} (section \ref{sec-rkhs}) proposent une modélisation similaire de la tendance-cycle mais utilisant la théorie des espaces de Hilbert à noyau reproduisant pour l'estimation, ce qui a notamment l'avantage de faciliter le calcul des différentes moyennes mobiles à différentes fréquences temporelles.

Les équivalences entre ces différentes méthodes sont présentées dans l'annexe \ref{an-diag}.

\hypertarget{sec-proietti}{%
\subsection{Régression polynomiale : approche de Proietti et Luati}\label{sec-proietti}}

\begin{summary_box}{Filtres locaux polynomiaux --- Proietti et Luati (2008)}

Approche fondée sur la modélisation locale de la tendance-cycle par des polynômes :

\begin{itemize}
\item
  Modèles avec une interprétation simple.
\item
  Le filtre asymétrique est indépendant de la date d'estimation.
  Toutefois, il dépend indirectement des données car il est généralement calibré en utilisant les données.
\item
  La \emph{timeliness} n'est pas contrôlée (mais peut être introduite dans le programme de minimisation).
\end{itemize}

\end{summary_box}

\hypertarget{filtres-symuxe9triques}{%
\subsubsection{Filtres symétriques}\label{filtres-symuxe9triques}}

En reprenant les notations de \textcite{proietti2008}, nous supposons que notre série temporelle \(y_t\) peut être décomposée en
\[
y_t=\mu_t+\varepsilon_t,
\]
où \(\mu_t\) est la tendance et \(\varepsilon_{t}\overset{i.i.d}{\sim}\mathcal{N}(0,\sigma^{2})\) est le bruit\footnote{La série est donc désaisonnalisée.}.
La tendance \(\mu_t\) est localement approchée par un polynôme de degré \(d\), de sorte que dans un voisinage \(h\) de \(t\) \(\mu_t\simeq m_{t}\) avec :
\[
\forall j\in\left\llbracket -h,h\right\rrbracket :\:
y_{t+j}=m_{t+j}+\varepsilon_{t+j},\quad m_{t+j}=\sum_{i=0}^{d}\beta_{i}j^{i}.
\]
Le problème d'extraction de la tendance est équivalent à l'estimation de \(m_t=\beta_0\) (la constante dans la formule précédente).

En notation matricielle :
\[
\underbrace{\begin{pmatrix}y_{t-h}\\
y_{t-(h-1)}\\
\vdots\\
y_{t}\\
\vdots\\
y_{t+(h-1)}\\
y_{t+h}
\end{pmatrix}}_{\boldsymbol y}=\underbrace{\begin{pmatrix}1 & -h & h^{2} & \cdots & (-h)^{d}\\
1 & -(h-1) & (h-1)^{2} & \cdots & (-(h-1))^{d}\\
\vdots & \vdots & \vdots & \cdots & \vdots\\
1 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \cdots & \vdots\\
1 & h-1 & (h-1)^{2} & \cdots & (h-1)^{d}\\
1 & h & h^{2} & \cdots & h^{d}
\end{pmatrix}}_{\boldsymbol X}\underbrace{\begin{pmatrix}\beta_{0}\\
\beta_{1}\\
\vdots\\
\vdots\\
\vdots\\
\vdots\\
\beta_{d}
\end{pmatrix}}_{\boldsymbol \beta}+\underbrace{\begin{pmatrix}\varepsilon_{t-h}\\
\varepsilon_{t-(h-1)}\\
\vdots\\
\varepsilon_{t}\\
\vdots\\
\varepsilon_{t+(h-1)}\\
\varepsilon_{t+h}
\end{pmatrix}}_{\boldsymbol \varepsilon}.
\]

Pour estimer \(\beta\) il faut \(2h+1\geq d+1\) et l'estimation est faite par moindres carrés pondérés --- \emph{weighted least squares} (WLS) ---, ce qui revient à minimiser la fonction objectif suivante :
\[
S(\hat{\beta}_{0},\dots,\hat{\beta}_{d})=\sum_{j=-h}^{h}\kappa_{j}(y_{t+j}-\hat{\beta}_{0}-\hat{\beta}_{1}j-\dots-\hat{\beta}_{d}j^{d})^{2}.
\]
où \(\kappa_j\) est un ensemble de poids appelés \emph{noyaux} (\emph{kernel}).
On a \(\kappa_j\geq 0:\kappa_{-j}=\kappa_j\), et en notant \(\boldsymbol K=diag(\kappa_{-h},\dots,\kappa_{h})\), l'estimateur \(\boldsymbol \beta\) peut s'écrire \(\hat{\boldsymbol \beta}=(\boldsymbol X'\boldsymbol K\boldsymbol X)^{-1}\boldsymbol X'\boldsymbol K\boldsymbol y\).
Avec \(\boldsymbol e_{1}=\begin{pmatrix}1&0&\cdots&0\end{pmatrix}'\), l'estimateur de la tendance peut donc s'écrire :
\begin{equation}
\hat{m}_{t}=\boldsymbol e_{1}'\hat{\boldsymbol \beta}=\boldsymbol \theta'\boldsymbol y=\sum_{j=-h}^{h}\theta_{j}y_{t-j}\text{ avec }\boldsymbol \theta=\boldsymbol K\boldsymbol X(\boldsymbol X'\boldsymbol K\boldsymbol X)^{-1}\boldsymbol e_{1}.
\label{eq:mmsym}
\end{equation}
En somme, l'estimation de la tendance \(\hat{m}_{t}\) est obtenue en appliquant une moyenne mobile symétrique \(\boldsymbol \theta\) à \(y_t\)\footnote{
  \(\boldsymbol \theta\) est symétrique du fait de la symétrie des noyaux \(\kappa_j\).}.
De plus, \(\boldsymbol X'\boldsymbol \theta=\boldsymbol e_{1}\) donc :
\[
\sum_{j=-h}^{h}\theta_{j}=1,\quad\forall r\in\left\llbracket 1,d\right\rrbracket :\sum_{j=-h}^{h}j^{r}\theta_{j}=0.
\]
Ainsi, la moyenne mobile \(\boldsymbol \theta\) préserve les polynômes de degré \(d\).

L'annexe \ref{an-noyaux} présente différentes formes de noyaux ainsi que des estimateurs classiques associés.

Concernant le choix des paramètres, l'idée générale qui prévaut est que le choix entre ces différents noyaux est secondaire\footnote{
  Voir par exemple \textcite{cleveland1996smoothing} ou \textcite{Loader1999}.
  Les seules contraintes souhaitées sur le noyau est qu'il accorde un poids plus important à l'estimation centrale (\(\kappa_0\)) et qu'il décroit vers 0 lorsqu'on s'éloigne de l'estimation centrale.
  Le noyau uniforme est donc à éviter.} et qu'il vaut mieux se concentrer sur deux autres paramètres :

\begin{itemize}
\item
  le degré du polynôme \(d\) : s'il est trop petit on risque d'avoir des estimations biaisées de la tendance-cycle et s'il est trop grand on risque d'avoir une trop grande variance dans les estimations (du fait d'un sur-ajustement) ;
\item
  le nombre de voisins \(2h+1\) (ou la fenêtre \(h\)) : s'il est trop petit alors trop peu de données seront utilisées pour les estimations (ce qui conduira à une grande variance dans les estimations) et s'il est trop grand alors l'approximation polynomiale sera vraisemblablement fausse ce qui conduira à avoir des estimations biaisées.
\end{itemize}

\hypertarget{subsec-lppasymf}{%
\subsubsection{Filtres asymétriques}\label{subsec-lppasymf}}

Comme mentionné dans la partie \ref{subec:mmetprev}, pour l'estimation en temps réel, plusieurs approches peuvent être utilisées :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Appliquer les filtres symétriques sur les séries prolongées par prévision \(\hat{y}_{n+l\mid n},l\in\left\llbracket 1,h\right\rrbracket\).
\item
  Construire un filtre asymétrique par approximation polynomiale locale sur les observations disponibles (\(y_{t}\) pour \(t\in\left\llbracket n-h,n\right\rrbracket\)).
\item
  Construire des filtres asymétriques qui minimisent l'erreur quadratique moyenne de révision sous des contraintes de reproduction de tendances polynomiales.
\end{enumerate}

\textcite{proietti2008} montrent que les deux premières approches sont équivalentes lorsque les prévisions sont faites par extrapolation polynomiale de degré \(d\).
Elles sont également équivalentes à la troisième approche sous les mêmes contraintes que celles du filtre symétrique.
La troisième méthode est appelée \emph{direct asymmetric filter} (DAF).
C'est cette méthode qui est utilisée pour l'estimation en temps réel dans la méthode de désaisonnalisation STL (\emph{Seasonal-Trend decomposition based on Loess}, voir \textcite{cleveland90}).
Même si les estimations des filtres DAF sont sans biais, c'est au coût d'une plus grande variance dans les estimations.

Pour résoudre le problème de la variance des estimations des filtres en temps réel, \textcite{proietti2008} proposent une méthode générale pour construire les filtres asymétriques qui permet de faire un compromis biais-variance.
Il s'agit d'une généralisation des filtres asymétriques de \textcite{musgrave1964set} (utilisés dans l'algorithme de désaisonnalisation X-13ARIMA).

On modélise ici la série initiale par :
\begin{equation}
\boldsymbol y=\boldsymbol U\boldsymbol \gamma+\boldsymbol Z\boldsymbol \delta+\boldsymbol \varepsilon,\quad
\boldsymbol \varepsilon\sim\mathcal{N}(\boldsymbol 0,\boldsymbol D).
\label{eq:lpgeneralmodel}
\end{equation}
où \([\boldsymbol U,\boldsymbol Z]\) est de rang plein et forme un sous-ensemble des colonnes de \(\boldsymbol X\).
L'objectif est de trouver un filtre \(\boldsymbol v\) qui minimise l'erreur quadratique moyenne de révision (au filtre symétrique \(\boldsymbol \theta\)) sous certaines contraintes.
Ces contraintes sont représentées par la matrice \(\boldsymbol U=\begin{pmatrix}\boldsymbol U_{p}'&\boldsymbol U_{f}'\end{pmatrix}'\) : \(\boldsymbol U_p'\boldsymbol v=\boldsymbol U'\boldsymbol \theta\) (avec \(\boldsymbol U_p\) la matrice \((h+q+1)\times (d+1)\) qui contient les observations de la matrice \(\boldsymbol U\) connues lors de l'estimation par le filtre asymétrique).
Le problème est équivalent à trouver \(\boldsymbol v\) qui minimise~:
\begin{equation}
\varphi(\boldsymbol v)=
\underbrace{
  \underbrace{(\boldsymbol v-\boldsymbol \theta_{p})'\boldsymbol D_{p}(\boldsymbol v-\boldsymbol \theta_{p})+
  \boldsymbol \theta_{f}'\boldsymbol D_{f}\boldsymbol \theta_{f}}_\text{variance de l'erreur de révision}+
  \underbrace{[\boldsymbol \delta'(\boldsymbol Z_{p}'\boldsymbol v-\boldsymbol Z'\boldsymbol \theta)]^{2}}_{biais^2}
}_\text{Erreur quadratique moyenne de révision}+
\underbrace{2\boldsymbol l'(\boldsymbol U_{p}'\boldsymbol v-\boldsymbol U'\boldsymbol \theta)}_{\text{contraintes}}.
\label{eq:lppasym}
\end{equation}
où \(\boldsymbol l\) est le vecteur des multiplicateurs de Lagrange.

Lorsque \(\boldsymbol U=\boldsymbol X\), la contrainte équivaut à préserver les polynômes de degré \(d\) : on retrouve les filtres directs asymétriques (DAF) lorsque \(\boldsymbol D=\boldsymbol K^{-1}\).

Lorsque \(\boldsymbol U=\begin{pmatrix}1&\cdots&1\end{pmatrix}'\), \(\boldsymbol Z=\begin{pmatrix}-h&\cdots&+h\end{pmatrix}'\), \(\boldsymbol \delta=\delta_1\), \(\boldsymbol D=\sigma^2\boldsymbol I\) et lorsque le filtre symétrique est le filtre d'Henderson, on retrouve les filtres asymétriques de Musgrave.
Ce filtre suppose que, pour l'estimation en temps réel, les données sont générées par un processus linéaire et que les filtres asymétriques préservent les constantes (\(\sum v_i=\sum \theta_i=1\)).
Ces filtres asymétriques dépendent du rapport \(\lvert\delta_1/\sigma\rvert\), qui est lié à l'I-C ratio \(R=\frac{\bar{I}}{\bar{C}}=\frac{\sum\lvert I_t-I_{t-1}\rvert}{\sum\lvert C_t-C_{t-1}\rvert}\) (et l'on a \(\delta_1/\sigma=2/(R\sqrt{\pi})\)), qui est notamment utilisé dans X-13ARIMA pour déterminer la longueur du filtre d'Henderson\footnote{
  Dans la majorité des cas un filtre de 13 termes est utilisé.
  Si le ratio est grand alors un filtre de 23 termes est utilisé (pour supprimer davantage de bruit) et si le ratio est petit un filtre de 9 termes est utilisé.}.

Lorsque \(\boldsymbol U\) correspond aux \(d^*+1\) premières colonnes de \(\boldsymbol X\), \(d^*<d\), la contrainte consiste à reproduire des tendances polynomiales de degré \(d^*\).
Cela introduit du biais mais réduit la variance.
Ainsi, \textcite{proietti2008} proposent trois classes de filtres asymétriques :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Linear-Constant} (LC) : \(y_t\) linéaire (\(d=1\)) et \(v\) préserve les constantes (\(d^*=0\)).
  On obtient le filtre de Musgrave avec le filtre d'Henderson comme filtre symétrique.
\item
  \emph{Quadratic-Linear} (QL) : \(y_t\) quadratique (\(d=2\)) et \(v\) préserve les tendances linéaires (\(d^*=1\)).
\item
  \emph{Cubic-Quadratic} (CQ) : \(y_t\) cubic (\(d=3\)) et \(v\) préserve les tendances quadratiques (\(d^*=2\)).
\end{enumerate}

Le tableau \ref{tab:criteriaLp} compare les critères de qualité des différentes méthodes en utilisant le filtre d'Henderson et \(h=6\) (filtre symétrique de 13 termes).
Pour les filtres en temps réel (\(q=0\)), plus le filtre asymétrique est complexe (en termes de préservation polynomiale), moins la \emph{timeliness} est élevée et plus la \emph{fidelity}/\emph{smoothness} est grande : la réduction du déphasage se fait au détriment d'une augmentation de la variance.
Ce résultat varie lorsque \(q\) augmente : pour \(q=2\) le filtre QL a une plus grande \emph{timeliness} que le filtre LC.
Ce résultat étonnant souligne le fait que le déphasage n'est pas contrôlé par l'approche de \textcite{proietti2008}.

En termes de révision, (\(A_w+S_w+T_w+R_w\)), les filtres LC et QL donnent toujours de meilleurs résultats que les filtres CQ et DAF.

Ces propriétés « théoriques » sont conformes aux résultats empiriques observés dans la section \ref{sec-comparison} : les révisions sont plus importantes pour les filtres CQ et DAF (ce qui conduit à plus de variabilité dans les estimations et à un délai plus grand dans la détection des points de retournement) ; et le filtre QL paramétrisé de manière globale (paramètre \(R\) fixé pour tous les filtres asymétriques) peut conduire à une détection plus tardive des points de retournement que le filtre LC\footnote{
  En effet, pour détecter un point de retournement à la date \(t\), il est nécessaire de connaître au moins 2 points après cette date afin de s'assurer qu'il y a bien un retournement de tendance.}.

\begin{table}[!h]

\caption{\label{tab:criteriaLp}Critères de qualité des filtres asymétriques ($q=0,1,2$) calculés par polynômes locaux en utilisant le noyau d'Henderson avec $h=6$ et $R=3,5$.}
{
\centering
\begin{tabular}[t]{cccccccccccc}
\toprule
Method & $b_c$ & $b_l$ & $b_q$ & $F_g$ & $S_g$ & $T_g \times 10^{-3}$ & $A_w$ & $S_w$ & $T_w$ & $R_w$ & $EQM_w$\\
\midrule
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{$q=0$}}\\
\hspace{1em}LC & 0 & -0,41 & -2,16 & 0,39 & 1,27 & 30,34 & 0,10 & 0,49 & 0,41 & 0,55 & 1,54\\
\hspace{1em}QL & 0 & 0,00 & -0,47 & 0,71 & 5,15 & 0,05 & 0,07 & 1,89 & 0,00 & 0,11 & 2,07\\
\hspace{1em}CQ & 0 & 0,00 & 0,00 & 0,91 & 11,94 & 0,01 & 0,02 & 2,23 & 0,00 & 0,10 & 2,35\\
\hspace{1em}DAF & 0 & 0,00 & 0,00 & 0,94 & 14,20 & 0,00 & 0,01 & 2,18 & 0,00 & 0,10 & 2,29\\
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{$q=1$}}\\
\hspace{1em}LC & 0 & -0,12 & -0,52 & 0,27 & 0,43 & 4,80 & 0,01 & 0,12 & 0,06 & 0,11 & 0,30\\
\hspace{1em}QL & 0 & 0,00 & -0,06 & 0,29 & 0,71 & 0,69 & 0,00 & 0,19 & 0,01 & 0,04 & 0,25\\
\hspace{1em}CQ & 0 & 0,00 & 0,00 & 0,37 & 0,57 & 0,16 & 0,02 & 0,58 & 0,00 & 0,06 & 0,66\\
\hspace{1em}DAF & 0 & 0,00 & 0,00 & 0,41 & 0,37 & 0,06 & 0,02 & 0,76 & 0,00 & 0,06 & 0,84\\
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{$q=2$}}\\
\hspace{1em}LC & 0 & 0,00 & 1,08 & 0,20 & 0,08 & 0,35 & 0,01 & 0,01 & 0,00 & 0,01 & 0,04\\
\hspace{1em}QL & 0 & 0,00 & 0,03 & 0,22 & 0,05 & 2,08 & 0,00 & 0,01 & 0,02 & 0,07 & 0,10\\
\hspace{1em}CQ & 0 & 0,00 & 0,00 & 0,37 & 0,66 & 0,13 & 0,02 & 0,56 & 0,00 & 0,06 & 0,64\\
\hspace{1em}DAF & 0 & 0,00 & 0,00 & 0,40 & 0,77 & 0,02 & 0,02 & 0,68 & 0,00 & 0,05 & 0,74\\
\bottomrule
\end{tabular}
}
\footnotesize


\emph{Note} : \emph{Avec \(EQM_w=A_w + S_w + T_w + R_w\).}


\emph{Note de lecture} : \emph{Dès qu'au moins deux points dans le futur sont connus (\(q=2\)), le filtre LC conserve sans biais les tendance linéaires (\(b_l = 0\)). Pour ce filtre, dès lors que l'on utilise au moins deux points dans le futur (\(q=2\)), l'erreur d'estimation provenant de l'utilisation d'un filtre asymétrique plutôt que symétrique (estimation finale) est quasiment nulle (\(EQM_w\simeq 0\)). Dès qu'au moins un point dans le futur est connu (\(q=1\)), le filtre QL conserve quasiment sans biais les tendances quadratiques (\(b_q\simeq 0\)). Pour ce filtre, la \emph{timeliness} est plus élevée lorsque deux points dans le futur sont connus (\(q=2\)) que pour l'estimation en temps réel (\(q=0\) ; \(T_g\) et \(T_w\) augmentent).}
\normalsize\end{table}

Une application en ligne, disponible à l'adresse \url{https://aqlt.shinyapps.io/FiltersProperties/}, permet de comparer les coefficients, les fonctions de gain et de déphasage entre les différentes méthodes et les différents noyaux.

Le graphique \ref{fig:graphs-ex-lp-es} montre les estimations successives de la série lissée du climat des affaires avec les filtres polynomiaux (paramétré à partir de l'IC-ratio).
C'est-à-dire qu'il trace la tendance-cycle estimée en utilisant les données observées jusqu'en novembre 2022, celle estimée en utilisant les données observées jusqu'en décembre 2022, etc.
Le graphique \ref{fig:graphs-ex-lp-if}\footnote{
  Afin qu'ils soient tous visibles, l'axe des ordonnées n'est pas le même entre les différents graphiques.} montre les prévisions implicites associées à ces différentes estimations successives : il s'agit des prévisions de la série brute qui, en appliquant le filtre symétrique sur la série prolongée, permettent d'obtenir la même tendance-cycle qu'en utilisant les moyennes mobiles asymétriques (voir section \ref{subec:mmetprev}).\\
Sur cette série et sur ces points, c'est le filtre LC qui donne les meilleurs résultats avec des prévisions implicites naïves (prolongement par une droite) mais plus cohérentes que celles des autres filtres.
Les filtres CQ et DAF conduisent à des estimations en temps-réel très éloignées des dernières estimations.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/ex/lp_es} 

}

\caption[Estimations successives de la série lissée du climat des affaires dans les matériels de transport avec des filtres polynomiaux locaux]{Estimations successives de la série lissée du climat des affaires dans les matériels de transport avec des filtres polynomiaux locaux.}\label{fig:graphs-ex-lp-es}

\footnotesize
\normalsize\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/ex/lp_if} 

}

\caption[Prévisions implicites liées aux estimations de la série lissée du climat des affaires dans les matériels de transport avec des filtres polynomiaux locaux]{Prévisions implicites liées aux estimations de la série lissée du climat des affaires dans les matériels de transport avec des filtres polynomiaux locaux.}\label{fig:graphs-ex-lp-if}

\footnotesize


\emph{Note} : \emph{L'axe des ordonnées n'est pas le même entre les différents graphiques.}
\normalsize\end{figure}

\hypertarget{subsec-localic}{%
\subsection{Paramétrisation locale des filtres asymétriques}\label{subsec-localic}}

Usuellement, les filtres asymétriques sont paramétrés de façon globale : \(\lvert\delta/\sigma\rvert\) estimé sur l'ensemble des données à partir de l'IC-ratio ou d'un critère de validation croisée.
En revanche, on pourrait préférer une paramétrisation locale : rapport \(\lvert\delta/\sigma\rvert\) qui varie en fonction du temps.
En effet, même si la paramétrisation globale est en moyenne valide, supposer le rapport \(\lvert\delta/\sigma\rvert\) constant pour l'ensemble des filtres asymétriques ne parait pas pertinent pour l'estimation en temps réel, en particulier pour des périodes de retournement conjoncturel.
Par exemple, avec la méthode LC, effectuer une paramétrisation globale revient à supposer que la pente de la tendance est constante, alors que pendant les périodes de retournement conjoncturel, elle tend vers 0 jusqu'au point de retournement.

C'est ce qui est proposé dans cet article, avec une paramétrisation locale des filtres asymétriques en estimant séparément \(\delta\) et \(\sigma^2\).
Même si cela ne donne pas un estimateur sans biais du rapport \(\lvert\delta/\sigma\rvert\), cela permet de capter les principales évolutions comme la décroissance vers 0 avant un point de retournement et la croissance après le point de retournement pour la méthode LC :

\begin{itemize}
\tightlist
\item
  La variance \(\sigma^2\) peut être estimée en utilisant l'ensemble des données observées et à partir du filtre symétrique \((w_{-p},\dots,w_p)\) :
  \[
  \hat\sigma^2=\frac{1}{n-2h}\sum_{t=h+1}^{n-h}\frac{(y_t-\hat \mu_t)^2}{1-2w_0^2+\sum w_i^2}.
  \]
\item
  Le paramètre \(\delta\) peut être estimé par moyenne mobile à partir de l'équation \eqref{eq:mmsym}.
  Par exemple, pour la méthode LC on peut utiliser la moyenne mobile \(\boldsymbol \theta_2=\boldsymbol K\boldsymbol X(\boldsymbol X'\boldsymbol K\boldsymbol X)^{-1}\boldsymbol e_{2}\) pour avoir une estimation locale de la pente et pour la méthode QL on peut utiliser \(\boldsymbol \theta_3=\boldsymbol K\boldsymbol X(\boldsymbol X'\boldsymbol K\boldsymbol X)^{-1}\boldsymbol e_{3}\) pour avoir une estimation locale de la concavité.
  La méthode DAF permet alors de simplement calculer les moyennes mobiles asymétriques associées.\\
  Même si une moyenne mobile de longueur différente de celle utilisée pour l'estimation de la tendance pourrait être envisagée, cela semble dégrader les résultats en termes de déphasage (en utilisant la même méthodologie que dans la section \ref{sec-comparison}).
  De plus, pour la construction des moyennes mobiles, la tendance peut être modélisée comme étant localement de degré 2 ou 3 (cela n'a pas d'impact pour l'estimation finale de la concavité).
  Nous retenons ici une modélisation de tendance de degré 2 : cela diminue le déphasage mais augmente légèrement les révisions liées à la première estimation de la tendance-cycle.
  La figure \ref{fig:mmpenteconcac} montre les moyennes mobiles utilisées.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{img/filters_used/mm_penteconcavite} 

}

\caption[Moyennes mobiles utilisées pour l'estimation en temps-réel de la pente et de la concavité]{Moyennes mobiles utilisées pour l'estimation en temps-réel de la pente et de la concavité.}\label{fig:mmpenteconcac}

\footnotesize
\normalsize\end{figure}

Dans les applications empiriques de la section \ref{sec-comparison}, la paramétrisation locale finale correspond à celle où \(\delta\) est estimé en utilisant l'ensemble des données (c'est-à-dire en utilisant les filtres symétriques de la figure \ref{fig:mmpenteconcac}) mais en gardant une estimation en temps réel de \(\sigma^2\).

Les graphiques \ref{fig:graphs-ex-lploc-es} et \ref{fig:graphs-ex-lploc-if} montrent les estimations successives de la série lissée du climat des affaires dans les matériels de transport avec une paramétrisation locale et avec une paramétrisation avec l'IC-ratio.
Sur cette série et sur les points étudiés, la paramétrisation locale ne semble pas avoir d'impact sur le filtre LC à l'exception de la dernière estimation (pour laquelle les prévisions implicites des filtres avec paramétrisation locale semblent plus plausibles).
Pour le filtre QL cela permet d'avoir moins de révisions dans les estimations intermédiaires (avec également des prévisions implicites plus plausibles avec la paramétrisation locale).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/ex/lp_local_es} 

}

\caption[Estimations successives de la série lissée du climat des affaires dans les matériels de transport avec des filtres polynomiaux locaux]{Estimations successives de la série lissée du climat des affaires dans les matériels de transport avec des filtres polynomiaux locaux.}\label{fig:graphs-ex-lploc-es}

\footnotesize
\normalsize\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/ex/lp_local_if} 

}

\caption[Prévisions implicites liées aux estimations de la série lissée du climat des affaires dans les matériels de transport avec des filtres polynomiaux locaux]{Prévisions implicites liées aux estimations de la série lissée du climat des affaires dans les matériels de transport avec des filtres polynomiaux locaux.}\label{fig:graphs-ex-lploc-if}

\footnotesize


\emph{Note} : \emph{L'axe des ordonnées n'est pas le même entre les différents graphiques.}
\normalsize\end{figure}

\hypertarget{subsec-lptimeliness}{%
\subsection{\texorpdfstring{Extension avec le critère de \emph{timeliness}}{Extension avec le critère de timeliness}}\label{subsec-lptimeliness}}

Un inconvénient de la méthode précédente est que le déphasage n'est pas contrôlé.
Il est en revanche possible de généraliser davantage la modélisation en ajoutant le critère de \emph{timeliness} défini par \textcite{ch15HBSA} dans l'équation \eqref{eq:lppasym}\footnote{
  C'est ce qui a été proposé par Jean Palate, puis codé en Java et intégré dans \texttt{rjd3filters}.}.

En utilisant les mêmes notations que dans \ref{subsec-lppasymf}, \(\boldsymbol \theta\) le filtre symétrique et \(\boldsymbol v\) le filtre asymétrique.
Notons également \(\boldsymbol \theta=\begin{pmatrix}\boldsymbol \theta_p\\\boldsymbol \theta_f\end{pmatrix}\) avec \(\boldsymbol \theta_p\) de même longueur que \(\boldsymbol v\), et \(\boldsymbol g=\boldsymbol v-\boldsymbol \theta_p\).
Le critère de \emph{timeliness} s'écrit :
\[
T_g(\boldsymbol v)=\boldsymbol v'\boldsymbol T\boldsymbol v=\boldsymbol g'\boldsymbol T\boldsymbol g+2\boldsymbol \theta_p'\boldsymbol T\boldsymbol g+\boldsymbol \theta_p'\boldsymbol T\boldsymbol \theta_p
\quad(\boldsymbol T\text{ étant symétrique)}.
\]
De plus, la fonction objectif \(\varphi\) de l'équation \eqref{eq:lppasym} peut se réécrire :
\begin{align*}
\varphi(\boldsymbol v)&=(\boldsymbol v-\boldsymbol \theta_p)'\boldsymbol D_{p}(\boldsymbol v-\boldsymbol \theta_p)+
  \boldsymbol \theta_f'\boldsymbol D_{f}\boldsymbol \theta_f+
  [\boldsymbol \delta'(\boldsymbol Z_{p}'\boldsymbol v-\boldsymbol Z'\boldsymbol \theta)]^{2}+
2\boldsymbol l'(\boldsymbol U_{p}'\boldsymbol v-\boldsymbol U'\boldsymbol \theta)\\
&=\boldsymbol g'\boldsymbol Q\boldsymbol g-2\boldsymbol P\boldsymbol g+2\boldsymbol l'(\boldsymbol U_{p}'\boldsymbol v-\boldsymbol U'\boldsymbol \theta)+\boldsymbol c\quad\text{avec }
\begin{cases}
\boldsymbol Q=\boldsymbol D_p+\boldsymbol Z_p\boldsymbol \delta\boldsymbol \delta'\boldsymbol Z'_p \\
\boldsymbol P=\boldsymbol \theta_f\boldsymbol Z_f\boldsymbol \delta\boldsymbol \delta'\boldsymbol Z_p'\\
\boldsymbol c\text{ une constante indépendante de }\boldsymbol v
\end{cases}.
\end{align*}

En ajoutant le critère de \emph{timeliness}, on obtient :
\[
\widetilde\varphi(\boldsymbol v)=\boldsymbol g'\widetilde {\boldsymbol Q}\boldsymbol g-
2\widetilde{\boldsymbol P}\boldsymbol g+2\boldsymbol l'(\boldsymbol U_{p}'\boldsymbol v-\boldsymbol U'\boldsymbol \theta)+
\widetilde{\boldsymbol c}\quad\text{avec }
\begin{cases}
\widetilde{\boldsymbol Q}=\boldsymbol D_p+\boldsymbol Z_p\boldsymbol \delta\boldsymbol \delta'\boldsymbol Z'_p +\alpha_T\boldsymbol T\\
\widetilde{\boldsymbol P}=\boldsymbol \theta_f\boldsymbol Z_f\boldsymbol \delta\delta'\boldsymbol Z_p'-\alpha_T\boldsymbol \theta_p\boldsymbol T\\
\widetilde{\boldsymbol c}\text{ une constante indépendante de }\boldsymbol v
\end{cases},
\]
où \(\alpha_T\) est le poids associé au critère de \emph{timeliness}.
Avec \(\alpha_T=0\) on retrouve \(\varphi(\boldsymbol v)\).
Cette extension permet donc de retrouver tous les filtres symétriques et asymétriques présentés dans la section précédente mais généralise également l'approche de \textcite{GrayThomson1996} présentée dans la section \ref{subsec-graythomson}.

\hypertarget{subsec-graythomson}{%
\subsection{Régression polynomiale : Gray et Thomson}\label{subsec-graythomson}}

\begin{summary_box}{Filtres locaux polynomiaux --- Gray et Thomson (1996)}

Approche fondée sur la modélisation locale de la tendance-cycle par des polynômes mais en prenant également en compte l'erreur d'approximation liée à cette modélisation :

\begin{itemize}
\item
  Modèles généraux qui permettent de prendre en compte l'autocorrélation entre les observations.
\item
  Interprétation statistique des différentes méthodes.
\item
  Le filtre asymétrique est indépendant de la date d'estimation.
  Toutefois, il dépend indirectement des données si le filtre est calibré sur l'I-C ratio.
\item
  La \emph{timeliness} n'est en revanche pas contrôlée.
\item
  La spécification du modèle peut être compliquée : si la structure d'autocorrélation est estimée à partir des données, cela rajoute de l'incertitude dans les estimations, ce qui peut avoir des effets indésirables.
\end{itemize}

\end{summary_box}

\hypertarget{filtres-symuxe9triques-1}{%
\subsubsection{Filtres symétriques}\label{filtres-symuxe9triques-1}}

L'approche de \textcite{GrayThomson1996} est proche de celles de \textcite{proietti2008} et de \textcite{ch15HBSA}.
De la même façon que pour les autres méthodes, ils considèrent que la série initiale \(y_t\) peut se décomposer en une somme de la tendance-cycle \(g_t\) et d'un bruit blanc \(\varepsilon_t\) de variance \(\sigma^2\) :
\[y_t = g_t+\varepsilon_t.\]
Toutefois, plutôt que de directement remplacer \(g_t\) par un polynôme local de degré \(d\), ils prennent en compte l'erreur d'approximation de la tendance :
\[
g_t=\sum_{j=0}^{d}\beta_{j}t^{j}+\xi_{t},
\]
où \(\xi_t\) est un processus stochastique de moyenne nulle, autocorrélé mais non corrélé à \(\varepsilon_t\).

La tendance \(g_t\) est estimée par une moyenne mobile :
\[
\hat{g}_{t}=\sum_{s=-r}^{r}\theta_{s}y_{t+s}.
\]

Pour le filtre central, les auteurs cherchent à avoir un estimateur \(\hat g_t\) qui soit sans biais (ce qui implique que \(\theta\) conserve les tendances de degré \(d\)) et qui minimise une somme pondérée d'un critère de \emph{fidelity} et d'un critère de \emph{smoothness} :
\begin{equation}
Q=\alpha\underbrace{\E{(\hat{g}_{t}-g_{t})^{2}}}_{=F_{GT}}+
(1-\alpha)\underbrace{\E{ (\Delta^{d+1}\hat{g}_{t})^{2}} }_{=S_{GT}}.
\label{eq:graythomsonindicators}
\end{equation}
La solution est un filtre symétrique qui peut s'écrire sous la forme
\[
\boldsymbol \theta=
\boldsymbol E_{\alpha}^{-1}\boldsymbol X\left[\boldsymbol X'\boldsymbol E_{\alpha}^{-1}\boldsymbol X\right]^{-1}\boldsymbol e_{1}
\text{ avec }
\boldsymbol E_{\alpha}=\alpha\left(\sigma^{2}\boldsymbol I+\boldsymbol \Omega\right)+(1-\alpha)\left(\sigma^{2}\boldsymbol B_{d+1}+\boldsymbol \Gamma\right),
\]
où :
\[
\begin{cases}
\Omega_{jk} & =cov\left(\xi_{t+j}-\xi_{t},\xi_{t+k}-\xi_{t}\right)\\
\Gamma_{jk} & =cov\left(\Delta^{d+1}\xi_{t+j},\Delta^{d+1}\xi_{t+k}\right)\\
\sigma^{2}\left(B_{d+1}\right)_{jk} & =cov\left(\Delta^{d+1}\varepsilon_{t+j},\Delta^{d+1}\varepsilon_{t+k}\right)
\end{cases}.
\]

En ne minimisant que la \emph{smoothness} et avec \(\xi_t=0\) on retrouve le filtre d'Henderson.
En ne minimisant que la \emph{fidelity}, cette méthode est équivalente à l'estimation de polynômes locaux par moindres carrés généralisés : on retrouve donc les filtres de \textcite{proietti2008} avec \(\sigma^2=0\) et \(\boldsymbol \Omega =\boldsymbol K^{-1}\), ainsi que le filtre de \textcite{macaulay1931smoothing}.

L'avantage de la modélisation de Gray et Thomson est que le paramètre \(\xi_t\) permet une spécification plus fine du modèle en prenant notamment en compte la corrélation entre les observations.
Par exemple, \textcite{mclaren2001rotation} ont étudié le lien entre le plan de sondage et l'estimation de la composante tendance-cycle et de la composante saisonnière.
Cette modélisation leur permet de prendre en compte, dans l'estimation de la tendance-cycle, la structure de corrélation induite par le plan de sondage de l'enquête emploi mensuelle de l'Australie (groupe de rotations avec une période de recouvrement).
Cependant, les auteurs avertissent que dans leur simulations (et dans la modélisation de Gray et Thomson) la structure d'autocorrélation de la variable aléatoire \(\xi_t\) est supposée connue.
Ce n'est généralement pas le cas en pratique, où cette structure doit être estimée, ce qui rajoute de l'incertitude dans les estimations.

\hypertarget{filtres-asymuxe9triques}{%
\subsubsection{Filtres asymétriques}\label{filtres-asymuxe9triques}}

L'approche retenue par \textcite{GrayThomson1996} est une approche de minimisation des révisions sous contraintes.
Étant donné un filtre symétrique \(\boldsymbol\theta^s\) utilisé pour estimer la tendance au centre de la série, l'objectif est de chercher un filtre asymétrique \(v\boldsymbol =(v_{-h},\dots,v_q)\) de sorte à minimiser l'erreur quadratique moyenne de révision :
\[
\E{\left(Y-\hat Y\right)^2} = 
\E{\left( \sum_{i=-h}^h\theta^s_iy_{t+s}-\sum_{i=-h}^qv_iy_{t+s} \right)^2}.
\]
Les auteurs étudient deux cas :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Dans le premier cas, ils cherchent un estimateur sans biais : cela implique que \(v\) conserve les mêmes tendances polynomiales que \(\boldsymbol\theta^s\).
  \(\hat Y\) est alors le meilleur prédicteur linéaire sans biais --- \emph{best linear unbiased predictor} (BLUP) --- de \(Y\).
\item
  Dans le second cas, ils autorisent l'estimateur à être biaisé mais imposent que ce biais soit constant dans le temps : si l'on modélise localement la tendance par un polynôme de degré \(d\), cela implique que \(\boldsymbol v\) conserve les tendances polynomiales de degré \(d-1\).
  \(\hat Y\) est alors le meilleur prédicteur linéaire à biais constant --- \emph{best linear time invariant predictor} (BLIP) --- de \(Y\).
  Cela permet notamment de reproduire les filtres asymétriques de Musgrave.
\end{enumerate}

La méthode utilisée est donc très proche de celle de \textcite{proietti2008} : on retrouve d'ailleurs le filtre DAF avec \(\sigma^2=0\) et \(\boldsymbol \Omega =\boldsymbol K^{-1}\) et en utilisant la première méthode (estimation du BLUP) et les méthodes LC (filtre de Musgrave), QL et CQ avec la seconde méthode en utilisant respectivement \(d=1\), \(d=2\) et \(d=3\).

\begin{remarque}
Pour la construction des filtres asymétriques, une approche alternative pourrait être d'utiliser la même méthode que celle utilisée pour construire les filtres symétriques.
C'est-à-dire minimiser \(Q\) (équation \eqref{eq:graythomsonindicators}) sous contrainte que le filtre asymétrique fournisse un estimateur sans biais de la tendance.
Comme discuté dans \textcite{GrayThomson1996}, les auteurs ne retiennent pas cette méthode pour deux raisons :

\begin{itemize}
\item
  Il n'est pas évident qu'il faille chercher à maintenir le même équilibre entre \emph{smoothness} et \emph{fidelity} en fin de série et au centre de la série.
  Le problème rencontré en fin de série est transitoire et disparaît au fur et à mesure que l'on a de nouvelles observations.
  Minimiser des critères de révision serait donc préférable puisque cela reviendrait à minimiser le coût de la transition (mais dans le cas où l'on ne minimise que la \emph{fidelity} les deux méthodes sont équivalentes).
\item
  Les valeurs de la \emph{fidelity} et de la \emph{smoothness} ne dépendent pas du temps au centre de la série mais en dépendent en fin de série.
  Ainsi, même si au centre de la série le choix des poids entre les deux critères contrôle indirectement le niveau des indicateurs, ce n'est plus le cas en fin de série.
  De plus, en fin de série, cela pourrait introduire des déphasages plus importants car \(S_{GT}\) dépend du temps et des valeurs passées (du fait de l'utilisation de l'opérateur différence).
\end{itemize}

Inversement, \textcite{ch15HBSA} justifient de ne pas intégrer le critère de révision dans leur problème car ce critère est fortement corrélé à une combinaison fixée, donc non ajustable par l'utilisateur, des critères \emph{fidelity} et \emph{timeliness}.

\end{remarque}

\hypertarget{sec-rkhs}{%
\subsection{Reproducing Kernel Hilbert Space (RKHS) : approche de Dagum et Bianconcini}\label{sec-rkhs}}

\begin{summary_box}{RKHS filters --- Dagum et Bianconcini (2008)}

Approche fondée sur la modélisation locale de la tendance-cycle par des polynômes mais avec une estimation grâce à la théorie des RKHS et par optimisation sur un critère de qualité~:

\begin{itemize}
\item
  Le filtre asymétrique est indépendant des données et de la date d'estimation.
\item
  La méthode est généralisable à des séries avec des fréquences irrégulières (par exemple avec beaucoup de valeurs manquantes).
\item
  Il peut y avoir des problèmes de minimisation.
\end{itemize}

\end{summary_box}

La théorie des \emph{Reproducing Kernel Hilbert Space} (RKHS) --- espaces de Hilbert à noyau reproduisant --- est une théorie générale dans l'apprentissage statistique non-paramétrique qui permet d'englober un grand nombre de méthodes.
C'est par exemple le cas des méthodes de régression par moindres carrés pénalisés, des Support Vector Machine (SVM), du filtre d'Hodrick-Prescott (utilisé pour décomposer tendance et cycle) ou encore des moyennes mobiles telles que celle d'Henderson.
Ainsi, \textcite{dagumbianconcini2008} utilisent la théorie des RKHS pour approcher le filtre d'Henderson et en dériver des filtres asymétriques associés.

Un RKHS \(\mathbb{L}^{2}(f_{0})\) est un espace de Hilbert caractérisé par un noyau qui permet de reproduire toutes les fonctions de cet espace.
Dit autrement, un RKHS est un espace mathématique de fonctions (ensemble des séries temporelles, ensemble des séries temporelles polynomiales d'un certain degré\ldots) qui possède une structure permettant de résoudre de nombreux problèmes.
En particulier, il est caractérisé par un produit scalaire \(\ps{\cdot}{\cdot}\) et une fonction de densité \(f_0\) qui permettent notamment de mesurer la proximité entre deux éléments de son espace (par exemple entre différentes estimations de la tendance-cycle).
Il est également à noyau reproduisant, ce qui signifie que tout élément de l'espace étudié (par exemple les tendances polynomiales locales) peut s'écrire à partir du produit scalaire et d'une certaine fonction (le noyau).
Comme nous le verrons, cette propriété permet notamment de calculer des moyennes mobiles pour l'estimation de la tendance-cycle.

Le produit scalaire \(\ps{\cdot}{\cdot}\) est défini par :
\[
\left\langle U,V\right\rangle =\E{UV}=\int_{\R}U(t)V(t)f_{0}(t)\ud t\quad
\forall U,V\in\mathbb{L}^{2}(f_{0}).
\]
La fonction \(f_0\) pondère donc chaque valeur en fonction de sa position temporelle : il s'agit de la version continue des noyaux définis dans la partie \ref{sec-kernels}.

Dans notre cas, on suppose que notre série initiale \(y_t\) est désaisonnalisée et peut s'écrire comme la somme d'une tendance-cycle, \(TC_t\), et d'une composante irrégulière, \(I_t\) (qui peut être un bruit blanc ou suivre un modèle ARIMA) :
\(y_t=TC_t+I_t\).
La tendance-cycle peut être déterministe ou stochastique.
On suppose que c'est une fonction régulière du temps, elle peut être localement approchée par un polynôme de degré \(d\) :
\[
TC_{t+j}=TC_t(j)=a_0+a_1j+\dots+a_dj^d+\varepsilon_{t+j},\quad
j\in\llbracket-h,h\rrbracket,
\]
où \(\varepsilon_t\) est un bruit blanc non corrélé à \(I_t\).

Les coefficients \(a_0,\dots,a_d\) peuvent être estimés par projection des observations au voisinage de \(y_t\) sur le sous-espace \(\mathbb P_d\) des polynômes de degré \(d\), ou, de manière équivalente, par minimisation de la distance entre \(y_t\) et \(TC_t(j)\) :
\begin{equation}
\underset{TC\in\mathbb P_d}{\min}\lVert y -TC \rVert^2 = 
\underset{TC\in\mathbb P_d}{\min}\int_\R (y(t+s)-TC_t(s))^2f_0(s)\ud s.
\label{eq:mintcrkhs}
\end{equation}
L'espace \(\mathbb P_d\) étant un espace de Hilbert à dimension finie, il admet un noyau reproduisant (voir, par exemple, \textcite{berlinet2004}).
Il existe ainsi une fonction \(R_d(\cdot,\cdot)\) telle que :
\[
\forall P\in \mathbb P_d: \forall t:
R_d(t,\cdot)\in\mathbb P_d\quad\text{et}\quad
P(t)=\ps{R_d(t,\cdot)}{P(\cdot)}.
\]

Le problème \eqref{eq:mintcrkhs} admet une solution unique qui dépend d'une fonction \(K_{d+1}\), appelée \emph{fonction de noyau} (\emph{kernel function}).
Cette fonction est dite d'ordre \(d+1\) car elle conserve les polynômes de degré \(d\)\footnote{
  C'est-à-dire \(\int_\R K_{d+1}(s)\ud s = 1\) et \(\int_\R K_{d+1}(s) s^i\ud s = 1\) pour \(i\in \llbracket 1, d\rrbracket\).}.
Cette solution s'écrit :
\begin{equation}
\widehat{TC}(t)=\int_\R y(t-s)K_{d+1}(s) \ud s.
\label{eq:rkhssoltc}
\end{equation}
Généralement \(f_0(t) = 0\) pour \(\lvert t \rvert>1\).
Cette solution s'écrit alors :
\begin{equation}
\widehat{TC}(t)=\int_{[-1,1]} y(t-s)K_{d+1}(s) \ud s.
\label{eq:rkhssoltc2}
\end{equation}
On peut, par ailleurs, montrer que \(K_{d+1}\) s'écrit en fonction de \(f_0\) et du noyau reproduisant \(R_d(\cdot,\cdot)\) et que ce dernier peut s'écrire en fonction de polynômes \((P_i)_{i\in \llbracket 0, d \rrbracket}\) qui forme une base orthonormée de \(\mathbb P_d\) (voir par exemple \textcite{berlinet1993}) :
\[
K_{d+1}(t) = R_d(t,0)f_0(t) = \sum_{i=0}^dP_i(t)P_i(0)f_0(t).
\]

De plus, dans le cas discret, la solution \eqref{eq:rkhssoltc2} s'écrit comme une somme pondérée au voisinage de \(y_t\) :
\begin{equation}
\widehat{TC}_t=\sum_{j=-h}^h w_j y_{t+j}\quad
\text{où} \quad
w_j=\frac{K_{d+1}(j/b)}{\sum_{i=-h}^{^h}K_{d+1}(i/b)}.
\label{eq:rkhssym}
\end{equation}
Le paramètre \(b\) (la fenêtre du noyau) est choisi de sorte que les \(2h+1\) points autour de \(y_t\) soient utilisés avec un poids non nul.

Pour les filtres asymétriques, la formule \eqref{eq:rkhssym} est simplement adaptée au nombre d'observations connues :
\begin{equation}
\forall j\in\left\llbracket -h,q\right\rrbracket\::\: w_{a,j}=\frac{K_{d+1}(j/b)}{\sum_{i=-h}^{^q}K_{d+1}(i/b)}.
\label{eq:rkhsasym}
\end{equation}
En utilisant \(b=h+1\) on retrouve les filtres symétriques obtenues par polynômes locaux.

Comme notamment montré par \textcite{dagumbianconcini2016seasonal}, \(K_{d+1}\) peut s'exprimer simplement à partir des moments de \(f_0\)\footnote{
  Cela vient en fait du procédé d'orthonormalisation de Gram-Schmidt.}.
Ainsi, notons \(\boldsymbol H_{d+1}\) la matrice de Hankel associée aux moments de \(f_0\) :
\[
\forall i,j\in \llbracket 0, d\rrbracket:
\left(\boldsymbol H_{d+1}\right)_{i,j}=\ps{X^i}{X^j}=\int s^{i+j}f_0(s)\ud s.
\]
Notons également \(\boldsymbol H_{d+1}[1,\boldsymbol x_t]\) la matrice obtenue en remplaçant la première ligne de \(\boldsymbol H_{d+1}\) par \(\boldsymbol x_t=\begin{pmatrix} 1 & t & t^2 & \dots & t^d\end{pmatrix}'\).
On a :
\begin{equation}
K_{d+1}(t)=\frac{\det{\boldsymbol H_{d+1}[1,\boldsymbol x_t]}}{\det{\boldsymbol H_{d+1}}}f_0(t).
\label{eq:rkhskernelfun}
\end{equation}
C'est cette formule qui est utilisée dans le \emph{package} \texttt{rjd3filters} pour calculer les différentes moyennes mobiles.

Comme discuté dans la partie \ref{sec-proietti}, le noyau d'Henderson dépend de la fenêtre utilisée.
Ainsi, tous les moments de l'équation \eqref{eq:rkhskernelfun} doivent être recalculés pour chaque valeur de \(h\).
Pour éviter cela, \textcite{dagumbianconcini2008} suggèrent d'utiliser le noyau quadratique (\emph{biweight}) pour approcher le noyau d'Henderson lorsque \(h\) est petit (\(h< 24\)) et le noyau cubique (\emph{triweight}) lorsque \(h\) est grand (\(h\geq 24\)).

Dans \textcite{dagumbianconcini2015new}, les auteures suggèrent de faire une sélection optimale du paramètre \(b\) pour chaque moyenne mobile asymétrique.
Notons \(\Gamma_s\) la fonction de transfert du filtre symétrique et \(\Gamma_{\boldsymbol\theta}\) celle du filtre asymétrique que l'on cherche à obtenir.
Le filtre asymétrique utilisant \(q\) points dans le futur peut par exemple être obtenu en utilisant la fenêtre \(b_q\) qui minimise l'erreur quadratique moyenne (option \texttt{"frequencyresponse"} dans \texttt{rjd3filters::rkhs\_filter()})\footnote{
  Dans leur article, les auteurs utilisent une formule différente pour la fonction de réponse (\(\Gamma_{\boldsymbol\theta}(\omega)=\sum_{k=-p}^{+f} \theta_k e^{2\pi i \omega k}\)), ce qui conduit à des bornes d'intégrales légèrement différentes, sans effet sur le résultat.} :
\[
b_{q,\Gamma}=\underset{b_q\in[h; 3h]}{\min}
2\int_{0}^{\pi}
\lvert \Gamma_s(\omega)-\Gamma_{\boldsymbol\theta}(\omega)\rvert^2\ud \omega.
\]
Cela suppose en fait que la série entrée \(y_t\) soit un bruit blanc.
En supposant \(y_t\) stationnaire, les critères définis dans l'article originel peuvent donc être étendus en multipliant les quantités sous les intégrales par la densité spectrale de \(y_t\) notée \(h\) :
\[
b_{q,\Gamma}=\underset{b_q\in[h; 3h]}{\min}
2\int_{0}^{\omega_1}
\lvert \Gamma_s(\omega)-\Gamma_{\boldsymbol\theta}(\omega)\rvert^2h(\omega)\ud \omega,
\]
avec \(\omega_1 = \pi\) dans \textcite{dagumbianconcini2015new}.
Cette erreur quadratique moyenne peut également se décomposer en plusieurs termes (voir équation \eqref{eq:msedef} de la section \ref{sec-WildiMcLeroy}).
Les fenêtres \(b_q\) peuvent donc également être obtenues en minimisant d'autres critères de qualité des moyennes mobiles (section \ref{subsec:crit-qual}).
En notant \(\rho_s\) la fonction de gain du filtre symétrique, \(\rho_{\boldsymbol\theta}\) et \(\varphi_{\boldsymbol\theta}\) la fonction de gain et de déphasage du filtre asymétrique que l'on cherche à obtenir, les fenêtre \(b_q\) peuvent être obtenues en minimisant :

\begin{itemize}
\item
  l'\emph{accuracy} qui correspond à la part de la révision liée aux différences de fonction de gain dans les fréquences liées à la tendance-cycle\footnote{
    Si l'on étudie des séries mensuelles et que l'on considère que la tendance-cycles correspondent aux cycles de 12 mois ou plus, on peut donc utiliser \(\omega_1=2\pi/12\) (voir section \ref{subsec:gain-deph}).} (avec \(\omega_1=\pi\) dans \textcite{dagumbianconcini2015new})
  \[
  b_{q,G}=\underset{b_q\in[h; 3h]}{\min}
  2\int_{0}^{\omega_1}
  \left(\rho_s(\omega)-\rho_{\boldsymbol\theta}(\omega)\right)^{2} h(\omega)\ud \omega
  \]
\item
  la \emph{smoothness} qui correspond à la part de la révision liée aux différences de fonction de gain dans les fréquences liées aux résidus\footnote{
    Méthode non utilisée dans \textcite{dagumbianconcini2015new} mais implémentée dans \texttt{rjd3filters}.}
  \[
  b_{q,s}=\underset{b_q\in[h; 3h]}{\min}
  2\int_{\omega_1}^{\pi}
  \left(\rho_s(\omega)-\rho_{\boldsymbol\theta}(\omega)\right)^{2} h(\omega)\ud \omega
  \]
\item
  la \emph{timeliness} qui correspond à la part de la révision liée au déphasage (avec \(\omega_1=2\pi/36\) dans \textcite{dagumbianconcini2015new} et une formulation légèrement différente du critère à minimiser)
  \[
  b_{q,\varphi}=\underset{b_q\in[h; 3h]}{\min}
  8\int_{0}^{\omega_1}
  \rho_s(\lambda)\rho_{\boldsymbol\theta}(\lambda)\sin^{2}\left(\frac{\varphi_{\boldsymbol\theta}(\omega)}{2}\right)h(\omega)\ud \omega
  \]
\end{itemize}

Dans \texttt{rjd3filters}, \(h\) peut être fixée à la densité spectrale d'un bruit blanc (\(h_{WN}(x)=1\), comme c'est le cas dans \textcite{dagumbianconcini2015new}) ou d'une marche aléatoire (\(h_{RW}(x)=\frac{1}{2(1-\cos(x))}\)).

Un des inconvénients de cette méthode est qu'il n'y a pas unicité de la solution et donc qu'il y a parfois plusieurs extrema (uniquement pour le calcul de \(b_{q,\varphi}\)).
Ainsi, la valeur optimale retenue par défaut par \texttt{rjd3filters} produit des discontinuités dans l'estimation de la tendance-cycle.\\
Par ailleurs, les valeurs de \(b_{q,G}\) varient fortement en fonction de si l'on retient \(\omega_1=2\pi/12\) ou \(\omega_1=\pi\) (tableau \ref{tab:optimalbwrkhs}).
Par cohérence et simplicité, nous utiliserons dans cet article les valeurs optimales présentées dans \textcite{dagumbianconcini2015new}.

\begin{table}[!h]

\caption{\label{tab:optimalbwrkhs}Fenêtres optimales pour les filtres asymétriques associés à un filtre symétrique de 13 termes ($h=6$) avec le noyau biweight.}
{
\centering
\begin{tabular}[t]{lcccccc}
\toprule
  & $q=0$ & $q=1$ & $q=2$ & $q=3$ & $q=4$ & $q=5$\\
\midrule
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{$b_{q,\Gamma}$}}\\
\hspace{1em}$\omega_1 = \pi$ (valeurs utilisées) & 9,54 & 7,88 & 7,07 & 6,88 & 6,87 & 6,94\\
\hspace{1em}$\omega_1 = 2\pi/12$ & 9,54 & 7,88 & 7,07 & 6,88 & 6,87 & 6,94\\
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{$b_{q,G}$}}\\
\hspace{1em}$\omega_1 = \pi$ (valeurs utilisées) & 11,78 & 9,24 & 7,34 & 6,85 & 6,84 & 6,95\\
\hspace{1em}$\omega_1 = 2\pi/12$ & 8,61 & 7,64 & 6,01 & 6,01 & 6,01 & 6,59\\
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{$b_{q,\varphi}$}}\\
\hspace{1em}Valeurs de l'article (avec $\omega_1 = 2\pi/36$) & 6,01 & 6,01 & 7,12 & 8,44 & 9,46 & 10,39\\
\hspace{1em}$\omega_1 = 2\pi/36$ & 6,01 & 6,01 & 7,21 & 8,47 & 9,46 & 6,01\\
\hspace{1em}$\omega_1 = 2\pi/12$ & 6,01 & 6,01 & 6,38 & 8,15 & 9,35 & 6,01\\
\bottomrule
\end{tabular}
}
\footnotesize


\emph{Notes} : \emph{Avec \(b_{q,\Gamma}=\underset{b_q\in[h; 3h]}{\min}2\int_{0}^{\omega_1}\lvert \Gamma_s(\omega)-\Gamma_{\boldsymbol\theta}(\omega)\rvert^2\ud \omega,\) \(b_{q,G}=\underset{b_q\in[h; 3h]}{\min}2\int_{0}^{\omega_1}\left(\rho_s(\omega)-\rho_{\boldsymbol\theta}(\omega)\right)^{2} \ud \omega\) et \(b_{q,\varphi}=\underset{b_q\in[h; 3h]}{\min}8\int_{0}^{\omega_1}\rho_s(\lambda)\rho_{\boldsymbol\theta}(\lambda)\sin^{2}\left(\frac{\varphi_{\boldsymbol\theta}(\omega)}{2}\right)\ud \omega.\)}

\emph{Le paramètre \(q\) désigne le nombre de points dans le futur utilisés par la moyenne mobile (pour \(q=0\), estimation en temps réel).}

\emph{Les valeurs utilisées correspondent aux valeurs de \textcite{dagumbianconcini2015new}. On les retrouve directement avec \texttt{rjd3filters}, sauf pour \(b_{q,\varphi}\) du fait de la présence de plusieurs extrema.}
\normalsize\end{table}

Le graphique \ref{fig:graphs-ex-rkhs} montre les estimations successives de la série lissée du climat des affaires dans les matériels de transport ainsi que les prévisions implicites pour les filtres RKHS de \textcite{dagumbianconcini2015new}.
Ici les dernières estimations (lorsqu'aucun point dans le futur n'est connu) sont fortement révisées, ce qui s'observe notamment par la valeur de la dernière prévision implicite qui est éloignée des valeurs que l'on pourrait attendre pour l'évolution de l'indicateur.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/ex/rkhs} 

}

\caption[Estimations successives et prévisions implicites de la série lissée du climat des affaires dans les matériels de transport avec les filtres RKHS]{Estimations successives et prévisions implicites de la série lissée du climat des affaires dans les matériels de transport avec les filtres RKHS.}\label{fig:graphs-ex-rkhs}

\footnotesize


\emph{Note} : \emph{L'axe des ordonnées n'est pas le même entre les différents graphiques.}
\normalsize\end{figure}

Dans \textcite{dagumBianconcini2023}, les auteures montrent comment la théorie des RKHS permet de retrouver les \emph{cascade linear filter} (CLF), qui sont des filtres alternatifs à celui de Henderson et qui n'ont pas été étudiés dans cette étude.

\hypertarget{subsec-GuggemosEtAl}{%
\section{\texorpdfstring{Approche \emph{Fidelity-Smoothness-Timeliness} (FST)}{Approche Fidelity-Smoothness-Timeliness (FST)}}\label{subsec-GuggemosEtAl}}

\begin{summary_box}{Filtres FST --- Grun-Rehomme et alii (2018)}
Approche fondée sur l'optimisation d'une somme pondérée de critères :

\begin{itemize}
\item
  Le filtre asymétrique est indépendant du filtre symétrique, des données et de la date d'estimation.
\item
  Le problème d'optimisation admet une solution unique (numériquement solvable).
\item
  Les différents critères ne sont pas normalisés : les poids accordés aux différents critères ne peuvent être comparés.
\end{itemize}

\textbf{Fonction \faIcon{r-project}} : \texttt{rjd3filters::fst\_filter()}.

\end{summary_box}

Pour construire les moyennes mobiles symétriques, \textcite{GrunRehommeLadiray1994} et \textcite{GrayThomson1996} proposent un programme de minimisation sous contrainte qui fait un compromis entre réduction de la variance et «~lissage~» de la tendance (les deux articles utilisent en revanche des notations différentes).
\textcite{ch15HBSA} étendent ces approches en les appliquant à la construction des filtres asymétriques et en ajoutant un critère permettant de contrôler le déphasage.
Il s'agit de l'approche \emph{Fidelity-Smoothness-Timeliness} --- Fidélité-Lissage-Rapidité --- (FST).
Pour la construction des filtres asymétriques, un quatrième critère pourrait également être rajouté qui prendrait en compte les révisions par rapport à l'utilisation d'un filtre symétrique de référence (cette méthode pourrait alors être appelée la méthode FRST --- \emph{Fidelity-Revisions-Smoothness-Timeliness}).
Cependant, le \emph{package} \texttt{rjd3filters} n'ayant implémenté que l'approche FST, nous nous restreignons dans cette étude à la description de l'approche sans critère de révision.

Les trois critères utilisés sont les suivants :

\begin{itemize}
\item
  \emph{Fidelity} (fidélité), \(F_g\) : c'est le rapport de réduction de la variance.
  Plus il est petit et plus la tendance-cycle estimée est un bon estimateur de la vraie tendance-cycle.
  \[
  F_g(\boldsymbol \theta) = \sum_{k=-p}^{+f}\theta_{k}^{2}.
  \]
  \(F_g\) peut également être écrite comme une forme quadratique positive : \(F_g(\boldsymbol \theta)=\boldsymbol \theta'\boldsymbol F\boldsymbol \theta\) avec \(\boldsymbol F\) la matrice identité d'ordre \(p+f+1\).
\item
  \emph{Smoothness} (lissage), \(S_g\) :
  \[
  S_g(\boldsymbol\theta) = \sum_{j}(\nabla^{d}\theta_{j})^{2}.
  \]
  Ce critère mesure la proximité de la série lissée à une tendance polynomiale de degré \(d-1\).
  Henderson utilise par exemple ce critère avec \(d=3\) pour construire des moyennes mobiles conservant des polynômes de degré 2.
  \(S_g\) peut également s'écrire sous une forme quadratique positive \(S_g(\boldsymbol \theta)=\boldsymbol \theta'\boldsymbol S\boldsymbol \theta\) avec \(S\) une matrice symétrique d'ordre \(p+f+1\) (voir annexe \ref{an-diag}).
\item
  \emph{Timeliness} (rapidité), \(T_g\) : il mesure le déphasage entre la série initiale et la série lissée à des fréquences spécifiques.
  Lorsqu'un filtre linéaire est appliqué, le niveau de la série initiale est également modifié par la fonction de gain : il est donc intuitif de considérer que plus le gain est élevé, plus l'impact du déphasage le sera.\\
  C'est pourquoi le critère de déphasage dépend des fonctions de gain et de déphasage (\(\rho_{\boldsymbol\theta}\) et \(\varphi_{\boldsymbol\theta}\)), le lien entre les deux fonctions étant fait à partir d'une fonction de pénalité \(f\)\footnote{
    \textcite{ch15HBSA} suggèrent 6 conditions de régularité à la fonction de pénalité afin qu'elle soit adaptée au problème de déphasage.
    Dans leur article, la fonction \(f\) ne dépend que du gain et du déphasage de \(\theta\) et les 6 conditions sont : \(f \geq 0\), \(f\left(\rho,0\right)=0\), \(f\left(0,\varphi\right)=0\), \(f\left(\rho,\varphi\right)=f\left(\rho,-\varphi\right)\), \(\frac{\partial f}{\partial \rho} \geq 0\) et
    \(\varphi \cdot \frac{\partial f}{\partial \varphi} \geq 0\).} :
  \[
  \int_{\omega_{1}}^{\omega_{2}}f(\rho_{\boldsymbol\theta}(\omega),\varphi_{\boldsymbol\theta}(\omega))\ud\omega.
  \]
  Comme fonction de pénalité, les auteurs suggèrent de prendre \(f\colon(\rho,\varphi)\mapsto\rho^2\sin(\varphi)^2\).
  Cela permet notamment d'avoir une \emph{timeliness} qui peut s'écrire comme une forme quadratique positive (\(T_g(\boldsymbol \theta)=\boldsymbol \theta'\boldsymbol T\boldsymbol \theta\) avec \(\boldsymbol T\) une matrice carré symétrique d'ordre \(p+f+1\), voir \textcite{ch15HBSA} pour la démonstration).
  Dans cet article nous utilisons \(\omega_1=0\) et \(\omega_2=2\pi/12\) : on ne s'intéresse qu'à des séries mensuelles et au déphasage associé aux cycles d'au minimum 12 mois.
\end{itemize}

En somme, l'approche FST consiste à minimiser une somme pondérée de ces trois critères sous certaines contraintes (généralement préservation polynomiale).

\begin{equation}
\begin{cases}
\underset{\boldsymbol\theta}{\min} &
\alpha F_g(\boldsymbol \theta)+\beta S_g(\boldsymbol \theta)+\gamma T_g(\boldsymbol \theta) = 
\boldsymbol \theta'(\alpha\boldsymbol F+\beta \boldsymbol S+ \gamma \boldsymbol T)\boldsymbol \theta\\
s.t. & \boldsymbol C\boldsymbol \theta=\boldsymbol a
\end{cases}. \label{eq:gugemmos}
\end{equation}

Les conditions \(0\leq\alpha,\beta,\gamma\leq 1\) et \(\alpha+\beta\ne0\) garantissent que \(\alpha F_g(\boldsymbol \theta)+\beta S_g(\boldsymbol \theta)+\gamma T_g(\boldsymbol \theta)\) soit strictement convexe et donc l'unicité de la solution.
Dans ce cas, la solution s'écrit \(\hat {\boldsymbol \theta} = [\alpha \boldsymbol F+\beta \boldsymbol S+ \gamma \boldsymbol T]^{-1}\boldsymbol C'\left(\boldsymbol C[\alpha \boldsymbol F+\beta \boldsymbol S+ \gamma \boldsymbol T]^{-1}\boldsymbol C'\right)^{-1}\boldsymbol a\).

Dans cette approche, les filtres asymétriques construits sont totalement indépendants des données, de la date d'estimation et du filtre symétrique choisis.

On obtient par exemple le filtre d'Henderson avec les paramètres suivants :
\[\boldsymbol C=\begin{pmatrix}
1 & \cdots&1\\
-h & \cdots&h \\
(-h)^2 & \cdots&h^2
\end{pmatrix},\quad
\boldsymbol a=\begin{pmatrix}
1 \\0\\0
\end{pmatrix},\quad
\alpha=\gamma=0,\quad
\beta=1,\quad d=3.\]

Un des inconvénients de cette approche est que les différents critères ne sont pas normalisés : leurs valeurs ne peuvent pas être comparées et n'ont donc pas de sens.
Il n'y a, par exemple, pas d'interprétation à donner à un poids deux fois plus important à la \emph{timeliness} qu'à la \emph{fidelity}.

Le graphique \ref{fig:graphs-ex-fst} montre les estimations successives de la série lissée du climat des affaires dans les matériels de transport ainsi que les prévisions implicites pour un ensemble particulier de poids \(\alpha,\beta,\gamma\) (voir section \ref{sec-comparison} pour la méthode utilisée pour les sélectionner).
Ici les dernières estimations (lorsqu'aucun point dans le futur n'est connu) sont fortement révisées, ce qui s'observe notamment par la valeur de la dernière prévision implicite qui est éloignée des valeurs que l'on pourrait attendre pour l'évolution de l'indicateur.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/ex/fst} 

}

\caption[Estimations successives et prévisions implicites de la série lissée du climat des affaires dans les matériels de transport avec la méthode FST en préservant les polynômes de degré 2 et avec \(\alpha=0,00\), \(\beta=0,05\) et \(\gamma=0,95\)]{Estimations successives et prévisions implicites de la série lissée du climat des affaires dans les matériels de transport avec la méthode FST en préservant les polynômes de degré 2 et avec \(\alpha=0,00\), \(\beta=0,05\) et \(\gamma=0,95\).}\label{fig:graphs-ex-fst}

\footnotesize
\normalsize\end{figure}

\hypertarget{sec-WildiMcLeroy}{%
\section{Filtres dépendant des données : trilemme ATS}\label{sec-WildiMcLeroy}}

\begin{summary_box}{ATS-trilemma -- Wildi et McElroy (2019)}
Approche fondée sur l'optimisation d'une somme pondérée de critères :

\begin{itemize}
\item
  Les valeurs des différents critères peuvent être comparées et les poids associés peuvent être interprétés (contrairement à la méthode FST).
\item
  Méthode généralisable aux cas multivariés.
\item
  Les filtres dépendent des données, de la date d'estimation et du filtre symétrique utilisé.
\item
  Il peut y avoir des problèmes d'optimisation (plusieurs minimums, etc.).
\end{itemize}

\textbf{\faIcon{r-project}} : \emph{package} MDFA \url{https://github.com/wiaidp/MDFA} ou \texttt{rjd3filters::dfa\_filter()} (version simplifiée).

\end{summary_box}

\textcite{trilemmaWMR2019} proposent une approche dépendante des données (\emph{data-dependent}) pour calculer des filtres linéaires.
L'idée est la même que pour le filtre FST : les moyennes mobiles sont calculées par minimisation d'une somme pondérée de critères de qualité, mais les critères sont calculés différemment.
Les auteurs décomposent l'erreur quadratique moyenne de révision en un trilemme de trois quantités appelées \emph{accuracy} (précision), \emph{timeliness} (rapidité) et \emph{smoothness} (lissage), d'où son nom \emph{ATS-trilemma}.

Soient :

\begin{itemize}
\item
  \(\left\{ y_{t}\right\}\) notre série temporelle en entrée\footnote{
    Par rapport à l'article originel, les notations ont été modifiées afin de garder une cohérence entre les différentes sections.}.
\item
  \(\left\{TC_{t}\right\}\) la série lissée finale (avec un filtre symétrique fini ou non), et soient respectivement \(\Gamma_s\), \(\rho_s\) et \(\varphi_s\) les fonctions de transfert, de gain et de déphasage associées à ce filtre symétrique.
\item
  \(\left\{\widehat{TC}_{t}\right\}\) une estimation de \(\left\{TC_{t}\right\}\), i.e.~le résultat d'un filtre asymétrique (lorsque toutes les observations ne sont pas disponibles), et soient respectivement \(\Gamma_{\boldsymbol\theta}\), \(\rho_{\boldsymbol\theta}\) et \(\varphi_{\boldsymbol\theta}\) les fonctions de transfert, de gain et de déphasage associées à ce filtre asymétrique.
\end{itemize}

Une approche directe\footnote{Par opposition aux approches indirectes par exemple utilisées dans X-13-ARIMA où le signal cible est approché en faisant une prévision sur la série initiale.}, \emph{Direct Filter Approach} (DFA), consiste à approcher directement la tendance-cycle finale par minimisation de l'erreur quadratique moyenne :
\[
\underset{\widehat{TC}_{t}}{\min} \E{(TC_{t}-\widehat{TC}_{t})^{2}}.
\]
Cette approche peut être approfondie en décomposant l'erreur quadratique moyenne en plusieurs éléments d'intérêt.

Si l'on suppose que la série \(\left\{ y_{t}\right\}\) est faiblement stationnaire\footnote{
  Les formules qui suivent peuvent également se généraliser aux processus intégrés non-stationnaires, par exemple en imposant une cointégration entre \(TC_t\) et \(\widehat{TC}_{t}\) et en utilisant le pseudo-spectre, voir \textcite{optimrtfWMR2013}.} avec une densité spectrale continue \(h\), l'erreur quadratique moyenne de révision, \(\E{(TC_{t}-\widehat{TC}_{t})^{2}}\), peut s'écrire dans le domaine spectral comme :
\begin{align}
\E{(TC_{t}-\widehat{TC}_{t})^{2}}&=\frac{1}{2\pi}\int_{-\pi}^{\pi}\left|\Gamma_s(\omega)-{\Gamma_{\boldsymbol\theta}}(\omega)\right|^{2}h(\omega)\ud\omega\nonumber\\
 & =\frac{1}{2\pi}\times2\times\int_{0}^{\pi}\left|\Gamma_s(\omega)-{\Gamma_{\boldsymbol\theta}}(\omega)\right|^{2}h(\omega)\ud\omega.
\label{eq:msedef}
\end{align}
On a :
\begin{align}
\left|\Gamma_s(\omega)-\Gamma_{\boldsymbol\theta}(\omega)\right|^{2} & =\rho_s(\omega)^{2}+\rho_{\boldsymbol\theta}(\omega)^{2}+2\rho_s(\omega)\rho_{\boldsymbol\theta}(\omega)\left(1-\cos(\varphi_s(\omega)-\varphi_{\boldsymbol\theta}(\omega)\right) \nonumber\\
 & =\left(\rho_s(\omega)-\rho_{\boldsymbol\theta}(\omega)\right)^{2}+4\rho_s(\omega)\rho_{\boldsymbol\theta}(\omega)\sin^{2}\left(\frac{\varphi_s(\omega)-\varphi_{\boldsymbol\theta}(\omega)}{2}\right).
 \label{eq:msedecomp}
\end{align}

L'intervalle \([0,\pi]\) peut être coupé en deux : une partie dite \emph{pass-band} \([0,\omega_1]\) (l'intervalle de fréquences associé à la tendance-cycle) et une partie dite \emph{stop-band} \([\omega_1,\pi]\) (l'intervalle de fréquences associé aux résidus).
Dans \texttt{rjd3filters} le paramètre \(\omega_1\) est fixé par l'utilisateur\footnote{
  Dans l'article originel, l'intervalle \emph{pass-band} dépend de la fonction de gain du filtre symétrique (pass-band\(=\{\omega |\rho_s(\omega)\geq 0,5\}\)) : cela correspond donc à l'intervalle contenant les fréquences conservées sans trop de distorsion par le filtre symétrique.
  Dans le cas du filtre symétrique d'Henderson de 13 termes, cela correspond à l'intervalle \([0, 2\pi/8]\), c'est-à-dire aux cycles de plus de 8 mois.} et est par défaut égal à \(2\pi/12\) : pour des séries mensuelles, cela signifie que l'on souhaite conserver les cycles d'au moins 12 mois (voir section \ref{subsec:gain-deph}).

L'erreur de l'équation \eqref{eq:msedef} peut être décomposée en 4 quantités :
\begin{align*}
Accuracy =A_w(\boldsymbol\theta)&= 2\int_0^{\omega_1}\left(\rho_s(\omega)-\rho_{\boldsymbol\theta}(\omega)\right)^{2}h(\omega)\ud\omega,\\
Timeliness =T_w(\boldsymbol\theta)&= 8\int_0^{\omega_1}\rho_s(\omega)\rho_{\boldsymbol\theta}(\omega)\sin^{2}\left(\frac{\varphi_{\boldsymbol\theta}(\omega)}{2}\right)h(\omega)\ud\omega,\\
Smoothness =S_w(\boldsymbol\theta)&= 2\int_{\omega_1}^\pi\left(\rho_s(\omega)-\rho_{\boldsymbol\theta}(\omega)\right)^{2}h(\omega)\ud\omega,\\
Residual =R_w(\boldsymbol\theta)&= 8\int_{\omega_1}^\pi\rho_s(\omega)\rho_{\boldsymbol\theta}(\omega)\sin^{2}\left(\frac{\varphi_{\boldsymbol\theta}(\omega)}{2}\right)h(\omega)\ud\omega.\\
\end{align*}

En général, le résidu \(R_w\) est petit puisque \(\rho_s(\omega)\rho_{\boldsymbol\theta}(\omega)\) est proche de 0 dans l'intervalle \emph{stop-band}\footnote{
  En pratique ce n'est pas toujours le cas comme montré dans la section \ref{subsec-lppasymf}.}.
Il est de plus rare que les utilisateurs s'intéressent aux propriétés de déphasage dans les fréquences \emph{stop-band}.
C'est pourquoi, pour la construction de filtres linéaires les auteurs suggèrent de faire une minimisation d'une somme pondérée des trois premiers critères :
\[
\mathcal{M}(\vartheta_{1},\vartheta_{2})=\vartheta_{1}T_w(\boldsymbol\theta)+\vartheta_{2}S_w(\boldsymbol\theta)+(1-\vartheta_{1}-\vartheta_{2})A_w(\boldsymbol\theta).
\]
Comme le montrent \textcite{tuckerwildi2020}, cette méthode peut également être étendue au cas multivarié, ce qui permet de prendre en compte les corrélations entre les composantes de différentes séries.

Un des inconvénients de cette méthode est qu'il n'y a pas de garantie d'unicité de la solution.
En revanche, son avantage par rapport à la méthode FST (section \ref{subsec-GuggemosEtAl}) est que la décomposition de l'erreur quadratique moyenne permet de normaliser les différents indicateurs, et les coefficients \(\vartheta_{1}\), \(\vartheta_{2}\) et \(1-\vartheta_{1}-\vartheta_{2}\) peuvent être comparés entre eux.

Cette méthode étant totalement dépendante des données, son intégration dans des algorithmes non-paramétriques tels que X-11 serait compliquée.
C'est pourquoi elle n'est pour l'instant pas comparée aux autres.
Pour avoir des critères qui ne dépendent pas des données, une idée serait de fixer la densité spectrale, par exemple à celle d'un bruit blanc (\(h_{WN}(x)=1\)) ou d'une marche aléatoire (\(h_{RW}(x)=\frac{1}{2(1-\cos(x))}\)).
C'est ce qui est implémenté dans la fonction \texttt{rjd3filters::dfa\_filter()}.

\hypertarget{sec-comparison}{%
\section{Comparaison des différentes méthodes}\label{sec-comparison}}

Les différentes méthodes de construction de moyennes mobiles asymétriques sont comparées sur des données simulées et des données réelles.

Pour toutes les séries, un filtre symétrique de 13 termes est utilisé.
Ces méthodes sont également comparées aux estimations obtenues en prolongeant la série par un modèle ARIMA\footnote{Le modèle ARIMA est déterminé automatiquement en n'utilisant pas de retard saisonnier (les séries étant désaisonnalisées) et en n'utilisant aucune variable extérieure (comme des régresseurs externes pour la correction des points atypiques).} puis en appliquant un filtre symétrique de Henderson de 13 termes.

La performance des différentes méthodes est jugée sur des critères relatifs aux révisions (entre deux estimations consécutives et par rapport à l'estimation finale) et sur le nombre de périodes nécessaires pour détecter les points de retournements.

\hypertarget{suxe9ries-simuluxe9es}{%
\subsection{Séries simulées}\label{suxe9ries-simuluxe9es}}

\hypertarget{muxe9thodologie}{%
\subsubsection{Méthodologie}\label{muxe9thodologie}}

En suivant une méthodologie proche de celle de \textcite{DarneDagum2009}, neuf séries mensuelles sont simulées entre janvier 1960 et décembre 2020 avec différent niveaux de variabilité.
Chaque série simulée \(y_t= C_t+ T_t + I_t\) peut s'écrire comme la somme de trois composantes :

\begin{itemize}
\item
  le cycle \(C_t = \rho [\cos (2 \pi t / \lambda) +\sin (2 \pi t / \lambda)]\), \(\lambda\) est fixé à 72 (cycles de 6 ans, il y a donc 19 points de retournement détectables) ;
\item
  la tendance \(T_t = T_{t-1} + \nu_t\) avec \(\nu_t \sim \mathcal{N}(0, \sigma_\nu^2)\), \(\sigma_\nu\) étant fixé à \(0,08\) ;
\item
  et l'irrégulier \(I_t = e_t\) avec \(e_t \sim \mathcal{N}(0, \sigma_e^2)\).
\end{itemize}

Pour les différentes simulations, nous faisons varier les paramètres \(\rho\) et \(\sigma_e^2\) afin d'avoir des séries avec différents rapports signal sur bruit (voir graphique \ref{fig:graphs-data-simul}) :

\begin{itemize}
\item
  Fort rapport signal sur bruit (c'est-à-dire un I-C ratio faible et une faible variabilité)~: \(\sigma_e^2=0,2\) et \(\rho = 3,0,\, 3,5\) ou \(4,0\) (I-C ratio compris entre 0,9 et 0,7) ;
\item
  Rapport signal sur bruit moyen (c'est-à-dire un I-C ratio moyen et une variabilité moyenne)~: \(\sigma_e^2=0,3\) et \(\rho = 1,5,\, 2,0\) ou \(3,0\) (I-C ratio compris entre 2,3 et 1,4) ;
\item
  Faible rapport signal sur bruit (c'est-à-dire un I-C ratio fort et une forte variabilité)~: \(\sigma_e^2=0,4\) et \(\rho = 0,5,\, 0,7\) ou \(1,0\) (I-C ratio compris entre 8,9 et 5,2).
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/simulations/simul_data} 

}

\caption[Séries simulées à variabilité faible (\(\sigma_e^2=0,2\) et \(\rho = 3,5\)), moyenne (\(\sigma_e^2=0,3\) et \(\rho = 2,0\)) et forte (\(\sigma_e^2=0,4\) et \(\rho = 1,0\))]{Séries simulées à variabilité faible (\(\sigma_e^2=0,2\) et \(\rho = 3,5\)), moyenne (\(\sigma_e^2=0,3\) et \(\rho = 2,0\)) et forte (\(\sigma_e^2=0,4\) et \(\rho = 1,0\)).}\label{fig:graphs-data-simul}

\footnotesize
\normalsize\end{figure}

Pour chaque série et chaque date, la tendance-cycle est estimée en utilisant les différentes méthodes présentées dans ce rapport.
Pour les régressions polynomiales locales, les filtres asymétriques sont calibrés en utilisant l'I-C ratio estimé à chaque date (en appliquant un filtre de Henderson de 13 termes) et pour la méthode FST, un quadrillage du plan est réalisé avec un pas de \(0,05\) et avec comme contraintes linéaires la préservation des polynômes de degrés 0 à 3.
Trois critères de qualité sont également calculés :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calcul du déphasage dans la détection des points de retournement. La définition de \textcite{Zellner1991} est utilisée pour déterminer les points de retournement :

  \begin{itemize}
  \tightlist
  \item
    on parle de redressement (\emph{upturn}) lorsqu'on passe d'une phase de récession à une phase d'expansion de l'économie.
    C'est le cas à la date \(t\) lorsque \(y_{t-3}\geq y_{t-2}\geq y_{t-1}<y_t\leq y_{t+1}\).\\
  \item
    on parle de ralentissement (\emph{downturn}) lorsqu'on passe d'une phase d'expansion à une phase de récession.
    C'est le cas à la date \(t\) lorsque \(y_{t-3}\leq y_{t-2}\leq y_{t-1}>y_t\geq y_{t+1}\).\\
    Il faut donc au moins 2 mois pour détecter un point de retournement.\\
    Le déphasage est souvent défini comme le nombre de mois nécessaires pour détecter le bon point de retournement (i.e., le point de retournement sur la composante cyclique).
    Nous utilisons ici un critère légèrement modifié : le déphasage est défini comme le nombre de mois nécessaires pour détecter le bon point de retournement sans aucune révision future.
    Il peut en effet arriver que le bon point de retournement soit détecté par des filtres asymétriques mais ne le soit pas avec l'estimation finale avec un filtre symétrique
    (c'est le cas de 41 points de retournements sur l'ensemble des 9 séries avec les filtres asymétriques de Musgrave) ou qu'il y ait des révisions dans les estimations successives (c'est le cas de 7 points de retournements sur l'ensemble des 9 séries avec les filtres asymétriques de Musgrave).
    Finalement, relativement peu de points de retournement sont détectés à la bonne date avec l'estimation finale.
    Avec le filtre de Henderson de 13 termes, 18 sont correctement détectés sur les séries avec une faible variabilité (sur les 57 possibles), 11 sur les séries à variabilité moyenne et 12 sur les séries à forte variabilité.
  \end{itemize}
\item
  Calcul de deux critères de révisions : la moyenne des écarts relatifs entre la \(q\)\textsuperscript{e} estimation et la dernière estimation \(MAE_{fe}(q)\) et la moyenne des écarts relatifs entre la \(q\)\textsuperscript{e} et la \(q+1\)\textsuperscript{e} estimation \(MAE_{qe}(q)\)
  \[
  MAE_{fe}(q)=\mathbb E\left[
  \left|\frac{
  y_{t|t+q} -  y_{t|last}
  }{
   y_{t|last}
  }\right|
  \right]
  \quad\text{et}\quad
  MAE_{qe}(q)=\mathbb E\left[
  \left|\frac{
  y_{t|t+q} - y_{t|t+q+1}
  }{
  y_{t|t+q+1}
  }\right|
  \right].
  \]
\end{enumerate}

Pour le choix des poids dans l'approche FST, l'idée retenue dans cette étude est de faire un quadrillage du plan \([0,1]^3\) avec un pas de 0,05 et en imposant \(\alpha + \beta + \gamma = 1\)\footnote{
  Comme il n'est pas possible d'avoir un poids associé à la \emph{timeliness} (\(\gamma\)) égal à 1 (sinon la fonction objectif n'est pas strictement convexe), on construit également un filtre avec un poids très proche de 1 (\(1-1/1000\)).}.
Pour chaque combinaison de poids, quatre ensembles de moyennes mobiles sont construits en forçant dans la minimisation la préservation de polynômes de degré 0 à 3.
Le filtre symétrique utilisé est toujours celui de Henderson.
Le degré de préservation polynomiale et l'ensemble de poids retenus sont ceux minimisant (en moyenne) le déphasage sur les séries simulées : pour l'ensemble des degrés de liberté, il s'agit toujours du filtre préservant les polynômes de degré 2 avec \(\alpha = 0,00\) (\emph{fidelity}), \(\beta =0,05\) (\emph{smoothness}) et \(\gamma = 0,95\) (\emph{timeliness}).

\hypertarget{comparaison}{%
\subsubsection{Comparaison}\label{comparaison}}

En excluant pour l'instant les paramétrisations locales des filtres polynomiaux, c'est le filtre FST qui semble donner les meilleurs résultats en termes de déphasage dans la détection des points de retournement (figure \ref{fig:graphstpsimul}), suivi du filtre polynomial linéaire-constant (LC)\footnote{
  Par simplification, pour l'approche polynomiale locale, nous ne présenterons ici que les résultats avec le noyau de Henderson.
  Il est en effet difficile de comparer proprement les résultats entre les différents noyaux car le filtre symétrique n'est pas le même.
  Le filtre symétrique étant celui utilisé pour la détection finale des points de retournement, cela a pour conséquence que des points de retournement différents peuvent être détectés.
  Par exemple, pour le filtre LC, sur les trois séries ayant une variabilité moyenne, seul 1 point de retournement est correctement détecté par l'ensemble des noyaux.
  Toutefois, une première analyse des résultats montrent que les différents noyaux ont des performances proches en termes de déphasage et de révisions, sauf le noyau uniforme qui produit de moins bons résultats.}.
Les performances sont relativement proches de celles obtenues en prolongeant la série grâce à un modèle ARIMA.
Toutefois, lorsque la variabilité est faible, le filtre LC semble donner de moins bons résultats et c'est le filtre polynomial quadratique-linéaire (QL) qui semble donner les meilleurs résultats.
C'est le filtre \(b_{q,\varphi}\) qui minimise le déphasage qui donne les moins bons résultats.
Les autres filtres issus des espaces de Hilbert à noyau reproduisant (RKHS) ont également une grande variabilité en termes de déphasage.
Cela peut s'expliquer par le fait que la courbe des coefficients des moyennes mobiles asymétriques sont assez éloignées des coefficients du filtre symétrique\footnote{
  Cet écart provient du fait que la fenêtre optimale retenue \(b_{q,\varphi}\) est croissante jusqu'à \(b_{5,\varphi}=10,39\) et s'écarte donc de la valeur utilisée du filtre symétrique (\(h+1=7\)).} : il y a donc potentiellement beaucoup de révisions dans la détection des points de retournement.
En effet, lorsque le déphasage est défini comme la première date à laquelle le bon point de retournement est détecté, c'est le filtre \(b_{q,G}\) qui donne les meilleurs résultats.

Pour les séries à variabilité moyenne, la paramétrisation locale des filtres LC et QL permet de réduire le déphasage.
Pour les séries à variabilité forte, le déphasage est uniquement réduit en utilisant les paramètres finaux \(\hat\delta\) : l'estimation en temps réel semble ajouter plus de variabilité.
Pour les séries à variabilité faible, les performances semblent légèrement améliorées uniquement avec le filtre LC.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{img/simulations/phase_shift_simul} 

}

\caption[Distribution des déphasages sur les séries simulées]{Distribution des déphasages sur les séries simulées.}\label{fig:graphstpsimul}

\footnotesize


\emph{Notes} : \emph{Filtres polynomiaux : \emph{Linear-Constant} (LC) et \emph{Quadratic-Linear} (QL) avec paramétrisation locale de l'IC-ratio (en utilisant un estimateur en temps-réel et l'estimateur final de ce paramètre), \emph{Cubic-Quadratic} (CQ) et direct (DAF).}

\emph{Filtres RKHS : minimisant l'erreur quadratique moyenne (\(b_{q,\Gamma}\)), les révisions liées à la fonction de gain (\(b_{q,G}\)) et celles liées au déphasage (\(b_{q,\varphi}\)).}

\emph{ARIMA : prolongement de la série par un ARIMA détecté automatiquement et utilisation du filtre symétrique d'Henderson.}

\emph{FST : filtre obtenu en préservant les polynômes de degré 2 et avec \(\alpha=0,00\), \(\beta=0,05\) et \(\gamma=0,95\).}
\normalsize\end{figure}

Concernant les révisions, la variabilité de la série a peu d'impact sur les performances respectives des différentes méthodes mais joue sur les ordres de grandeurs, c'est pourquoi les résultats ne sont présentés qu'avec les séries à variabilité moyenne (tableau \ref{tab:simulrev}).
Globalement, les filtres LC minimisent toujours les révisions (avec globalement peu d'impact de la paramétrisation locale des filtres) et les révisions sont plus importantes avec les filtres polynomiaux cubique-quadratique (CQ), direct (DAF) et les filtres RKHS autres que \(b_{q,\varphi}\).
Par ailleurs, c'est le filtre \(b_{q,\varphi}\) qui conduit à la révision la plus grande entre l'avant-dernière et la dernière estimation, ce qui s'explique par la différence importante entre la courbe des coefficients de la dernière moyenne mobile asymétrique et celle du filtre symétrique.
Les autres filtres RKHS, \(b_{q,\Gamma}\) et \(b_{q,G}\), conduisent eux à de fortes révisions entre la quatrième et la cinquième estimation.

Pour le filtre QL, il y a une forte révision entre la deuxième et la troisième estimation : cela peut venir du fait que pour la deuxième estimation (lorsqu'on connait un point dans le futur), le filtre QL associe un poids plus important à l'estimation en \(t+1\) qu'à l'estimation en \(t\), ce qui crée une discontinuité.
Cette révision est fortement réduite avec la paramétrisation locale du filtre.
Pour les filtres polynomiaux autres que le filtre LC, les révisions importantes à la première estimation étaient prévisibles au vu de la courbe des coefficients : un poids très important est associé à l'observation courante et il y une forte discontinuité entre la moyenne mobile utilisée pour l'estimation en temps réel (lorsqu'aucun point dans le futur n'est connu) et les autres moyennes mobiles.

Enfin, pour le filtre issu de la méthode FST, la première estimation est fortement révisée, ce qui pouvait être attendu au vu de l'analyse de la courbe de coefficients (figure \ref{fig:graphsfst}).
Ainsi, pour cette méthode, utiliser le même ensemble de poids (\(\alpha\), \(\beta\) et \(\gamma\)) pour construire l'ensemble des moyennes mobiles asymétriques ne semble pas pertinent : pour le filtre utilisé en temps réel (lorsqu'aucun point dans le futur est connu), on pourrait préférer donner un poids plus important à la fidélité afin de minimiser ces révisions.

Le prolongement de la série par un modèle ARIMA donne des révisions avec les dernières estimations du même ordre de grandeur que le filtre LC mais des révisions légèrement plus importantes entre les estimations consécutives, notamment entre la quatrième et la cinquième estimation (on pouvait s'y attendre comme souligné dans la section \ref{subec:mmetprev}).

\begin{table}[!h]

\caption{\label{tab:simulrev}Moyenne des écarts relatifs des révisions pour les différents filtres sur les séries simulées à variabilité moyenne.}
{
\centering
\begin{tabular}[t]{ccccccc}
\toprule
Méthode & $q=0$ & $q=1$ & $q=2$ & $q=3$ & $q=4$ & $q=5$\\
\midrule
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{$MAE_{fe}(q) = \mathbb E\left[\left|(y_{t|t+q} -  y_{t|last})/y_{t|last}\right|\right]$}}\\
\hspace{1em}LC & 0,21 & 0,10 & 0,03 & 0,03 & 0,03 & 0,01\\
\hspace{1em}LC param. locale (finale) & 0,19 & 0,09 & 0,03 & 0,03 & 0,03 & 0,01\\
\hspace{1em}LC param. locale & 0,29 & 0,10 & 0,03 & 0,03 & 0,03 & 0,01\\
\hspace{1em}QL & 0,33 & 0,10 & 0,04 & 0,04 & 0,03 & 0,01\\
\hspace{1em}QL param. locale (finale) & 0,21 & 0,10 & 0,03 & 0,03 & 0,03 & 0,01\\
\hspace{1em}QL param. locale & 0,30 & 0,10 & 0,04 & 0,03 & 0,03 & 0,01\\
\hspace{1em}CQ & 0,45 & 0,13 & 0,13 & 0,09 & 0,06 & 0,02\\
\hspace{1em}DAF & 0,47 & 0,15 & 0,15 & 0,09 & 0,06 & 0,02\\
\hspace{1em}$b_{q,\Gamma}$ & 0,63 & 0,21 & 0,03 & 0,09 & 0,09 & 0,04\\
\hspace{1em}$b_{q,G}$ & 0,83 & 0,37 & 0,03 & 0,09 & 0,09 & 0,04\\
\hspace{1em}$b_{q,\varphi}$ & 0,31 & 0,11 & 0,03 & 0,05 & 0,07 & 0,09\\
\hspace{1em}ARIMA & 0,22 & 0,10 & 0,03 & 0,03 & 0,03 & 0,01\\
\hspace{1em}FST & 0,36 & 0,13 & 0,05 & 0,05 & 0,04 & 0,02\\
\addlinespace[0.3em]
\multicolumn{7}{l}{\textbf{$MAE_{ce}(q)=\mathbb E\left[
\left|(y_{t|t+q} - y_{t|t+q+1})/y_{t|t+q+1}\right|
\right]$}}\\
\hspace{1em}LC & 0,19 & 0,10 & 0,02 & 0,01 & 0,07 & 0,01\\
\hspace{1em}LC param. locale (finale) & 0,20 & 0,10 & 0,03 & 0,01 & 0,05 & 0,01\\
\hspace{1em}LC param. locale & 0,24 & 0,11 & 0,03 & 0,01 & 0,05 & 0,01\\
\hspace{1em}QL & 0,29 & 3,46 & 0,00 & 0,03 & 0,04 & 0,01\\
\hspace{1em}QL param. locale (finale) & 0,31 & 0,11 & 0,02 & 0,01 & 0,04 & 0,01\\
\hspace{1em}QL param. locale & 0,24 & 0,16 & 0,00 & 0,03 & 0,04 & 0,01\\
\hspace{1em}CQ & 0,43 & 0,02 & 0,10 & 0,07 & 0,05 & 0,02\\
\hspace{1em}DAF & 0,66 & 0,24 & 0,11 & 0,14 & 0,06 & 0,02\\
\hspace{1em}$b_{q,\Gamma}$ & 0,38 & 0,32 & 0,09 & 0,00 & 0,41 & 0,04\\
\hspace{1em}$b_{q,G}$ & 0,70 & 0,46 & 0,10 & 0,00 & 0,43 & 0,04\\
\hspace{1em}$b_{q,\varphi}$ & 0,22 & 0,16 & 0,08 & 0,05 & 0,03 & 0,09\\
\hspace{1em}ARIMA & 0,21 & 0,13 & 0,02 & 0,02 & 0,25 & 0,01\\
\hspace{1em}FST & 0,31 & 0,11 & 0,02 & 0,02 & 0,03 & 0,02\\
\bottomrule
\end{tabular}
}
\footnotesize


\emph{Note} : \emph{Le paramètre \(q\) désigne le nombre de points dans le futur utilisés par la moyenne mobile (pour \(q=0\), estimation en temps réel).}
\normalsize\end{table}

\hypertarget{suxe9rie-ruxe9elle}{%
\subsection{Série réelle}\label{suxe9rie-ruxe9elle}}

Les différences entre les méthodes sont également illustrées à partir d'un exemple issu de la base FRED-MD (\textcite{fredmd}) contenant des séries économiques sur les États-Unis\footnote{
  Les séries étudiées correspondent à la base publiée en novembre 2022.}.
La série étudiée est le niveau d'emploi aux États-Unis (série \texttt{CE16OV}, utilisée en logarithme) autour du point de retournement de février 2001, cohérent avec la datation mensuelle des points de retournement.
Ce point et cette série ont été choisis car le retournement conjoncturel est particulièrement visible sur la série brute (voir figure \ref{fig:ce16ov} présentant les prévisions implicites), que cette série est utilisée pour la datation des cycles conjoncturels de l'économie et que la base FRED-MD facilite la reproductibilité des résultats (grâce à la disponibilité des séries publiées aux dates passées).
Cette série a une variabilité moyenne\footnote{
  La variabilité est déterminée en étudiant les séries jusqu'en janvier 2020.} (un filtre symétrique de 13 termes est donc adapté).
La figure \ref{fig:ce16ovlp} montre les estimations successives de la tendance-cycle avec les différentes méthodes étudiées.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{img/nber/ce16ov} 

}

\caption[Série brute et lissée (tendance-cycle) de l'emploi (en logarithme) aux États-Unis autour du point de retournement de février 2001]{Série brute et lissée (tendance-cycle) de l'emploi (en logarithme) aux États-Unis autour du point de retournement de février 2001.}\label{fig:ce16ov}

\footnotesize
\normalsize\end{figure}

Sur cette série, le déphasage est de 6 mois pour les méthodes RKHS, la méthode LC, la méthode CQ et le prolongement par ARIMA de la série.
Sauf pour la méthode LC (où le point de retournement est d'abord détecté en janvier 2001), le déphasage élevé provient de révisions dans le point de retournement détecté dans les estimations intermédiaires (même si le bon point de retournement est détecté au bout de 2 mois, il ne l'est plus au bout de 3 mois).
Il est de deux mois pour les autres méthodes (QL, DAF et FST).

La paramétrisation locale ne permet pas ici de réduire le déphasage mais permet de réduire les révisions.
Les polynomiales CQ et DAF conduisent à plus de variabilité dans les estimations intermédiaires, en particulier en février 2001.
Concernant les filtres RKHS, les estimations intermédiaires du filtre \(b_{q,\varphi}\) semblent très erratiques, ce qui s'explique, encore une fois, par le fait que les moyennes mobiles asymétriques utilisées lorsqu'on se rapproche du cas symétrique sont éloignées de la moyenne mobile symétrique d'Henderson.
Les filtres \(b_{q,\Gamma}\) et \(b_{q,G}\) conduisent à des estimations intermédiaires relativement constantes (estimations en temps réel proches des estimations lorsque quelques points dans le futur sont connus) : ces estimations intermédiaires (notamment celles en temps-réel) sont peu cohérentes en période de points de retournement et conduisent, dans ce cas, à un déphasage élevé.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{img/nber/ce16ov_fev2001} 

}

\caption[Estimations successives de la tendance-cycle de l'emploi (en logarithme) aux États-Unis]{Estimations successives de la tendance-cycle de l'emploi (en logarithme) aux États-Unis.}\label{fig:ce16ovlp}

\footnotesize


\emph{Notes} : \emph{Filtres polynomiaux : \emph{Linear-Constant} (LC) et \emph{Quadratic-Linear} (QL) avec paramétrisation locale de l'IC-ratio (en utilisant un estimateur en temps-réel et l'estimateur final de ce paramètre), \emph{Cubic-Quadratic} (CQ) et direct (DAF).}

\emph{Filtres RKHS : minimisant l'erreur quadratique moyenne (\(b_{q,\Gamma}\)), les révisions liées à la fonction de gain (\(b_{q,G}\)) et celles liées au déphasage (\(b_{q,\varphi}\)).}

\emph{ARIMA : prolongement de la série par un ARIMA détecté automatiquement et utilisation du filtre symétrique d'Henderson.}

\emph{FST : filtre obtenu en préservant les polynômes de degré 2 et avec \(\alpha=0,00\), \(\beta=0,05\) et \(\gamma=0,95\).}
\normalsize\end{figure}

La qualité des estimations intermédiaires peut également être analysée grâces aux prévisions implicites des différentes méthodes (figure \ref{fig:ce16ov-previmp-lp}).
Pour rappel, il s'agit des prévisions de la série brute qui, en appliquant le filtre symétrique de Henderson sur la série prolongée, donnent les mêmes estimations que les moyennes mobiles asymétriques.
Les prévisions du modèle ARIMA sont naïves et ne prennent pas en compte le point de retournement, contrairement aux autres méthodes.
Autour du point de retournement, les prévisions implicites de la méthode FST sont peu plausibles (car assez éloignées de l'estimation finale), essentiellement du fait de la moyenne mobile utilisée pour l'estimation en temps réel (et donc pour la prévision à l'horizon de 6 mois).
Enfin, la paramétrisation locale du filtre QL permet d'aboutir à des prévisions bien plus cohérentes même si le déphasage n'est pas modifié.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{img/nber/ce16ov_fev2001_prev_imp} 

}

\caption[Prévisions implicites liées aux estimations successives de la tendance-cycle de l'emploi (en logarithme) aux États-Unis]{Prévisions implicites liées aux estimations successives de la tendance-cycle de l'emploi (en logarithme) aux États-Unis.}\label{fig:ce16ov-previmp-lp}

\footnotesize


\emph{Notes} : \emph{L'axe des ordonnées n'est pas le même entre les différents graphiques.}

\emph{Filtres polynomiaux : \emph{Linear-Constant} (LC) et \emph{Quadratic-Linear} (QL) avec paramétrisation locale de l'IC-ratio (en utilisant un estimateur en temps-réel et l'estimateur final de ce paramètre), \emph{Cubic-Quadratic} (CQ) et direct (DAF).}

\emph{Filtres RKHS : minimisant l'erreur quadratique moyenne (\(b_{q,\Gamma}\)), les révisions liées à la fonction de gain (\(b_{q,G}\)) et celles liées au déphasage (\(b_{q,\varphi}\)).}

\emph{ARIMA : prolongement de la série par un ARIMA détecté automatiquement et utilisation du filtre symétrique d'Henderson.}

\emph{FST : filtre obtenu en préservant les polynômes de degré 2 et avec \(\alpha=0,00\), \(\beta=0,05\) et \(\gamma=0,95\).}
\normalsize\end{figure}

\hypertarget{discussion}{%
\subsection{Discussion}\label{discussion}}

Plusieurs enseignements peuvent être tirés de cette analyse empirique sur données simulées et réelles.

Sur les méthodes polynomiales locales, chercher à préserver des tendances polynomiales de degré supérieur à 2 (méthodes cubique-quadratique, CQ, et directe, DAF) augmente la variance des estimations (et donc l'ampleur des révisions) et le délai pour détecter des points de retournement.
Ainsi, l'estimation en temps réel des méthodes utilisant les filtres asymétriques directs (DAF) peut être facilement améliorée en utilisant le filtre linéaire-constant (LC) ou quadratique-linéaire (QL).
C'est par exemple le cas de la méthode de désaisonnalisation STL (\emph{Seasonal-Trend decomposition based on Loess}) proposée par \textcite{cleveland90} qui modélise par défaut une tendance locale de degré 1 : le gain à utiliser la méthode LC est donc dans ce cas limité mais il peut être important lorsque ce degré est modifié par l'utilisateur.

La paramétrisation locale des filtres polynomiaux locaux permet d'améliorer la qualité des estimations, surtout pour le filtre QL.
Même lorsque le déphasage n'est pas diminué, elle permet de réduire les révisions et d'avoir des estimations plus cohérentes, notamment en termes de prévisions implicites.
De plus, estimer localement et en temps-réel les paramètres des méthodes polynomiale permet d'améliorer les estimations.

Concernant les méthodes fondées sur les espaces de Hilbert à noyau reproduisant (RKHS), les deux premières estimations des filtres \(b_{q,\Gamma}\) et \(b_{q,G}\) semblent peu cohérentes (ce qui a peu d'impact sur le déphasage puisqu'il faut au moins 2 mois pour détecter un point de retournement), comme les dernières estimations du filtre \(b_{q,\varphi}\) (ce qui peut provenir d'un problème d'optimisation).
Cela suggère que la méthode utilisée pour calibrer ces filtres n'est pas optimale pour tous les horizons de prévision.
En effet, pour les premières estimations, on cherche plutôt à minimiser le déphasage (\(b_{q,\varphi}\)) et lorsque que l'on se rapproche du cas symétrique (lorsqu'au moins 4 points dans le futur sont connus, \(q \geq 4\)) on cherche plutôt à minimiser les révisions (\(b_{q,\Gamma}\) ou \(b_{q,G}\)).
Les performances du filtre \(b_{q,\varphi}\) pourraient ainsi sûrement être améliorées en utilisant d'autres moyennes mobiles lorsqu'on se rapproche du filtre symétrique (\(q \geq 4\)).

Pour l'approche \emph{Fidelity-Smoothness-Timeliness} (FST), les poids retenus pour les simulations (\(\alpha = 0,00\) \emph{fidelity}, \(\beta =0,05\) \emph{smoothness} et \(\gamma = 0,95\) \emph{timeliness}) semblent donner de bons résultats en termes de déphasage et de révisions lorsqu'au moins un point dans le futur est connu (\(q\geq1\)).
Les critères utilisés dans cette méthode n'étant pas normalisés, les poids retenus reviennent à donner un poids plus important à la \emph{timeliness} pour les premières estimations et un poids plus important à la \emph{smoothness} lorsqu'on se rapproche du cas d'utilisation du filtre symétrique.
En effet, plus \(q\) augmente (i.e., plus on utilise de points dans le futur), plus le déphasage et donc le critère associé à la \emph{timeliness} est petit : le poids associé à la \emph{smoothness} augmente donc et les moyennes mobiles trouvées se rapprochent du filtre symétrique d'Henderson (qui peut être construit en ne minimisant que la \emph{smoothness}).
En revanche, pour l'estimation en temps-réel (\(q=0\)), les statistiques sur les révisions suggèrent que l'on pourrait améliorer les résultats en utilisant d'autres pondérations (par exemple avec un poids non nul à la \emph{fidelity}).
Cela suggère également que si l'on utilisait la méthode ATS, où les critères sont normalisés, il faudrait sûrement associer un poids décroissant à la \emph{timeliness} en fonction de l'horizon de prévision.

Enfin, dans cet article et dans ceux associés aux méthodes étudiées, les filtres asymétriques sont appliqués et comparés sur des séries déjà désaisonnalisées ou sans saisonnalité (séries simulées).
Les révisions et donc le déphasage sont limités à 6 mois (lorsque le filtre symétrique est de 13 termes) et cela a l'avantage d'isoler les impacts des différents filtres des autres processus inhérents à la désaisonnalisation.
Il y a toutefois deux inconvénients à cette simplification :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  D'une part, l'estimation de la série désaisonnalisée dépend de la méthode utilisée pour extraire la tendance-cycle.
  Le choix de la méthode utilisée pour l'estimation de la tendance-cycle peut donc avoir un impact bien au-delà de 6 mois.
\item
  D'autre part, les moyennes mobiles étant des opérateurs linéaires, ils sont sensibles à la présence de points atypiques.
  L'application directe des méthodes peut donc conduire à des estimations biaisées, du fait de leur présence, alors que les méthodes de désaisonnalisation (comme la méthode X-13ARIMA) ont un module de correction des points atypiques.
  Par ailleurs, comme notamment montré par \textcite{dagum1996new}, le filtre symétrique final utilisé par X-13ARIMA pour extraire la tendance-cycle (et donc celui indirectement utilisé lorsqu'on applique les méthodes sur les séries désaisonnalisées) laisse passer environ 72 \% des cycles de 9 ou 10 mois (généralement associés à du bruit plutôt qu'à la tendance-cycle).
  Les filtres asymétriques finaux amplifient même les cycles de 9 ou 10 mois.
  Cela peut avoir pour conséquence l'introduction d'ondulations indésirables, c'est-à-dire la détection de faux points de retournement.
  Ce problème est réduit par la correction des points atypiques (ces cycles étant considérés comme de l'irrégulier).
  C'est ainsi que le \emph{Nonlinear Dagum Filter} (NLDF) a été développé et consiste à :

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    appliquer l'algorithme de correction des points atypiques de X-13ARIMA sur la série désaisonnalisée, puis de la prolonger par un modèle ARIMA ;
  \item
    effectuer une nouvelle correction des points atypiques en utilisant un seuil bien plus strict et appliquer ensuite le filtre symétrique de 13 termes.
    En supposant une distribution normale cela revient à modifier 48 \% des valeurs de l'irrégulier.
  \end{enumerate}

  Les \emph{cascade linear filter} (CLF), notamment étudiés dans \textcite{dagumBianconcini2023}, correspondent à une approximation des NLDF en utilisant un filtre de 13 termes et lorsque les prévisions sont obtenus à partir d'un modèle ARIMA(0,1,1) où \(\theta=0,40.\)
\end{enumerate}

Une piste d'étude serait alors d'étudier plus précisément l'effet des points atypiques sur l'estimation de la tendance-cycle et la détection des points de retournement, mais aussi d'explorer de nouveaux types de filtres asymétriques fondés sur des méthodes robustes (comme les régressions locales robustes, les médianes mobiles, etc.).

\newpage

\hypertarget{conclusion}{%
\section*{Conclusion}\label{conclusion}}
\addcontentsline{toc}{section}{Conclusion}

Pour l'analyse conjoncturelle, la majorité des statisticiens fait directement ou indirectement appel à des méthodes d'extraction de la tendance-cycle.
Elles sont par exemple utilisées pour réduire le bruit d'un indicateur afin d'en améliorer son analyse, et les modèles utilisés (comme les modèles de prévision) mobilisent généralement des séries désaisonnalisées qui s'appuient sur ces méthodes.

Le premier apport de cette étude est d'unifier la théorie autour des méthodes de construction des filtres asymétriques pour l'estimation en temps réel de la tendance-cycle.
Toutes ces méthodes peuvent se voir comme des cas particuliers d'une formulation générale de construction des moyennes mobiles, ce qui permet de montrer leurs similitudes et leurs différences et ainsi les comparer plus facilement.
Elles sont par ailleurs facilement mobilisables et comparables grâce à leur implémentation dans le \emph{package} \faIcon{r-project} \texttt{rjd3filters}.
Celui-ci permet d'utiliser plusieurs outils, comme la construction des prévisions implicites, qui peuvent aider les statisticiens à évaluer la qualité des estimations récentes des différents filtres.

La comparaison des différentes méthodes permet de tirer quelques enseignements pour la construction de ces moyennes mobiles.

Premièrement, en période de retournement conjoncturel, des filtres asymétriques utilisés comme alternative au prolongement de la série par modèle ARIMA peuvent permettre de réduire les révisions des estimations intermédiaires de la tendance-cycle et une détection plus rapide des points de retournement.

Deuxièmement, en fin de période, modéliser des tendances polynomiales de degré supérieur à trois (cubique-quadratique, CQ, et directe, DAF) semble introduire de la variance dans les estimations (et donc plus de révisions) sans permettre de détection plus rapide des points de retournement.
En fin de période, pour l'estimation de la tendance-cycle, nous pouvons donc nous restreindre aux méthodes modélisant des tendances polynomiales de degré au plus 2 (linéaire-constante, LC, et quadratique-linéaire, QL).
Par ailleurs, paramétrer localement les filtres polynomiaux permet de détecter plus rapidement les points de retournement (surtout pour le filtre QL).
Même lorsque le déphasage n'est pas diminué, la paramétrisation locale est recommandée car elle permet de réduire les révisions et d'avoir des estimations intermédiaires plus cohérentes avec les évolutions futures attendues.
En revanche, avec ces méthodes, la longueur du filtre utilisé doit être adaptée à la variabilité de la série : si le filtre utilisé est trop court (c'est-à-dire si la variabilité de la série est «~faible~»), conserver des tendances polynomiale de degré au plus 1 (méthode LC) produit de moins bons résultats en termes de détection des points de retournement.

Enfin, sur les méthodes s'appuyant sur l'optimisation d'une somme pondérée de certains critères (espaces de Hilbert à noyau reproduisant, RKHS, approche \emph{Fidelity-Smoothness-Timeliness}, FST, ou approche \emph{accuracy-timeliness-smoothness}, ATS), les performances pourraient être améliorées en utilisant des critères différents pour les différents filtres asymétriques.
En effet, pour les premières estimations (lorsque peu de points dans le futur sont connus), le conjoncturiste peut préférer minimiser le déphasage alors que lorsqu'on se rapproche du cas d'utilisation du filtre symétrique (c'est-à-dire lorsqu'on se rapproche des estimations finales), le critère le plus important pourrait être la réduction des révisions.
Par ailleurs, puisque les méthodes RKHS ou ATS sont sujettes à des problèmes d'optimisation (non unicité de la solution), une attention particulière doit être portée aux résultats de ces méthodes.
Pour le filtre RKHS \(b_{q,\varphi}\), minimisant le déphasage, cela conduit par exemple à de grandes révisions entre l'avant-dernière et la dernière estimation.

Cette étude pourrait être étendue de plusieurs manières.

Tout d'abord, elle n'est pas exhaustive et pourrait donc être complétée.\\
Parmi les approches étudiées, l'extension proposée aux méthodes polynomiales locales afin d'ajouter un critère sur le déphasage pourrait donner des résultats prometteurs.
Parmi les approches récentes non étudiées, nous pouvons citer \textcite{vasyechko2014new} qui utilisent le noyau d'Epanechnikov pour construire des filtres asymétriques de 13 termes\footnote{
  Cela consiste à choisir la fenêtre par la méthode des plus proches voisins : quel que soit le nombre de points dans le futur connus, on utilisera toujours 13 observations pour estimer la tendance-cycle.
  Pour l'estimation en temps-réel, la moyenne mobile utilisera donc l'observation courante et 12 points passés.
  Ainsi, pour les estimations intermédiaires, on utilisera plus de points dans le passé que pour le filtre symétrique.} ; \textcite{FengSchafer2021} qui proposent, en fin de période, l'utilisation de poids optimaux (au sens de l'erreur quadratique moyenne) dans les régressions polynomiales locales ; ou \textcite{dagumBianconcini2023} qui étudient également des filtres symétriques alternatifs à celui d'Henderson.

Parmi les pistes d'extension, on pourrait s'intéresser à l'impact de la longueur des filtres dans la détection des points de retournement.
En effet, les filtres asymétriques sont calibrés avec des indicateurs calculés pour l'estimation des filtres symétriques (par exemple pour déterminer automatiquement sa longueur), alors qu'une estimation locale pourrait être préférée.
Par ailleurs, nous nous sommes concentrés uniquement sur les séries mensuelles dont le filtre symétrique est de 13 termes, mais les résultats peuvent être différents si le filtre symétrique étudié est plus long/court et si l'on étudie des séries à d'autres fréquences (trimestrielles ou journalières par exemple).

Une autre piste pourrait être d'étudier l'impact des points atypiques : les moyennes mobiles, comme tout opérateur linéaire, sont très sensibles à la présence des points atypiques.
Pour limiter leur impact, dans X-13ARIMA une forte correction des points atypiques est effectuée sur la composante irrégulière avant d'appliquer les filtres pour extraire la tendance-cycle.
Cela amène donc à étudier l'impact de ces points sur l'estimation de la tendance-cycle et des points de retournement, mais aussi à explorer de nouveaux types de filtres asymétriques fondés sur des méthodes robustes (comme les régressions locales robustes ou les médianes mobiles).

\newpage

\hypertarget{appendix-annexe}{%
\appendix}


\hypertarget{an-diag}{%
\section{Synthèse des liens entre les différentes méthodes de construction de moyennes mobiles}\label{an-diag}}

Cette annexe montre comment tous les filtres étudiés peuvent se retrouver à partir d'une formule générale de construction des moyennes mobiles.
Elle décrit également les relations d'équivalences entre les différentes méthodes.
Enfin, des graphiques synthétiques résument ces propriétés.

\hypertarget{formule-guxe9nuxe9rale-de-construction-des-filtres}{%
\subsection{Formule générale de construction des filtres}\label{formule-guxe9nuxe9rale-de-construction-des-filtres}}

Pour établir une formule générale englobant les principaux filtres linéaires, \textcite{ch15HBSA} définissent deux critères.
En changeant légèrement les notations utilisées par les auteurs afin d'avoir une formulation plus générale, ces deux critères peuvent s'écrire :
\begin{align}
I(\boldsymbol\theta,q,y_t,u_t)&=\E{(\Delta^{q}(M_{\boldsymbol\theta} y_t-u_t))^{2}} \label{eq:formulegen1} \\
J(\boldsymbol\theta,f, \omega_1,\omega_2)&=\int_{\omega_1}^{\omega_2} f\left[\rho_{\boldsymbol\theta}(\omega), \varphi_{\boldsymbol\theta} (\omega), \omega\right] \ud \omega \label{eq:formulegen2}
\end{align}
où \(y_t\) est la série étudiée, \(u_t\) une série de référence\footnote{
  La série de référence est en général un estimateur robuste de la tendance-cycle de \(Y_t\) que l'on cherche à approcher par application de la moyenne mobile \(M_{\boldsymbol\theta}\) sur les observations \(y_t\).
  Cet estimateur peut également dépendre de \(M_{\boldsymbol\theta}.\)} et \(\Delta\) est l'opérateur différence (\(\Delta y_t=y_t-y_{t-1}\) et \(\Delta^q=\underbrace{\Delta \circ \dots \circ \Delta}_{q\text{ fois}}\) pour \(q\in\N\)).
Dans la majorité des cas, la fonction \(f\) ne dépendra que de la fonction de gain, \(\rho_{\boldsymbol\theta}\), et de la fonction de déphasage, \(\varphi_{\boldsymbol\theta}\).
Dans ce cas, par simplification on écrira \(f\left[\rho_{\boldsymbol\theta}(\omega), \varphi_{\boldsymbol\theta} (\omega), \omega\right] = f\left[\rho_{\boldsymbol\theta}(\omega), \varphi_{\boldsymbol\theta} (\omega)\right]\).

La majorité des filtres linéaires peut s'obtenir par une minimisation d'une somme pondérée de ces critères, sous contrainte linéaire sur les coefficients~:
\[
\begin{cases}
\underset{\boldsymbol\theta}{\min} & \sum_i \alpha_i I(\boldsymbol\theta,\, q_i,\, y_t,\, u_t^{(i)})+
\beta_iJ(\boldsymbol\theta,\, f_i,\, \omega_{1,i},\, \omega_{2,i})\\
s.t. & \boldsymbol C\boldsymbol \theta=\boldsymbol a
\end{cases}
\]

En effet :

\begin{itemize}
\item
  L'extension des méthodes polynomiales de \textcite{proietti2008} présentée dans la section \ref{subsec-lptimeliness} revient en effet à minimiser une somme pondérée de l'erreur quadratique de révision :
  \[
  \E{\left( \sum_{i=-h}^h\theta^s_{i}y_{t+s}-\sum_{i=-h}^qv_iy_{t+s} \right)^2}
  = I(v,\,0,\,y_t,\,M_{\boldsymbol \theta^s} y_t),
  \]
  et du critère de \emph{timeliness} :
  \[
  T_g(\boldsymbol\theta) = J(f\colon(\rho,\varphi)\mapsto\rho^2\sin(\varphi)^2,\,\omega_1, \,\omega_2).
  \]
  sous une contrainte linéaire.
\item
  Les critères des filtres symétriques de \textcite{GrayThomson1996} (section \ref{subsec-graythomson}) sont :
  \begin{align*}
  F_{GT}(\boldsymbol\theta)&=I(\boldsymbol\theta,0,y_t,g_t),\\
  S_{GT}(\boldsymbol\theta)&=I(\boldsymbol\theta,d+1,y_t,0).
  \end{align*}
  Les filtres asymétriques sont construits en minimisant l'erreur quadratique moyenne des révisions sous contraintes linéaires (préservation d'un polynôme de degré \(p\)).
\item
  Les trois critères de la méthode FST de \textcite{ch15HBSA} se retrouvent en notant \(y_t=TC_t+\varepsilon_t,\quad\varepsilon_t\sim\Norm(0,\sigma^2)\) avec \(TC_t\) une tendance déterministe :
  \begin{align*}
  F_g(\boldsymbol\theta) & = I(\boldsymbol\theta,\,0,\,y_t,\,\E{M_{\boldsymbol\theta} y_t})\\
  S_g(\boldsymbol\theta) & = I(\boldsymbol\theta,\,q,\,y_t,\,\E{M_{\boldsymbol\theta} y_t})\\
  T_g(\boldsymbol\theta) & = J(f\colon(\rho,\varphi)\mapsto\rho^2\sin(\varphi)^2,\,\omega_1, \,\omega_2).
  \end{align*}
\item
  Les quatre critères \(A_w\), \(T_w\), \(S_w\) et \(R_w\) des filtres de \textcite{trilemmaWMR2019} sont des cas particuliers du critère \(J\) défini dans l'équation \eqref{eq:formulegen1}.
  En notant :
  \[
  \begin{cases}
    f_1\colon&(\rho,\varphi, \omega)\mapsto2\left(\rho_s(\omega)-\rho\right)^{2}h(\omega) \\
    f_2\colon&(\rho,\varphi, \omega)\mapsto8\rho_s(\omega)\rho\sin^{2}\left(\frac{\varphi}{2}\right)h(\omega)
  \end{cases},
  \]
  on a :
  \begin{align*}
  A_w(\boldsymbol\theta)&= J(\boldsymbol\theta,f_1,0,\omega_1),\\
  T_w(\boldsymbol\theta)&= J(\boldsymbol\theta,f_2,0,\omega_1),\\
  S_w(\boldsymbol\theta)&= J(\boldsymbol\theta,f_1,\omega_1,\pi),\\
  R_w(\boldsymbol\theta)&= J(\boldsymbol\theta,f_2,\omega_1,\pi).
  \end{align*}
\item
  Les filtres des espaces de Hilbert à noyau reproduisant correspondent à une sélection optimale du paramètre \(b\) selon les critères précédents, en imposant comme contrainte linéaire que les coefficients soient sous la forme \(w_j=\frac{K_{d+1}(j/b)}{\sum_{i=-h}^{^p}K_{d+1}(i/b)}\).
\end{itemize}

\hypertarget{liens-entre-les-diffuxe9rentes-muxe9thodes}{%
\subsection{Liens entre les différentes méthodes}\label{liens-entre-les-diffuxe9rentes-muxe9thodes}}

\hypertarget{crituxe8res-de-gray-et-thomson-et-ceux-de-grun-rehomme-et-alii}{%
\subsubsection{\texorpdfstring{Critères de Gray et Thomson et ceux de Grun-Rehomme \emph{et alii}}{Critères de Gray et Thomson et ceux de Grun-Rehomme et alii}}\label{crituxe8res-de-gray-et-thomson-et-ceux-de-grun-rehomme-et-alii}}

Les critères \(F_g\) et \(S_g\) peuvent se déduire de \(F_{GT}\) et \(S_{GT}\).
Les approches de \textcite{GrayThomson1996} et \textcite{ch15HBSA} sont donc équivalentes pour la construction de filtres symétriques.

Notons \(\boldsymbol x_{t}=\begin{pmatrix}1 & t & t^{2} & \cdots & t^{d}\end{pmatrix}'\), \(\boldsymbol \beta=\begin{pmatrix}\beta_{0} & \cdots & \beta_{d}\end{pmatrix}'\).

Pour le critère de \emph{fidelity} :
\[
\hat{g}_{t}-g_{t}=\left(\sum_{j=-h}^{+h}\theta_{j}\boldsymbol x_{t+j}-\boldsymbol x_{t}\right)\boldsymbol \beta+\sum_{j=-h}^{+h}\theta_{j}\varepsilon_{t+j}+\sum_{j=-h}^{+h}\theta_{j}(\xi_{t+j}-\xi_{t}),
\]
Si \(\theta\) préserve les polynômes de degré \(d\) alors \(\sum_{j=-h}^{+h}\theta_{j}\boldsymbol x_{t+j}=\boldsymbol x_{t}\).
Puis, comme \(\xi_{t}\) et \(\varepsilon_{t}\) sont de moyenne nulle et sont non corrélés :
\[
F_{GT}(\boldsymbol\theta)=\E{(\hat{g}_{t}-g_{t})^{2}}=\boldsymbol \theta^{'}\left(\sigma^{2}\boldsymbol I+\boldsymbol \Omega\right)\boldsymbol \theta.
\]
Si \(\xi_t=0\) alors \(\boldsymbol \Omega=0\) et \(F_{GT}(\boldsymbol\theta)=F_g(\boldsymbol\theta)\).

Pour la \emph{smoothness} on a :
\[
\nabla^{q}\hat{g}_{t}=\sum_{j=h}^{h}\theta_{j}\underbrace{\nabla^{q}\left(\left(x_{j}-x_{0}\right)\beta\right)}_{=0\text{ si }q\geq d+1}+\sum_{j=h}^{h}\theta_{j}\nabla^{q}\varepsilon_{t+j}+\sum_{j=h}^{h}\theta_{j}\nabla^{q}\xi_{t+j}.
\]
D'où pour \(q=d+1\) :
\[
S_{GT}(\boldsymbol\theta)=\E{(\nabla^{q}\hat{g}_{t})^{2}}=\boldsymbol \theta^{'}\left(\sigma^{2}\boldsymbol B_{q}+\boldsymbol \Gamma_{q}\right)\theta.
\]
On peut, par ailleurs, montrer que pour toute série temporelle \(X_t\),
\[
\nabla^{q}(M_{\boldsymbol\theta}X_{t})=\left(-1\right)^{q}\sum_{k\in\Z}\left(\nabla^{q}\theta_{k}\right)X_{t+k-q},
\]
avec \(\theta_k=0\) pour \(|k|\geq h+1\).
Avec \(\xi_t=0\) on trouve donc que \(S_{GT}(\boldsymbol\theta)=\sigma^2S_g(\boldsymbol\theta)\).

\hypertarget{uxe9quivalence-avec-les-moindres-carruxe9s-ponduxe9ruxe9s}{%
\subsubsection{Équivalence avec les moindres carrés pondérés}\label{uxe9quivalence-avec-les-moindres-carruxe9s-ponduxe9ruxe9s}}

Du fait de la forme des filtres obtenus par la méthode de \textcite{ch15HBSA}, lorsque les contraintes imposées sont la préservation des tendances de degré \(d\), celle-ci est équivalente à une estimation locale d'une tendance polynomiale de degré \(d\) par moindres carrés généralisés.
En effet, dans ce cas, la solution est \(\hat{\boldsymbol \theta} = \boldsymbol \Sigma^{-1}\boldsymbol X_p'\left(\boldsymbol X_p\boldsymbol \Sigma^{-1}\boldsymbol X_p'\right)^{-1}\boldsymbol e_1\) avec \(\boldsymbol \Sigma=\alpha \boldsymbol F+\beta \boldsymbol S+ \gamma \boldsymbol T\), et c'est l'estimation de la constante obtenue par moindres carrés généralisés lorsque la variance des résidus est \(\boldsymbol \Sigma\).
L'équivalence entre les deux méthodes peut donc se voir comme un cas particulier de l'équivalence entre les moindres carrés pondérés et les moindres carrés généralisés.
C'est par exemple le cas des filtres symétriques d'Henderson qui peuvent s'obtenir par les deux méthodes.

Dans ce sens, \textcite{henderson1916note} a montré que les poids \(\boldsymbol w=(w_{-p},\dots, w_{f})\) associés à une moyenne mobile issue de la régression polynomiale locale par moindres carrés pondérés pouvaient s'écrire sous la forme :
\[
w_i = \kappa_i P\left(\frac{i}{p+f+1}\right)\text{ où }P\text{ est un polynôme de degré }d.
\]
Il a également montré l'inverse : toute moyenne mobile \(\boldsymbol \theta=(\theta_{-p},\dots, \theta_{f})\) qui préserve les tendances de degré \(d\) et dont le diagramme des coefficients (c'est-à-dire la courbe de \(\theta_t\) en fonction de \(t\)) change au plus \(d\) fois de signes peut être obtenue par une régression polynomiale locale de degré \(p\) estimée par moindres carrés pondérés.
Pour cela il suffit de trouver un polynôme \(P\left(\frac{X}{p+f+1}\right)\) de degré inférieur ou égal à \(d\) et dont les changements de signes coïncident avec les changements de signes de \(\boldsymbol \theta\).
Le noyau associé est alors \(\kappa_i=\frac{ \theta_i}{P\left(\frac{i}{p+f+1}\right)}\).
C'est le cas de tous les filtres symétriques issus de l'approche FST et de la majorité des filtres asymétriques (figure~\ref{fig:thhendersonh6}).

\begin{figure}[H]

{\centering \includegraphics{img/bookdown/pdf/thhendersonh6-1} 

}

\caption[Ensemble des poids pour lesquels la méthode FST n'est pas équivalente aux moindres carrés pondérés pour \(h=6\) (filtre symétrique de 13 termes), sous contrainte de préservation des polynômes de degré au plus 3 (\(d=0,1,2,3\))]{Ensemble des poids pour lesquels la méthode FST n'est pas équivalente aux moindres carrés pondérés pour \(h=6\) (filtre symétrique de 13 termes), sous contrainte de préservation des polynômes de degré au plus 3 (\(d=0,1,2,3\)).}\label{fig:thhendersonh6}

\footnotesize


\emph{Notes} : \emph{La \emph{smoothness} est calculée avec le paramètre \(d=3\) (\(S_g(\boldsymbol \theta) = \sum_{j}(\nabla^{3}\theta_{j})^{2}\)), comme pour le filtre symétrique d'Henderson.}

\emph{Les poids sont calculés à partir d'un quadrillage de 200 points de l'intervalle \([0,1]\) et en ne gardant que ceux tels que leur somme fasse 1.}

\emph{Lorsque tous les filtres FST sont équivalents à une approche polynomiale, aucun graphique n'est tracé. Pour un filtre symétrique de 13 ermes, la méthode FST est donc équivalente à une approche polynômiale sauf lorsqu'on ne contraint que la préservation des constante (\(d=0\)), pour les filtres nécessitant au plus deux observations futures (\(q=0,1\) ou 2) et seulement lorsque le poids associé à la \emph{timeliness} est proche de 1.}
\normalsize\end{figure}

Plus récemment, \textcite{LuatiProietti2011} se sont intéressés aux cas d'équivalences entre les moindres carrés pondérés et les moindres carrés généralisés pour déterminer des noyaux optimaux (au sens de Gauss-Markov).
Ils montrent que le noyau d'Epanechnikov est le noyau optimal associé à la régression polynomiale locale où le résidu, \(\varepsilon_t\), est un processus moyenne mobile (MA) non inversible d'ordre 1 (i.e., \(\varepsilon_t=(1-B)\xi_t\), avec \(\xi_t\) un bruit blanc).
Dans ce cas, la matrice \(\boldsymbol \Sigma\) de variance-covariance correspond à la matrice obtenue par le critère de \emph{smoothness} avec le paramètre \(q=2\) (\(\sum_{j}(\nabla^{2}\theta_{j})^{2} = \boldsymbol \theta'\boldsymbol \Sigma\boldsymbol \theta\)) : il y a donc équivalence avec l'approche FST.
De même, le noyau d'Henderson est le noyau optimal associé à la régression polynomiale locale où le résidu est un processus moyenne mobile (MA) non inversible d'ordre 2 (i.e., \(\varepsilon_t=(1-B)^2\xi_t\), avec \(\xi_t\) un bruit blanc).

\hypertarget{rkhs-et-polynuxf4mes-locaux}{%
\subsubsection{RKHS et polynômes locaux}\label{rkhs-et-polynuxf4mes-locaux}}

Comme montré dans la section précédente, la théorie des espaces de Hilbert à noyau reproduisant permet de reproduire les filtres symétriques par approximation polynomiale locale.
Comme le montrent \textcite{LuatiProietti2011}, cette théorie permet donc également de reproduire les filtres directs asymétriques (DAF), qui sont équivalents à l'approximation polynomiale locale mais en utilisant une fenêtre d'estimation asymétrique.
Cependant, ils ne peuvent pas être obtenus par la formalisation de \textcite{dagumbianconcini2008} mais par une discrétisation différente de la formule \eqref{eq:rkhskernelfun} :
\[
K_{d+1}(t)=\frac{\det{\boldsymbol H_{d+1}[1,\boldsymbol x_t]}}{\det{\boldsymbol H_{d+1}}}f_0(t),
\]
où \(\boldsymbol H_{d+1}[1,\boldsymbol x_t]\) est la matrice obtenue en remplaçant la première ligne de \(\boldsymbol H_{d+1}\) par \(\boldsymbol x_t=\begin{pmatrix} 1 & t & t^2 & \dots & t^d\end{pmatrix}'\).
Dans le cas discret, \(f_0(t)\) est remplacé par \(\kappa_j\) et en remplaçant les moments théoriques par les moments empiriques \(\boldsymbol H_{d+1}\) devient \(\boldsymbol X'_p\boldsymbol K_p\boldsymbol X_p\) et les coefficients du filtre asymétrique sont obtenus en utilisant la formule :
\[
w_{a,j}=\frac{\det{\boldsymbol X'_p\boldsymbol K_p\boldsymbol X_p[1,\boldsymbol x_j]}
}{
\det{\boldsymbol X'_p\boldsymbol K_p\boldsymbol X_p}
}\kappa_j.
\]
En effet, la règle de Cramer permet de trouver une solution explicite à l'équation des moindres carrés \((\boldsymbol X'_p\boldsymbol K_p\boldsymbol X_p)\hat{\boldsymbol \beta}=\boldsymbol X'_p\boldsymbol K_p \boldsymbol y_p\) où \(\hat \beta_0=\hat m_t\) :
\[
\hat \beta_0 = \frac{\det{\boldsymbol X'_p\boldsymbol K_p\boldsymbol X_p[1,\boldsymbol b]}}{\det{\boldsymbol X'_p\boldsymbol K_p\boldsymbol X_p}}f_0(t)
\quad\text{où}\quad \boldsymbol b=\boldsymbol X'_p\boldsymbol K_p\boldsymbol y_p.
\]
Comme \(\boldsymbol b=\sum_{j=-h}^q\boldsymbol x_j\kappa_jy_{t+j}\) il vient :
\[
\det{\boldsymbol X'_p\boldsymbol K_p\boldsymbol X_p[1,\boldsymbol b]} = \sum_{j=-h}^q\det{\boldsymbol X'_p\boldsymbol K_p\boldsymbol X_p[1,\boldsymbol x_j]}\kappa_jy_{t+j}.
\]
Et enfin :
\[
\hat \beta_0 = \hat m_t= \sum_{j=-h}^q\frac{\det{\boldsymbol X'_p\boldsymbol K_p\boldsymbol X_p[1,\boldsymbol x_j]}
}{
\det{\boldsymbol X'_p\boldsymbol K_p\boldsymbol X_p}
}\kappa_j y_{t+j}.
\]

\hypertarget{diagrammes-synthuxe9tiques}{%
\subsection{Diagrammes synthétiques}\label{diagrammes-synthuxe9tiques}}

Les figures \ref{fig:diag-gen-sym-fig} et \ref{fig:diag-gen-asym-fig} résument les liens entre toutes les méthodes.

\newpage

\begin{figure}[!ht]

{\centering \includegraphics[width=0.8\textheight,angle=90]{img/diagrams/diag-gen-sym-1} 

}

\caption[Synthèse des méthodes de construction de moyennes mobiles symétriques \(\boldsymbol\theta=(\theta_{-h},\dots,\theta_{h})\) de \(2h+1\) termes]{Synthèse des méthodes de construction de moyennes mobiles symétriques \(\boldsymbol\theta=(\theta_{-h},\dots,\theta_{h})\) de \(2h+1\) termes.}\label{fig:diag-gen-sym-fig}

\footnotesize


\emph{Note de lecture} : \emph{\(\boldsymbol X = \boldsymbol X_d = \begin{pmatrix} \boldsymbol x_0 \quad\cdots \quad \boldsymbol x_d \end{pmatrix}\) avec \(\boldsymbol x_i=\begin{pmatrix} (-h)^i \quad \cdots \quad (h)^i\end{pmatrix}'\).}
\normalsize\end{figure}

\begin{figure}[!ht]

{\centering \includegraphics[width=0.8\textheight,angle=90]{img/diagrams/diag-gen-asym-1} 

}

\caption[Synthèse des méthodes de construction de moyennes mobiles asymétriques \(\boldsymbol\theta=(\theta_{-h},\dots,\theta_{q})\), \(0\leq q< h\) avec \(\boldsymbol\theta^s\) le filtre symétrique de référence de \(2h+1\) termes]{Synthèse des méthodes de construction de moyennes mobiles asymétriques \(\boldsymbol\theta=(\theta_{-h},\dots,\theta_{q})\), \(0\leq q< h\) avec \(\boldsymbol\theta^s\) le filtre symétrique de référence de \(2h+1\) termes.}\label{fig:diag-gen-asym-fig}

\footnotesize


\emph{Note de lecture} : \emph{\(\boldsymbol X_d = \begin{pmatrix} \boldsymbol x_0 \quad\cdots \quad \boldsymbol x_d \end{pmatrix}\) avec \(\boldsymbol x_i=\begin{pmatrix} (-h)^i \quad \cdots \quad (q)^i\end{pmatrix}'\) et \(\boldsymbol X=\boldsymbol X_d\) avec \(q=h\).}
\normalsize\end{figure}

\newpage

\hypertarget{an-graphs}{%
\section{Coefficients, fonctions de gain et de déphasage}\label{an-graphs}}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{img/filters_used/lc} 

}

\caption[Coefficients, fonctions de gain et de déphasage pour le filtre \emph{Linear-Constant} (LC) avec \(I/C=3,5\)]{Coefficients, fonctions de gain et de déphasage pour le filtre \emph{Linear-Constant} (LC) avec \(I/C=3,5\).}\label{fig:graphslc}

\footnotesize
\normalsize\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{img/filters_used/ql} 

}

\caption[Coefficients, fonctions de gain et de déphasage pour le filtre \emph{Quadratic-Linear} (QL) avec \(I/C=3,5\)]{Coefficients, fonctions de gain et de déphasage pour le filtre \emph{Quadratic-Linear} (QL) avec \(I/C=3,5\).}\label{fig:graphsql}

\footnotesize
\normalsize\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{img/filters_used/cq} 

}

\caption[Coefficients, fonctions de gain et de déphasage pour le filtre \emph{Cubic-Quadratic} (CQ) avec \(I/C=3,5\)]{Coefficients, fonctions de gain et de déphasage pour le filtre \emph{Cubic-Quadratic} (CQ) avec \(I/C=3,5\).}\label{fig:graphscq}

\footnotesize
\normalsize\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{img/filters_used/daf} 

}

\caption[Coefficients, fonctions de gain et de déphasage pour le filtre asymétrique direct (DAF)]{Coefficients, fonctions de gain et de déphasage pour le filtre asymétrique direct (DAF).}\label{fig:graphsdaf}

\footnotesize
\normalsize\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{img/filters_used/frf} 

}

\caption[Coefficients, fonctions de gain et de déphasage pour le filtre RKHS \(b_{q,\Gamma}\)]{Coefficients, fonctions de gain et de déphasage pour le filtre RKHS \(b_{q,\Gamma}\).}\label{fig:graphsrkhsfrf}

\footnotesize
\normalsize\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{img/filters_used/gain} 

}

\caption[Coefficients, fonctions de gain et de déphasage pour le filtre RKHS \(b_{q,G}\)]{Coefficients, fonctions de gain et de déphasage pour le filtre RKHS \(b_{q,G}\).}\label{fig:graphsrkhsgain}

\footnotesize
\normalsize\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{img/filters_used/phase} 

}

\caption[Coefficients, fonctions de gain et de déphasage pour le filtre RKHS \(b_{q,\varphi}\)]{Coefficients, fonctions de gain et de déphasage pour le filtre RKHS \(b_{q,\varphi}\).}\label{fig:graphsrkhsphase}

\footnotesize
\normalsize\end{figure}

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{img/filters_used/fst} 

}

\caption[Coefficients, fonctions de gain et de déphasage pour le filtre FST préservant les polynômes de degré 2 avec \(\alpha = 0,00\) (\emph{fidelity}), \(\beta =0,05\) (\emph{smoothness}) et \(\gamma = 0,95\) (\emph{timeliness)}]{Coefficients, fonctions de gain et de déphasage pour le filtre FST préservant les polynômes de degré 2 avec \(\alpha = 0,00\) (\emph{fidelity}), \(\beta =0,05\) (\emph{smoothness}) et \(\gamma = 0,95\) (\emph{timeliness)}.}\label{fig:graphsfst}

\footnotesize
\normalsize\end{figure}

\hypertarget{ann-ex-r}{%
\section{Exemple sous R}\label{ann-ex-r}}

Cette annexe présente le code utilisé pour les exemples sur le climat des affaires dans l'industrie automobile.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# remotes::install\_github("rjdemetra/rjd3toolkit")}
\CommentTok{\# remotes::install\_github("rjdemetra/rjd3filters")}
\CommentTok{\# remotes::install\_github("rjdemetra/rjd3x11plus")}
\FunctionTok{library}\NormalTok{(rjd3filters)}
\FunctionTok{library}\NormalTok{(rjd3x11plus)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(patchwork)}
\FunctionTok{library}\NormalTok{(zoo)}
\FunctionTok{library}\NormalTok{(forecast)}
\FunctionTok{library}\NormalTok{(insee)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(insee}\SpecialCharTok{::}\FunctionTok{get\_insee\_idbank}\NormalTok{(}\StringTok{"001786505"}\NormalTok{))}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ y[}\FunctionTok{order}\NormalTok{(y[,}\StringTok{"TIME\_PERIOD"}\NormalTok{]), ]}
\NormalTok{first\_date }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{strsplit}\NormalTok{(y}\SpecialCharTok{$}\NormalTok{TIME\_PERIOD[}\DecValTok{1}\NormalTok{], }\StringTok{"{-}"}\NormalTok{)[[}\DecValTok{1}\NormalTok{]])}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{ts}\NormalTok{(y}\SpecialCharTok{$}\NormalTok{OBS\_VALUE, }\AttributeTok{start =}\NormalTok{ first\_date, }\AttributeTok{frequency =} \DecValTok{12}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{window}\NormalTok{(y, }\AttributeTok{end =} \FunctionTok{c}\NormalTok{(}\DecValTok{2023}\NormalTok{, }\DecValTok{5}\NormalTok{))}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{window}\NormalTok{(y, }\AttributeTok{start =} \DecValTok{2010}\NormalTok{)}
\NormalTok{last\_dates }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{tail}\NormalTok{(}\FunctionTok{time}\NormalTok{(y), }\DecValTok{7}\NormalTok{))}
\FunctionTok{names}\NormalTok{(last\_dates) }\OtherTok{\textless{}{-}} \FunctionTok{as.character}\NormalTok{(zoo}\SpecialCharTok{::}\FunctionTok{as.yearmon}\NormalTok{(last\_dates))}
\NormalTok{der\_est }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(last\_dates, }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{window}\NormalTok{(y, }\AttributeTok{end =}\NormalTok{ x))}

\CommentTok{\# MM de longueur 13 adaptées : }
\FunctionTok{select\_trend\_filter}\NormalTok{(y)}
\CommentTok{\# Estimation finale}
\NormalTok{tc\_f }\OtherTok{\textless{}{-}} \FunctionTok{henderson}\NormalTok{(y, }\AttributeTok{length =} \DecValTok{13}\NormalTok{, }\AttributeTok{musgrave =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# Estimation finale}
\FunctionTok{plot}\NormalTok{(y)}
\FunctionTok{lines}\NormalTok{(tc\_f, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}

\NormalTok{forecast}\SpecialCharTok{::}\FunctionTok{autoplot}\NormalTok{(}\FunctionTok{ts.union}\NormalTok{(y, tc\_f))}


\DocumentationTok{\#\# Méthode polynomiale}

\CommentTok{\# Calcul des IC{-}ratios : }
\NormalTok{icr }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(der\_est, }\ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{ic\_ratio}\NormalTok{(x, }\FunctionTok{henderson}\NormalTok{(x, }\AttributeTok{length =} \DecValTok{13}\NormalTok{, }\AttributeTok{musgrave =} \ConstantTok{FALSE}\NormalTok{))}
\NormalTok{\})}
\NormalTok{lp\_est }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"LC"}\NormalTok{, }\StringTok{"QL"}\NormalTok{, }\StringTok{"CQ"}\NormalTok{, }\StringTok{"DAF"}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(method) \{}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}\FunctionTok{seq\_along}\NormalTok{(icr), }\ControlFlowTok{function}\NormalTok{(i) \{}
\NormalTok{    lp\_coef }\OtherTok{\textless{}{-}} \FunctionTok{lp\_filter}\NormalTok{(}\AttributeTok{horizon =} \DecValTok{6}\NormalTok{,}
                         \AttributeTok{kernel =} \StringTok{"Henderson"}\NormalTok{,}
                         \AttributeTok{endpoints =}\NormalTok{ method,}
                         \AttributeTok{ic =}\NormalTok{ icr[i])}
\NormalTok{    rjd3filters}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(der\_est[[i]], lp\_coef)}
\NormalTok{  \})}
  \FunctionTok{names}\NormalTok{(res) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(der\_est)}
\NormalTok{  res}
\NormalTok{\})}
\NormalTok{lp\_if }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"LC"}\NormalTok{, }\StringTok{"QL"}\NormalTok{, }\StringTok{"CQ"}\NormalTok{, }\StringTok{"DAF"}\NormalTok{), }\ControlFlowTok{function}\NormalTok{(method) \{}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}\FunctionTok{seq\_along}\NormalTok{(icr), }\ControlFlowTok{function}\NormalTok{(i) \{}
\NormalTok{    lp\_coef }\OtherTok{\textless{}{-}} \FunctionTok{lp\_filter}\NormalTok{(}\AttributeTok{horizon =} \DecValTok{6}\NormalTok{,}
                         \AttributeTok{kernel =} \StringTok{"Henderson"}\NormalTok{,}
                         \AttributeTok{endpoints =}\NormalTok{ method,}
                         \AttributeTok{ic =}\NormalTok{ icr[i])}
    \FunctionTok{implicit\_forecast}\NormalTok{(der\_est[[i]], lp\_coef)}
\NormalTok{  \})}
  \FunctionTok{names}\NormalTok{(res) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(der\_est)}
\NormalTok{  res}
\NormalTok{\})}
\FunctionTok{names}\NormalTok{(lp\_est) }\OtherTok{\textless{}{-}} \FunctionTok{names}\NormalTok{(lp\_if) }\OtherTok{\textless{}{-}}  \FunctionTok{c}\NormalTok{(}\StringTok{"LC"}\NormalTok{, }\StringTok{"QL"}\NormalTok{, }\StringTok{"CQ"}\NormalTok{, }\StringTok{"DAF"}\NormalTok{)}

\CommentTok{\# Estimation locale des IC{-}ratios }
\CommentTok{\# On réplique l\textquotesingle{}estimation directe pour avoir }
\CommentTok{\# des estimateurs de la pente et de la concavité }

\NormalTok{gen\_MM }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{p=}\DecValTok{6}\NormalTok{, }\AttributeTok{q=}\NormalTok{p, }\AttributeTok{d=}\DecValTok{2}\NormalTok{)\{}
\NormalTok{  X\_gen }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{d =} \DecValTok{1}\NormalTok{, }\AttributeTok{p =} \DecValTok{6}\NormalTok{, }\AttributeTok{q =}\NormalTok{ p)\{}
    \FunctionTok{sapply}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\NormalTok{d, }\ControlFlowTok{function}\NormalTok{(exp) }\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{p, q)}\SpecialCharTok{\^{}}\NormalTok{exp)}
\NormalTok{  \}}
\NormalTok{  k }\OtherTok{=}\NormalTok{ rjd3filters}\SpecialCharTok{::}\FunctionTok{get\_kernel}\NormalTok{(}\StringTok{"Henderson"}\NormalTok{, }\AttributeTok{h =}\NormalTok{ p)}
\NormalTok{  k }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{rev}\NormalTok{(k}\SpecialCharTok{$}\NormalTok{coef[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]), k}\SpecialCharTok{$}\NormalTok{coef[}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,q)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{])}
\NormalTok{  K }\OtherTok{=} \FunctionTok{diag}\NormalTok{(k)}
\NormalTok{  X }\OtherTok{=} \FunctionTok{X\_gen}\NormalTok{(}\AttributeTok{d=}\NormalTok{d, }\AttributeTok{p =}\NormalTok{ p, }\AttributeTok{q =}\NormalTok{ q)}
\NormalTok{  e1 }\OtherTok{=}\NormalTok{ e2 }\OtherTok{=}\NormalTok{ e3 }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{, }\AttributeTok{nrow =}\NormalTok{ d}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}
\NormalTok{  e1[}\DecValTok{1}\NormalTok{] }\OtherTok{=} \DecValTok{1}
\NormalTok{  e2[}\DecValTok{2}\NormalTok{] }\OtherTok{=} \DecValTok{1}
\NormalTok{  e3[}\DecValTok{3}\NormalTok{] }\OtherTok{=} \DecValTok{1}
  \CommentTok{\# Estimateur de la constante}
\NormalTok{  M1 }\OtherTok{=}\NormalTok{ K }\SpecialCharTok{\%*\%}\NormalTok{ X }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ K }\SpecialCharTok{\%*\%}\NormalTok{ X, e1)}
  \CommentTok{\# estimateur de la pente}
\NormalTok{  M2 }\OtherTok{=}\NormalTok{ K }\SpecialCharTok{\%*\%}\NormalTok{ X }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ K }\SpecialCharTok{\%*\%}\NormalTok{ X, e2)}
  \CommentTok{\# estimateur de la concavité}
\NormalTok{  M3 }\OtherTok{=}\NormalTok{ K }\SpecialCharTok{\%*\%}\NormalTok{ X }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ K }\SpecialCharTok{\%*\%}\NormalTok{ X, e3)}
\NormalTok{  mm }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{const =}\NormalTok{ M1, }\AttributeTok{pente =}\NormalTok{ M2, }\AttributeTok{concav =}\NormalTok{ M3)}
  \FunctionTok{lapply}\NormalTok{(mm, moving\_average, }\AttributeTok{lags =} \SpecialCharTok{{-}}\NormalTok{p)}
\NormalTok{\}}
\NormalTok{all\_mm }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}\DecValTok{6}\SpecialCharTok{:}\DecValTok{0}\NormalTok{, gen\_MM, }\AttributeTok{p =} \DecValTok{6}\NormalTok{, }\AttributeTok{d =} \DecValTok{2}\NormalTok{)}
\NormalTok{est\_pente }\OtherTok{\textless{}{-}} \FunctionTok{finite\_filters}\NormalTok{(all\_mm[[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{pente,}
                            \FunctionTok{lapply}\NormalTok{(all\_mm[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{], }\StringTok{\textasciigrave{}}\AttributeTok{[[}\StringTok{\textasciigrave{}}\NormalTok{, }\StringTok{"pente"}\NormalTok{))}
\NormalTok{est\_concav }\OtherTok{\textless{}{-}} \FunctionTok{finite\_filters}\NormalTok{(all\_mm[[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{concav,}
                             \FunctionTok{lapply}\NormalTok{(all\_mm[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{], }\StringTok{\textasciigrave{}}\AttributeTok{[[}\StringTok{\textasciigrave{}}\NormalTok{, }\StringTok{"concav"}\NormalTok{))}

\NormalTok{henderson\_f }\OtherTok{\textless{}{-}} \FunctionTok{lp\_filter}\NormalTok{(}\AttributeTok{h=}\DecValTok{6}\NormalTok{)}\SpecialCharTok{@}\NormalTok{sfilter}
\NormalTok{lp\_filter2 }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(icr, }\AttributeTok{method =} \StringTok{"LC"}\NormalTok{, }\AttributeTok{h =} \DecValTok{6}\NormalTok{, }\AttributeTok{kernel =} \StringTok{"Henderson"}\NormalTok{)\{}
\NormalTok{  all\_coef }\OtherTok{=} \FunctionTok{lapply}\NormalTok{(icr, }\ControlFlowTok{function}\NormalTok{(ic)\{}
    \FunctionTok{lp\_filter}\NormalTok{(}\AttributeTok{horizon =}\NormalTok{ h,}
              \AttributeTok{kernel =}\NormalTok{ kernel,}
              \AttributeTok{endpoints =}\NormalTok{ method,}
              \AttributeTok{ic =}\NormalTok{ ic)}
\NormalTok{  \})}
\NormalTok{  rfilters }\OtherTok{=} \FunctionTok{lapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{h, }\ControlFlowTok{function}\NormalTok{(i)\{}
\NormalTok{    q }\OtherTok{\textless{}{-}}\NormalTok{ h }\SpecialCharTok{{-}}\NormalTok{ i}
\NormalTok{    all\_coef[[i]][,}\FunctionTok{sprintf}\NormalTok{(}\StringTok{"q=\%i"}\NormalTok{, q)]}
\NormalTok{  \})}
  \FunctionTok{finite\_filters}\NormalTok{(henderson\_f, }\AttributeTok{rfilters =}\NormalTok{ rfilters)}
\NormalTok{\}}
\NormalTok{loc\_lc\_est }\OtherTok{\textless{}{-}} 
  \FunctionTok{lapply}\NormalTok{(der\_est, }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{    est\_loc\_pente }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{tail}\NormalTok{(est\_pente }\SpecialCharTok{*}\NormalTok{ x, }\DecValTok{6}\NormalTok{))}
\NormalTok{    sigma2 }\OtherTok{\textless{}{-}} \FunctionTok{var\_estimator}\NormalTok{(x, henderson\_f)}
\NormalTok{    icr }\OtherTok{=} \DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(pi) }\SpecialCharTok{*}\NormalTok{ (est\_loc\_pente }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(sigma2)))}
\NormalTok{    lp\_coef }\OtherTok{=} \FunctionTok{lp\_filter2}\NormalTok{(}\AttributeTok{ic =}\NormalTok{ icr, }
                         \AttributeTok{method =} \StringTok{"LC"}\NormalTok{, }\AttributeTok{h =} \DecValTok{6}\NormalTok{, }\AttributeTok{kernel =} \StringTok{"Henderson"}\NormalTok{)}
\NormalTok{    rjd3filters}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(x, lp\_coef)}
\NormalTok{  \})}
\NormalTok{loc\_lc\_if }\OtherTok{\textless{}{-}} 
  \FunctionTok{lapply}\NormalTok{(der\_est, }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{    est\_loc\_pente }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{tail}\NormalTok{(est\_pente }\SpecialCharTok{*}\NormalTok{ x, }\DecValTok{6}\NormalTok{))}
\NormalTok{    sigma2 }\OtherTok{\textless{}{-}} \FunctionTok{var\_estimator}\NormalTok{(x, henderson\_f)}
\NormalTok{    icr }\OtherTok{=} \DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(pi) }\SpecialCharTok{*}\NormalTok{ (est\_loc\_pente }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(sigma2)))}
\NormalTok{    lp\_coef }\OtherTok{=} \FunctionTok{lp\_filter2}\NormalTok{(}\AttributeTok{ic =}\NormalTok{ icr, }
                         \AttributeTok{method =} \StringTok{"LC"}\NormalTok{, }\AttributeTok{h =} \DecValTok{6}\NormalTok{, }\AttributeTok{kernel =} \StringTok{"Henderson"}\NormalTok{)}
    \FunctionTok{implicit\_forecast}\NormalTok{(x, lp\_coef)}
\NormalTok{  \})}
\NormalTok{loc\_ql\_est }\OtherTok{\textless{}{-}} 
  \FunctionTok{lapply}\NormalTok{(der\_est, }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{    est\_loc\_concav }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{tail}\NormalTok{(est\_concav }\SpecialCharTok{*}\NormalTok{ x, }\DecValTok{6}\NormalTok{))}
\NormalTok{    sigma2 }\OtherTok{\textless{}{-}} \FunctionTok{var\_estimator}\NormalTok{(x, henderson\_f)}
\NormalTok{    icr }\OtherTok{=} \DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(pi) }\SpecialCharTok{*}\NormalTok{ (est\_loc\_concav }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(sigma2)))}
\NormalTok{    lp\_coef }\OtherTok{=} \FunctionTok{lp\_filter2}\NormalTok{(}\AttributeTok{ic =}\NormalTok{ icr, }
                         \AttributeTok{method =} \StringTok{"QL"}\NormalTok{, }\AttributeTok{h =} \DecValTok{6}\NormalTok{, }\AttributeTok{kernel =} \StringTok{"Henderson"}\NormalTok{)}
\NormalTok{    rjd3filters}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(x, lp\_coef)}
\NormalTok{  \})}
\NormalTok{loc\_ql\_if }\OtherTok{\textless{}{-}} 
  \FunctionTok{lapply}\NormalTok{(der\_est, }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{    est\_loc\_concav }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{tail}\NormalTok{(est\_concav }\SpecialCharTok{*}\NormalTok{ x, }\DecValTok{6}\NormalTok{))}
\NormalTok{    sigma2 }\OtherTok{\textless{}{-}} \FunctionTok{var\_estimator}\NormalTok{(x, henderson\_f)}
\NormalTok{    icr }\OtherTok{=} \DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(pi) }\SpecialCharTok{*}\NormalTok{ (est\_loc\_concav }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(sigma2)))}
\NormalTok{    lp\_coef }\OtherTok{=} \FunctionTok{lp\_filter2}\NormalTok{(}\AttributeTok{ic =}\NormalTok{ icr, }
                         \AttributeTok{method =} \StringTok{"QL"}\NormalTok{, }\AttributeTok{h =} \DecValTok{6}\NormalTok{, }\AttributeTok{kernel =} \StringTok{"Henderson"}\NormalTok{)}
    \FunctionTok{implicit\_forecast}\NormalTok{(x, lp\_coef)}
\NormalTok{  \})}
\CommentTok{\# Pour la méthode RKHS, pour le filtre b\_q,phi on utilise }
\CommentTok{\# le paramètre bw de l\textquotesingle{}article Dagum et Bianconcini (2015) }
\NormalTok{bw\_phase }\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{q=6}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{7}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{q=5}\StringTok{\textasciigrave{}} \OtherTok{=} \FloatTok{6.95}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{q=4}\StringTok{\textasciigrave{}} \OtherTok{=} \FloatTok{6.84}\NormalTok{,}
             \StringTok{\textasciigrave{}}\AttributeTok{q=3}\StringTok{\textasciigrave{}} \OtherTok{=} \FloatTok{6.85}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{q=2}\StringTok{\textasciigrave{}} \OtherTok{=} \FloatTok{7.34}\NormalTok{, }
             \StringTok{\textasciigrave{}}\AttributeTok{q=1}\StringTok{\textasciigrave{}} \OtherTok{=} \FloatTok{9.24}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{q=0}\StringTok{\textasciigrave{}} \OtherTok{=} \FloatTok{11.78}\NormalTok{)}
\NormalTok{rkhs\_filter }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \StringTok{"$b\_\{q,}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{Gamma\}$"} \OtherTok{=} \FunctionTok{rkhs\_filter}\NormalTok{(}
    \AttributeTok{horizon =} \DecValTok{6}\NormalTok{, }\AttributeTok{degree =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{asymmetricCriterion =} \StringTok{"FrequencyResponse"}\NormalTok{,}
    \AttributeTok{kernel =} \StringTok{"Biweight"}\NormalTok{, }\AttributeTok{passband =}\NormalTok{ pi}
\NormalTok{  ),}
  \StringTok{"$b\_\{q,G\}$"} \OtherTok{=} \FunctionTok{rkhs\_filter}\NormalTok{(}
    \AttributeTok{horizon =} \DecValTok{6}\NormalTok{, }\AttributeTok{degree =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{asymmetricCriterion =} \StringTok{"Accuracy"}\NormalTok{,}
    \AttributeTok{kernel =} \StringTok{"Biweight"}\NormalTok{, }\AttributeTok{passband =}\NormalTok{ pi}
\NormalTok{  ),}
  \StringTok{"$b\_\{q,}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{phi\}$"} \OtherTok{=} \FunctionTok{finite\_filters}\NormalTok{(}
    \FunctionTok{do.call}\NormalTok{(cbind, }\FunctionTok{lapply}\NormalTok{(}\FunctionTok{seq\_along}\NormalTok{(bw\_phase), }\ControlFlowTok{function}\NormalTok{(i)\{}
      \FunctionTok{rkhs\_filter}\NormalTok{(}\AttributeTok{horizon =} \DecValTok{6}\NormalTok{, }\AttributeTok{degree =} \DecValTok{3}\NormalTok{,}
                  \AttributeTok{kernel =} \StringTok{"Biweight"}\NormalTok{,}
                  \AttributeTok{optimalbw =} \ConstantTok{FALSE}\NormalTok{,}
                  \AttributeTok{bandwidth =}\NormalTok{ bw\_phase[i])[,}\FunctionTok{names}\NormalTok{(bw\_phase)[i]]}
\NormalTok{    \}))}
\NormalTok{  )}
\NormalTok{)}
\NormalTok{rkhs\_est  }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(rkhs\_filter, }\ControlFlowTok{function}\NormalTok{(method) \{}
  \FunctionTok{lapply}\NormalTok{(der\_est, }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{    x }\SpecialCharTok{*}\NormalTok{ method}
\NormalTok{  \})}
\NormalTok{\})}

\NormalTok{rkhs\_if }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(rkhs\_filter, }\ControlFlowTok{function}\NormalTok{(method) \{}
  \FunctionTok{lapply}\NormalTok{(der\_est, }\ControlFlowTok{function}\NormalTok{(x) \{}
    \FunctionTok{implicit\_forecast}\NormalTok{(x, method)}
\NormalTok{  \})}
\NormalTok{\})}

\NormalTok{fst\_f }\OtherTok{\textless{}{-}} \FunctionTok{finite\_filters}\NormalTok{(}
  \AttributeTok{sfilter =} \FunctionTok{fst\_filter}\NormalTok{(}\AttributeTok{lags =} \DecValTok{6}\NormalTok{, }\AttributeTok{leads =} \DecValTok{6}\NormalTok{, }\AttributeTok{pdegree=}\DecValTok{2}\NormalTok{, }
                       \AttributeTok{smoothness.weight=}\FloatTok{0.05}\NormalTok{, }\AttributeTok{smoothness.degree=}\DecValTok{3}\NormalTok{,}
                       \AttributeTok{timeliness.weight=}\FloatTok{0.95}\NormalTok{,}
                       \AttributeTok{timeliness.passband=}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{/}\DecValTok{12}\NormalTok{,}
                       \AttributeTok{timeliness.antiphase=}\ConstantTok{TRUE}\NormalTok{),}
  \AttributeTok{rfilters =} \FunctionTok{lapply}\NormalTok{(}\DecValTok{5}\SpecialCharTok{:}\DecValTok{0}\NormalTok{, fst\_filter, }\AttributeTok{lags =} \DecValTok{6}\NormalTok{, }\AttributeTok{pdegree=}\DecValTok{2}\NormalTok{, }
                    \AttributeTok{smoothness.weight=}\FloatTok{0.05}\NormalTok{, }\AttributeTok{smoothness.degree=}\DecValTok{3}\NormalTok{,}
                    \AttributeTok{timeliness.weight=}\FloatTok{0.95}\NormalTok{,}
                    \AttributeTok{timeliness.passband=}\DecValTok{2}\SpecialCharTok{*}\NormalTok{pi}\SpecialCharTok{/}\DecValTok{12}\NormalTok{,}
                    \AttributeTok{timeliness.antiphase=}\ConstantTok{TRUE}\NormalTok{))}
\NormalTok{fst\_est }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(der\_est, }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{  x }\SpecialCharTok{*}\NormalTok{ fst\_f}
\NormalTok{\})}
\NormalTok{fst\_if }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(der\_est, }\ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{implicit\_forecast}\NormalTok{(x, fst\_f)}
\NormalTok{\})}

\DocumentationTok{\#\# Graphiques}

\CommentTok{\# Pour tracer toutes les estimations}
\NormalTok{plot\_est }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, }\AttributeTok{nperiod =} \DecValTok{6}\NormalTok{) \{}
\NormalTok{  joint\_data }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(ts.union, data)}
\NormalTok{  joint\_data }\OtherTok{\textless{}{-}} 
    \FunctionTok{window}\NormalTok{(joint\_data,}
           \AttributeTok{start =}\NormalTok{ last\_dates[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ nperiod }\SpecialCharTok{*} \FunctionTok{deltat}\NormalTok{(joint\_data))}
  
\NormalTok{  data\_legend }\OtherTok{\textless{}{-}} 
    \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ last\_dates,}
               \AttributeTok{y =} \FunctionTok{sapply}\NormalTok{(data, tail, }\DecValTok{1}\NormalTok{),}
               \AttributeTok{label =} \FunctionTok{colnames}\NormalTok{(joint\_data))}
  
\NormalTok{  forecast}\SpecialCharTok{::}\FunctionTok{autoplot}\NormalTok{(joint\_data) }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ zoo}\SpecialCharTok{::}\NormalTok{as.yearmon) }\SpecialCharTok{+}
    \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{label =}\NormalTok{ label, }\AttributeTok{colour =}\NormalTok{ label), }
              \AttributeTok{data =}\NormalTok{ data\_legend,}
              \AttributeTok{check\_overlap =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{0}\NormalTok{, }\AttributeTok{nudge\_x =} \FloatTok{0.01}\NormalTok{,}
              \AttributeTok{size =} \DecValTok{2}\NormalTok{, }\AttributeTok{inherit.aes =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{)  }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{y =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{\}}

\NormalTok{plot\_prevs }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{ (data, }\AttributeTok{nperiod =} \DecValTok{6}\NormalTok{) \{}
\NormalTok{  joint\_data }\OtherTok{\textless{}{-}} \FunctionTok{do.call}\NormalTok{(ts.union, }\FunctionTok{lapply}\NormalTok{(data, }\ControlFlowTok{function}\NormalTok{(x) \{}
\NormalTok{    first\_date }\OtherTok{\textless{}{-}} \FunctionTok{time}\NormalTok{(x)[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{{-}} \FunctionTok{deltat}\NormalTok{(x)}
    \CommentTok{\# On rajoute la dernière date observée par lisibilité}
    \FunctionTok{ts}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{window}\NormalTok{(y, }\AttributeTok{start =}\NormalTok{ first\_date, }\AttributeTok{end =}\NormalTok{ first\_date), x), }
       \AttributeTok{start =}\NormalTok{ first\_date, }\AttributeTok{frequency =} \FunctionTok{frequency}\NormalTok{(x))}
\NormalTok{  \}))}
  
\NormalTok{  data\_legend }\OtherTok{\textless{}{-}} 
    \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \FunctionTok{sapply}\NormalTok{(data, }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{tail}\NormalTok{(}\FunctionTok{time}\NormalTok{(x), }\DecValTok{1}\NormalTok{)),}
               \AttributeTok{y =} \FunctionTok{sapply}\NormalTok{(data, tail, }\DecValTok{1}\NormalTok{),}
               \AttributeTok{label =} \FunctionTok{colnames}\NormalTok{(joint\_data))}
\NormalTok{  forecast}\SpecialCharTok{::}\FunctionTok{autoplot}\NormalTok{(joint\_data, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+} 
\NormalTok{    forecast}\SpecialCharTok{::}\FunctionTok{autolayer}\NormalTok{(}
      \FunctionTok{window}\NormalTok{(y, }\AttributeTok{start =}\NormalTok{ last\_dates[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ nperiod }\SpecialCharTok{*} \FunctionTok{deltat}\NormalTok{(y)),}
      \AttributeTok{colour =} \ConstantTok{FALSE}
\NormalTok{    ) }\SpecialCharTok{+}
    \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ zoo}\SpecialCharTok{::}\NormalTok{as.yearmon) }\SpecialCharTok{+}
    \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{label =}\NormalTok{ label, }\AttributeTok{colour =}\NormalTok{ label),}
              \AttributeTok{data =}\NormalTok{ data\_legend,}
              \AttributeTok{check\_overlap =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{hjust =} \DecValTok{0}\NormalTok{, }\AttributeTok{nudge\_x =} \FloatTok{0.01}\NormalTok{,}
              \AttributeTok{size =} \DecValTok{2}\NormalTok{, }\AttributeTok{inherit.aes =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{y =} \ConstantTok{NULL}\NormalTok{)}
\NormalTok{\}}

\NormalTok{all\_est }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(lp\_est, }\FunctionTok{list}\NormalTok{(}\StringTok{"LC param. locale"} \OtherTok{=}\NormalTok{ loc\_lc\_est),}
             \FunctionTok{list}\NormalTok{(}\StringTok{"QL param. locale"} \OtherTok{=}\NormalTok{ loc\_ql\_est), }
\NormalTok{             rkhs\_est,}
             \FunctionTok{list}\NormalTok{(}\AttributeTok{FST =}\NormalTok{ fst\_est))}
\NormalTok{all\_if }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(lp\_if, }\FunctionTok{list}\NormalTok{(}\StringTok{"LC param. locale"} \OtherTok{=}\NormalTok{ loc\_lc\_if),}
            \FunctionTok{list}\NormalTok{(}\StringTok{"QL param. locale"} \OtherTok{=}\NormalTok{ loc\_ql\_if), }
\NormalTok{            rkhs\_if,}
            \FunctionTok{list}\NormalTok{(}\AttributeTok{FST =}\NormalTok{ fst\_if))}
\NormalTok{y\_lim }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{102}\NormalTok{, }\DecValTok{105}\NormalTok{)}
\NormalTok{all\_plots\_est }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}
  \FunctionTok{names}\NormalTok{(all\_est), }
  \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{plot\_est}\NormalTok{(all\_est[[x]]) }\SpecialCharTok{+} 
    \FunctionTok{ggtitle}\NormalTok{(latex2exp}\SpecialCharTok{::}\FunctionTok{TeX}\NormalTok{(}\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Tendance{-}cycle avec \%s"}\NormalTok{, x))) }\SpecialCharTok{+}
    \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{ylim =}\NormalTok{ y\_lim)}
\NormalTok{)}
\NormalTok{all\_plots\_prev }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(}
  \FunctionTok{names}\NormalTok{(all\_if), }
  \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{plot\_prevs}\NormalTok{(all\_if[[x]]) }\SpecialCharTok{+} 
    \FunctionTok{ggtitle}\NormalTok{(latex2exp}\SpecialCharTok{::}\FunctionTok{TeX}\NormalTok{(}\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Prévisions implicites avec \%s"}\NormalTok{, x)))}
\NormalTok{)}

\FunctionTok{wrap\_plots}\NormalTok{(all\_plots\_est[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{], }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}
\FunctionTok{wrap\_plots}\NormalTok{(all\_plots\_prev[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{], }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}

\FunctionTok{wrap\_plots}\NormalTok{(all\_plots\_est[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{6}\NormalTok{)], }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}
\FunctionTok{wrap\_plots}\NormalTok{(all\_plots\_prev[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{6}\NormalTok{)], }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}

\FunctionTok{wrap\_plots}\NormalTok{(}\FunctionTok{c}\NormalTok{(all\_plots\_est[}\DecValTok{7}\SpecialCharTok{:}\DecValTok{9}\NormalTok{],}
\NormalTok{             all\_plots\_prev[}\DecValTok{7}\SpecialCharTok{:}\DecValTok{9}\NormalTok{]), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{)}

\FunctionTok{wrap\_plots}\NormalTok{(}\FunctionTok{c}\NormalTok{(all\_plots\_est[}\DecValTok{10}\NormalTok{], all\_plots\_prev[}\DecValTok{10}\NormalTok{]), }\AttributeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{an-noyaux}{%
\section{Noyaux et régression locale}\label{an-noyaux}}

\hypertarget{sec-kernels}{%
\subsection{Les différents noyaux}\label{sec-kernels}}

Dans les problèmes d'extraction du signal, les observations sont généralement pondérées par rapport à leur distance à la date \(t\) : pour estimer la tendance-cycle à la date \(t\), on accorde généralement plus d'importance aux observations qui sont proches de \(t\).

Dans le cas continu, un noyau \(\kappa\) est une fonction positive, paire et intégrable telle que \(\int_{-\infty}^{+\infty}\kappa(u) \ud u=1\) et \(\kappa(u)=\kappa(-u)\).
Dans le cas discret, un noyau est un ensemble de poids \(\kappa_j\), \(j=0,\pm1,\dots,\pm h\) avec \(\kappa_j \geq0\) et \(\kappa_j=\kappa_{-j}\).

Une classe importante de noyaux est celle des noyaux Beta.
Dans le cas discret, à un facteur multiplicatif près (de sorte que \(\sum_{j=-h}^h\kappa_j=1\))~:
\[
\kappa_j = \left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^r
\right)^s,\quad\text{avec }r>0,s\geq 0
\]
Cette classe englobe la majorité des noyaux présentés dans cette étude, à l'exception des noyaux d'Henderson, trapézoïdal et gaussien.
Les principaux noyaux (qui sont également implémentés dans \texttt{rjd3filters}) sont~:

\begin{multicols}{2}

\begin{itemize}
\item
  \(r=1,s=0\) noyau uniforme :
  \[\kappa_j^U=1\]
\item
  \(r=s=1\) noyau triangulaire :
  \[\kappa_j^T=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert
  \right)\]
\item
  \(r=2,s=1\) noyau d'Epanechnikov (ou parabolique) :
  \[\kappa_j^E=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^2
  \right)\]
\item
  \(r=s=2\) noyau quadratique (\emph{biweight})~:
  \[\kappa_j^{BW}=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^2
  \right)^2\]
\item
  \(r = 2, s = 3\) noyau cubique (\emph{triweight}) :
  \[\kappa_j^{TW}=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^2
  \right)^3\]
\item
  \(r = s = 3\) noyau tricube :
  \[\kappa_j^{TC}=\left(
  1-
  \left\lvert
  \frac j {h+1}
  \right\lvert^3
  \right)^3\]
\item
  noyau d'Henderson (voir partie \ref{sec-sympolyfilter} pour plus de détails) :
  \[
  \kappa_{j}=\left[1-\frac{j^2}{(h+1)^2}\right]
  \left[1-\frac{j^2}{(h+2)^2}\right]
  \left[1-\frac{j^2}{(h+3)^2}\right]
  \]
\item
  noyau trapézoïdal :
  \[
  \kappa_j^{TP}=
  \begin{cases}
  \frac{1}{3(2h-1)} & \text{ si }j=\pm h 
  \\
  \frac{2}{3(2h-1)} & \text{ si }j=\pm (h-1)\\
  \frac{1}{2h-1}& \text{ sinon}
  \end{cases}
  \]
\item
  noyau gaussien\footnote{
    Dans \texttt{rjd3filters} \(\sigma^2\) est fixé arbitrairement à \(\sigma^2=0,25\).}:
  \[
  \kappa_j^G=\exp\left(
  -\frac{
  j^2
  }{
  2\sigma^2h^2
  }\right)
  \]
\end{itemize}

\end{multicols}

Les noyaux d'Henderson, trapézoïdal et gaussien sont particuliers :

\begin{itemize}
\item
  Les fonctions noyau d'Henderson et trapézoïdal changent avec la fenêtre (les autres dépendent uniquement du rapport \(j/(h+1)\)).
\item
  Pour les noyaux trapézoïdal et gaussien, d'autres définitions pourraient être utilisées et ils sont donc définis arbitrairement.
  Pour le noyau trapézoïdal on pourrait par exemple prendre une pente utilisant moins de points (par exemple \(\begin{cases}\frac{1}{4h} &\text{ si }j=\pm h \\ \frac{2}{4h}& \text{ sinon}\end{cases}\)) ou donner un poids plus faible aux observations extrêmes (par exemple \(\begin{cases}\frac{1}{6h-1} &\text{ si }j=\pm h \\ \frac{3}{6h-1}& \text{ sinon}\end{cases}\)) ; pour le noyau gaussien on pourrait prendre une variance plus ou moins élevée.\\
  Le noyau trapézoïdal implémenté dans \texttt{rjd3filters} permet de calculer les moyennes mobiles utilisées dans l'algorithme X-13ARIMA pour l'extraction des composantes saisonnières.
  Il n'est pas adapté dans le cas de l'extraction de la tendance-cycle.
\end{itemize}

\hypertarget{sec-sympolyfilter}{%
\subsection{Quelques filtres symétriques particuliers}\label{sec-sympolyfilter}}

Lorsque l'on effectue une régression locale en modélisant une constante locale (\(d=0\)), on obtient l'estimateur de \textbf{Nadaraya-Watson} (ou l'estimateur par noyaux).

Avec le noyau uniforme on obtient le filtre de \textcite{macaulay1931smoothing}.
Lorsque \(d=0\) ou \(d=1\), on retrouve la moyenne arithmétique : \(w_j=w=\frac{1}{2h+1}\).

Le noyau d'\textbf{Epanechnikov} est souvent recommandé comme le noyau optimal car il minimise l'erreur quadratique moyenne de l'estimation par polynômes locaux.

Le \textbf{Loess}, \emph{locally estimated scatterplot smoothing} (notammenet utilisé dans la méthode de désaisonnalisation STL), est une régression locale pondérée qui utilise le noyau tricube.

Le \textbf{filtre d'Henderson} est un cas particulier de l'approximation locale cubique (\(d=3\)), couramment utilisée pour l'extraction de la tendance-cycle (c'est par exemple le filtre utilisé dans le logiciel de désaisonnalisation X-13ARIMA).
Pour une fenêtre fixée, Henderson a trouvé le noyau qui donnait l'estimation la plus lisse de la tendance.
Il montre l'équivalence entre les trois problèmes suivants :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  minimiser la variance de la différence d'ordre trois de la série lissée par l'application d'une moyenne mobile ;\\
\item
  minimiser la somme du carré de la différence d'ordre trois des coefficients du filtre, c'est le critère de lissage (\emph{smoothness}) : \(S=\sum_j(\nabla^{3}\theta_{j})^{2}\) ;\\
\item
  estimer une tendance localement cubique par les moindres carrés pondérés, où les poids sont choisis de sorte à minimiser la \emph{smoothness} (cela conduit au noyau présenté dans la section \ref{sec-kernels}).
\end{enumerate}

\newpage

\printbibliography[heading=bibintoc]

\end{document}
